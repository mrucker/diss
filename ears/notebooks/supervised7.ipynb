{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203f7238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice, chain, count, product, repeat\n",
    "from contextlib import nullcontext\n",
    "\n",
    "data_dir = \"../data\"\n",
    "\n",
    "import coba as cb\n",
    "\n",
    "import warnings\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, MinMaxScaler, StandardScaler, Binarizer\n",
    "from sklearn.feature_selection import  mutual_info_classif, f_classif, GenericUnivariateSelect\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict, StratifiedKFold, GridSearchCV, LeaveOneGroupOut, StratifiedShuffleSplit\n",
    "from sklearn.metrics import balanced_accuracy_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import peft\n",
    "from parameterfree import COCOB\n",
    "\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import coba as cb\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "try:\n",
    "    torch.set_num_threads(3)\n",
    "    torch.set_num_interop_threads(3)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "c0 = \"#444\"\n",
    "c1 = \"#0072B2\"\n",
    "c2 = \"#E69F00\"\n",
    "c3 = \"#009E73\"\n",
    "c4 = \"#56B4E9\"\n",
    "c5 = \"#D55E00\"\n",
    "c6 = \"#F0E442\"\n",
    "c7 = \"#CC79A7\"\n",
    "c8 = \"#000000\"\n",
    "c9 = \"#332288\"\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "plt.rc('font', **{'size': 20})\n",
    "\n",
    "df = pd.read_csv(f\"{data_dir}/all_features_1h_v3.csv\")\n",
    "\n",
    "G  = df[\"id_participant\"].to_numpy()\n",
    "X1 = df[[c for c in df.columns if c.startswith(\"acc_\")]].to_numpy()\n",
    "X2 = df[[c for c in df.columns if c.startswith(\"acc_\") or c.startswith(\"gps_\") or c.startswith(\"motion_\")]].to_numpy()\n",
    "Y1 = df[\"ER_desire\"].astype(float).to_numpy()\n",
    "Y2 = (df[\"INT_availability\"] == \"yes\").astype(float).to_numpy()\n",
    "\n",
    "no_na = ~(np.isnan(Y1) | np.isnan(Y2))\n",
    "\n",
    "G  = G [no_na]\n",
    "X1 = X1[no_na]\n",
    "X2 = X2[no_na]\n",
    "X3 = X2.copy()\n",
    "X4 = X2.copy()\n",
    "Y1 = np.expand_dims(Y1[no_na],axis=1)\n",
    "Y2 = np.expand_dims(Y2[no_na],axis=1)\n",
    "\n",
    "for g in set(G):\n",
    "    Y1[G == g] = Binarizer(threshold=np.mean(Y1[G == g].squeeze())).fit_transform(Y1[G == g])\n",
    "    X1[G == g] = StandardScaler().fit_transform(X1[G == g])\n",
    "    X2[G == g] = StandardScaler().fit_transform(X2[G == g])\n",
    "    X3[G == g] = StandardScaler().fit_transform(X3[G == g])\n",
    "    X4[G == g] = StandardScaler().fit_transform(X4[G == g])\n",
    "\n",
    "X3 = np.concatenate([X3, np.expand_dims((df[\"Platform\"] == \"Android\").astype(float).to_numpy(),1)[no_na]],axis=1)\n",
    "X3 = np.concatenate([X3, np.expand_dims((df[\"Platform\"] == \"iPhone\" ).astype(float).to_numpy(),1)[no_na]],axis=1)\n",
    "\n",
    "X4 = np.concatenate([X4, np.expand_dims((df[\"Platform\"] == \"Android\").astype(float).to_numpy(),1)[no_na]],axis=1)\n",
    "X4 = np.concatenate([X4, np.expand_dims((df[\"Platform\"] == \"iPhone\" ).astype(float).to_numpy(),1)[no_na]],axis=1)\n",
    "X4 = np.concatenate([X4, np.expand_dims((df[\"tag\"] == \"evening\" ).astype(float).to_numpy(),1)[no_na]],axis=1)\n",
    "X4 = np.concatenate([X4, np.expand_dims((df[\"tag\"] == \"morning\" ).astype(float).to_numpy(),1)[no_na]],axis=1)\n",
    "X4 = np.concatenate([X4, np.expand_dims((df[\"tag\"] == \"afternoon\" ).astype(float).to_numpy(),1)[no_na]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81a7c5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 54, 57, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 121, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 293, 311, 319, 357, 373, 374, 377, 382, 390, 402, 426, 442, 447, 451, 452, 455, 463, 469, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 189, 'Interactions': 81648}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl), 'y': y, 'n_models': n_models}\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "\n",
    "        rng = cb.CobaRandom(1)\n",
    "        torch.set_num_threads(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            sa = torch.nn.Sequential(*list(s1.children())[len(self.s1)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*list(s1.children())[:len(self.s1)-self.dae_dropn])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,s3] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,s2opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s2(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y = env.train()[:2]\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        X, Y = env.test()\n",
    "        trn,tst = next(StratifiedShuffleSplit(1,train_size=35/len(X),random_state=1).split(X,Y))\n",
    "        X = X[np.hstack([trn,tst])]\n",
    "        Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            s3 = mods[-1]\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "            opts[-1] = s3opt\n",
    "\n",
    "        lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "        lrnys = [[] for _ in range(len(mods_opts))]\n",
    "        memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "        def predict(X):\n",
    "            preds = torch.tensor(0)\n",
    "            for mods, _ in mods_opts:\n",
    "                [s1,_,s2,s3] = mods\n",
    "                preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "            return preds/len(mods_opts)\n",
    "\n",
    "        def score(X,Y):\n",
    "            with torch.no_grad():\n",
    "                return {f'auc{y}': roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y) }\n",
    "\n",
    "        yield score(X[35:,:], Y[35:,:])\n",
    "\n",
    "        loss = torch.nn.BCEWithLogitsLoss()\n",
    "        for i in range(35):\n",
    "\n",
    "            for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                [s1,_,s2,s3 ] = mods\n",
    "                [_,_,_,s3opt] = opts\n",
    "\n",
    "                x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                if self.pers_lrn_cnt:\n",
    "                    lrnx.append(x)\n",
    "                    lrny.append(y)\n",
    "\n",
    "                if self.pers_mem_cnt: \n",
    "                    mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                if len(mems) > self.pers_mem_cnt:\n",
    "                    rng.shuffle(mems, inplace=True)\n",
    "                    for j in reversed(range(self.pers_mem_rcl)):\n",
    "                        x,y,n = mems[j]\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "                        if n == 1: mems.pop(j)\n",
    "                        else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                    x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                    y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                    if s3opt: s3opt.zero_grad()\n",
    "                    loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                    if s3opt: s3opt.step()\n",
    "                    del lrnx[:self.pers_lrn_cnt]\n",
    "                    del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "            yield score(X[35:,:], Y[35:,:])\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y1,Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    #dae (1) + one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r'), (30,1), 2, 3, 0, 3, 3, 20, 2, 2, [1], 1),\n",
    "    \n",
    "    #one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((), (len(x),30,'l','r'), (30,1), 0, 0, 0, 3, 3, 20, 2, 2, [1], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 20, 2, 2, [1], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r',-1), (-1,1), 2, 3, 5, 3, 3, 20, 2, 2, [1], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r',-1), (-1,1), 2, 3, 5, 2, 15, 20, 1, 1, [1], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 1, 3, 20, 2, 2, [1], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 0, 1, 20, 2, 2, [1], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 3, 25, 0, 0, 0, [1], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 0, 1, 0, 0, 0, [1], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 1, 1, 0, 0, 0, [1], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 1, 0, 0, 0, [1], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,.2,1), 2, 3, 5, 2, 1, 0, 0, 0, [1], 2),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/1.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4266b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 219, 'Interactions': 94608}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "\n",
    "        rng = cb.CobaRandom(1)\n",
    "        torch.set_num_threads(1)\n",
    "        torch.manual_seed(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            sa = torch.nn.Sequential(*list(s1.children())[len(self.s1)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*list(s1.children())[:len(self.s1)-self.dae_dropn])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,s3] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,s2opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s2(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y = env.train()[:2]\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        X, Y = env.test()\n",
    "        trn,tst = next(StratifiedShuffleSplit(1,train_size=35/len(X),random_state=1).split(X,Y))\n",
    "        X = X[np.hstack([trn,tst])]\n",
    "        Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            if not self.pers_rank:\n",
    "                opts[-1] = COCOB(mods[-1].parameters())\n",
    "            else:\n",
    "                targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "        lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "        lrnys = [[] for _ in range(len(mods_opts))]\n",
    "        memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "        def predict(X):\n",
    "            preds = torch.tensor(0)\n",
    "            for mods, _ in mods_opts:\n",
    "                [s1,_,s2,s3] = mods\n",
    "                preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "            return preds/len(mods_opts)\n",
    "\n",
    "        def score(X,Y):\n",
    "            with torch.no_grad():\n",
    "                return {f'auc{y}': roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y) }\n",
    "\n",
    "        yield score(X[35:,:], Y[35:,:])\n",
    "\n",
    "        loss = torch.nn.BCEWithLogitsLoss()\n",
    "        for i in range(35):\n",
    "\n",
    "            for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                [s1,_,s2,s3 ] = mods\n",
    "                [_,_,_,s3opt] = opts\n",
    "\n",
    "                x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                if self.pers_lrn_cnt:\n",
    "                    lrnx.append(x)\n",
    "                    lrny.append(y)\n",
    "\n",
    "                if self.pers_mem_cnt: \n",
    "                    mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                if len(mems) > self.pers_mem_cnt:\n",
    "                    rng.shuffle(mems, inplace=True)\n",
    "                    for j in reversed(range(self.pers_mem_rcl)):\n",
    "                        x,y,n = mems[j]\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "                        if n == 1: mems.pop(j)\n",
    "                        else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                    x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                    y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                    if s3opt: s3opt.zero_grad()\n",
    "                    loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                    if s3opt: s3opt.step()\n",
    "                    del lrnx[:self.pers_lrn_cnt]\n",
    "                    del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "            yield score(X[35:,:], Y[35:,:])\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    #dae (1) + one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r'), (30,1), 2, 3, 0, 3, 3, 20, 2, 2, 0, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 1, 3, 20, 2, 2, 0, [0], 1),\n",
    "\n",
    "    #dae (1) + one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r'), (30,1), 2, 3, 0, 3, 3, 20, 2, 2, 1, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 1, 3, 20, 2, 2, 1, [0], 1),\n",
    "\n",
    "    #dae (1) + one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r'), (30,1), 2, 3, 0, 3, 3, 20, 2, 2, 2, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 1, 3, 20, 2, 2, 2, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 20, 2, 2, 1, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 20, 2, 2, 2, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 20, 2, 2, 3, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,30,'l','r',1), 2, 3, 5, 2, 3, 20, 2, 2, 3, [0], 1),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 3, 3, 20, 2, 2, 3, [0], 2),\n",
    "\n",
    "    #dae (1) + one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((), (len(x),60,'l','r'), (60,1), 0, 0, 0, 3, 3, 20, 2, 2, 3, [0], 1),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/2.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed3542dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 657, 'Interactions': 426072}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            sa = torch.nn.Sequential(*list(s1.children())[len(self.s1)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*list(s1.children())[:len(self.s1)-self.dae_dropn])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,s3] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,s2opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s2(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y = env.train()[:2]\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,s3 ] = mods\n",
    "                    [_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in enumerate(torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,3))\n",
    "vals = lambda x: [\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 20, 2, 2, 3, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 15, 2, 2, 3, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 15, 1, 1, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 1, 1, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 2, 10, 1, 1, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + one-sl (1,2,3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r'), (30,1), 2, 3, 0, 3, 3, 20, 2, 2, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 2, 5, 1, .5, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 2, 0, 0, 0, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 1, 10, 1, 1, 2, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 1, 10, 1, 1, 1, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 2, 10, 1, 1, 3, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 15, 2, 2, 4, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 15, 2, 2, 0, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 15, 2, 2, 5, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 2),\n",
    "\n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 15, 3, 3, 4, [0], 2),\n",
    "    \n",
    "    #dae (1) + sep-sl (1,2) + one-sl (3) + pers (3)\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 3, 3, 15, 2, 2, 4, [0], 2),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/3.log',processes=20,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c16fc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 219, 'Interactions': 143664}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            sa = torch.nn.Sequential(*list(s1.children())[len(self.s1)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*list(s1.children())[:len(self.s1)-self.dae_dropn])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,s3] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,s2opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s2(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y = env.train()[:2]\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,s3 ] = mods\n",
    "                    [_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in enumerate(torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.1,60,'l','r',len(x)), (60,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.2,60,'l','r',len(x)), (60,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,60,'l','r',len(x)), (60,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "\n",
    "    MyEvaluator((len(x),.1,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.2,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "\n",
    "    MyEvaluator((len(x),.1,120,'l','r',len(x)), (120,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.2,120,'l','r',len(x)), (120,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,120,'l','r',len(x)), (120,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "\n",
    "    MyEvaluator((len(x),.4,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 2, 6, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    \n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 3, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 4, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    \n",
    "    MyEvaluator((), (len(x),'l','r',-1), (-1,1), 0, 0, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/4.log',processes=30,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8f569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 657, 'Interactions': 80811}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            sa = torch.nn.Sequential(*list(s1.children())[len(self.s1)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*list(s1.children())[:len(self.s1)-self.dae_dropn])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,s3] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,s2opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s2(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y = env.train()[:2]\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,s3 ] = mods\n",
    "                    [_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in enumerate(torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,3))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 5, 2, 3, 10, 2, 2, 4, [0], 2),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((), (len(x),'l','r',-1), (-1,1), 0, 0, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.2,90 ,'l','r',len(x)), (90,30,'l','r'), (30,1), 2, 3, 0, 3, 3, 20, 2, 2, 2, [0], 1),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/5.log',processes=30,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "384c1ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 438, 'Interactions': 53874}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            sa = torch.nn.Sequential(*list(s1.children())[len(self.s1)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*list(s1.children())[:len(self.s1)-self.dae_dropn])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,s3] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "\n",
    "                opts = list(filter(None,[s1opt,s2opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s2(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y = env.train()[:2]\n",
    "                Y = Y[:,self.y]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,s3 ] = mods\n",
    "                    [_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in enumerate(torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,2))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 4, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 5, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 6, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/6.log',processes=30,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cd7ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 219, 'Interactions': 98769}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models  = n_models\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            \n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "            \n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X = env.train()[0]\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = torch.zeros((len(Y),1))\n",
    "                weights = Counter(G.tolist())\n",
    "                for g,w in weights.items():\n",
    "                    W[G==g] = 1/w\n",
    "\n",
    "                i = defaultdict(lambda c= count(0):next(c))\n",
    "                I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                \n",
    "                W = torch.zeros((len(Y),1))\n",
    "                weights = Counter(G.tolist())\n",
    "                for g,w in weights.items():\n",
    "                    W[G==g] = 1/w\n",
    "\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in enumerate(torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "#test with waiting\n",
    "#test with flat-out\n",
    "#test with multi-task\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 1, 4, 0, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 2, 4, 0, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,'l','r',-1), (-1,1), 1, 3, 4, 0, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (-1,1), 1, 1, 4, 0, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,120,'l','r',-1), (-1,1), 1, 1, 4, 0, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,120,'l','r',-1), (120,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 4, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (-1,1), 1, 1, 4, 0, 2, 3, 10, 2, 2, 3, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 3, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 5, [0], 1),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/7.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40daed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 54, 57, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 121, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 293, 311, 319, 357, 373, 374, 377, 382, 390, 402, 426, 442, 447, 451, 452, 455, 463, 469, 486, 536, 547]\n",
      "2025-05-22 03:54:15 -- pid-2728468 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:54:48 -- pid-2728679 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:54:50 -- pid-2728591 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:54:58 -- pid-2728422 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:54:59 -- pid-2728228 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:55:05 -- pid-2728418 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:55:11 -- pid-2728275 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:55:14 -- pid-2728283 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 03:55:17 -- pid-2728226 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/1883076926.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "{'Learners': 1, 'Environments': 189, 'Interactions': 69372}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.mean())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        s2_is_neg_1 = self.s2[-1] == -1\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            \n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "            \n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()])\n",
    "                        #if self.weighted and 1 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()])\n",
    "                        l = l.reshape((-1,len(self.y)))\n",
    "                        if self.weighted and 2 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(s3(s2(s1(_X.nan_to_num()))),_y)\n",
    "                        if self.weighted and 2 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "#test with weighting\n",
    "#test with flat-out\n",
    "#test with multi-task\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y1,Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  1], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,2), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0,1], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  1], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 2), (90,2), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0,1], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  1], 1, weighted=[2,3]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,2), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0,1], 1, weighted=[2,3]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  1], 1, weighted=[2,3]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 2), (90,2), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0,1], 1, weighted=[2,3]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r'), (90,2), 1, 1, 0, 0, 2, 3, 10, 2, 2, 0, [0,1], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/8.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4db71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 54, 57, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 121, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 293, 311, 319, 357, 373, 374, 377, 382, 390, 402, 426, 442, 447, 451, 452, 455, 463, 469, 486, 536, 547]\n",
      "2025-05-22 09:51:57 -- pid-2822882 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/2874644570.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/2874644570.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/2874644570.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-22 09:52:09 -- pid-2822697 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_2573299/2874644570.py\", line 313, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/2874644570.py\", line 311, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_2573299/2874644570.py\", line 311, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "{'Learners': 1, 'Environments': 189, 'Interactions': 15416}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.mean())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        s2_is_neg_1 = self.s2[-1] == -1\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            \n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "            \n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()])\n",
    "                        #if self.weighted and 1 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()])\n",
    "                        l = l.reshape((-1,len(self.y)))\n",
    "                        if self.weighted and 2 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(s3(s2(s1(_X.nan_to_num()))),_y)\n",
    "                        if self.weighted and 2 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [ 0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [ 0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/9.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa12aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 219, 'Interactions': 58015}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "            \n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "            \n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss(reduction=\"none\")\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()])\n",
    "                        #if self.weighted and 1 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()])\n",
    "                        l = l.reshape((-1,len(self.y)))\n",
    "                        if self.weighted and 2 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        l = loss(s3(s2(s1(_X.nan_to_num()))),_y)\n",
    "                        if self.weighted and 2 in self.weighted: l *= _w\n",
    "                        l.mean().backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2,3]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r', 1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2,3]),\n",
    "    \n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 3, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 5, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 6, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[]),\n",
    "    \n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,45,'l','r',-1), (45,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,45,'l','r',-1), (45,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/10.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10f5731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 133, 'Interactions': 31078}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[] ),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  4, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  5, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  6, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 20, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[2]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 20, 1, 2, 3, 10, 2, 2, 0, [  0], 1, weighted=[] ),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/11.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e73ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 219, 'Interactions': 107748}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X3,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 16, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1, 16, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1, 16, 4, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,180,'l','r',-1), (180,1), 1, 1,  8, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,180,'l','r',-1), (180,1), 1, 1, 16, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    \n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 1, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 2, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X3[0])).run('../logs/7/12.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "559ede12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [2, 25, 61, 81, 82, 87, 89, 97, 99, 103, 111, 117, 118, 122, 132, 137, 146, 181, 196, 203, 206, 227, 231, 246, 311, 319, 357, 373, 377, 382, 390, 402, 426, 442, 451, 452, 455, 463, 486, 536, 547]\n",
      "{'Learners': 1, 'Environments': 219, 'Interactions': 80811}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X4,np.hstack([Y2]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 16, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,180,'l','r',-1), (180,1), 1, 1,  8, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    \n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 1, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',90,'l','r',-1), (90,1), 1, 1,  8, 1, 2, 2, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X4[0])).run('../logs/7/13.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7ffaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [54, 57, 81, 118, 121, 132, 293, 374, 447, 469]\n",
      "2025-05-23 14:25:30 -- pid-3411000 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 305, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-23 14:25:35 -- pid-3411043 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 305, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-23 14:30:54 -- pid-3411267 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 305, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "2025-05-23 14:31:21 -- pid-3410994 -- Unexpected exception:\n",
      "\n",
      "  File \"/home/mrucker/Projects/coba/coba/experiments/process.py\", line 179, in filter\n",
      "    yield [\"T4\", (env_id, lrn_id, val_id), list(SafeEvaluator(val).evaluate(env,lrn))]\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 305, in evaluate\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in score\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/tmp/ipykernel_3374098/3685941534.py\", line 303, in <listcomp>\n",
      "    <unknown code, this is likely due to the code being in a Jupyter cell>\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 640, in roc_auc_score\n",
      "    return _average_binary_score(\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_base.py\", line 75, in _average_binary_score\n",
      "    return binary_metric(y_true, y_score, sample_weight=sample_weight)\n",
      "  File \"/home/mrucker/miniconda3/envs/diss/lib/python3.10/site-packages/sklearn/metrics/_ranking.py\", line 382, in _binary_roc_auc_score\n",
      "    raise ValueError(\n",
      "\n",
      "  ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
      "\n",
      "{'Learners': 1, 'Environments': 247, 'Interactions': 20090}\n"
     ]
    }
   ],
   "source": [
    "import coba as cb\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(\"Bad Layer\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        if self.ws_steps0:\n",
    "            if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "            if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 40\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R):\n",
    "    X, Y, G = torch.tensor(X).float(), torch.tensor(Y).float(), torch.tensor(G)\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], 'rest', g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(make_envs(X4,np.hstack([Y1]),G,1))\n",
    "vals = lambda x: [\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator((len(x),.3,90,'l','r',len(x)), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals(X4[0])).run('../logs/7/14.log',processes=35,quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d47327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
