{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "203f7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Code\n",
    "\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice, chain, count, product\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "data_dir = \"../data\"\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import coba as cb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from parameterfree import COCOB\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "try:\n",
    "    torch.set_num_threads(3)\n",
    "    torch.set_num_interop_threads(3)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "c0 = \"#444\"\n",
    "c1 = \"#0072B2\"\n",
    "c2 = \"#E69F00\"\n",
    "c3 = \"#009E73\"\n",
    "c4 = \"#56B4E9\"\n",
    "c5 = \"#D55E00\"\n",
    "c6 = \"#F0E442\"\n",
    "c7 = \"#CC79A7\"\n",
    "c8 = \"#000000\"\n",
    "c9 = \"#332288\"\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "plt.rc('font', **{'size': 20})\n",
    "\n",
    "def make_emotions_df():\n",
    "\n",
    "    def add_day_columns(df, timestamp_col, participant_df):\n",
    "        return add_rel_day(add_start_day(add_day(df, timestamp_col), participant_df))\n",
    "\n",
    "    def add_day(df, timestamp_col):\n",
    "        df = df.copy()\n",
    "        df[\"Day\"] = (df[timestamp_col]/(60*60*24)).apply(np.floor)\n",
    "        return df\n",
    "\n",
    "    def add_start_day(df, participant_df):\n",
    "        participant_df = participant_df.copy()\n",
    "        participant_df[\"StartDay\"] = (participant_df[\"DataStartStampUtc\"]/(60*60*24)).apply(np.floor)\n",
    "        return pd.merge(df, participant_df[['ParticipantId',\"StartDay\"]])\n",
    "\n",
    "    def add_rel_day(df):\n",
    "        df = df.copy()\n",
    "        df[\"RelDay\"] = df[\"Day\"]-df[\"StartDay\"]\n",
    "        return df\n",
    "\n",
    "    def drop_all1_ends(df):\n",
    "        last, last_gt_1, keep = df.copy(),df.copy(), df.copy()\n",
    "        \n",
    "        last_gt_1 = last_gt_1[last_gt_1[\"State Anxiety\"]!= 1]\n",
    "        last_gt_1 = last_gt_1.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        last_gt_1 = last_gt_1.rename(columns={\"RelDay\":\"Last Day > 1\"})\n",
    "\n",
    "        last = last.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        last = last.rename(columns={\"RelDay\":\"Last Day\"})\n",
    "\n",
    "        for pid in last[\"ParticipantId\"]:\n",
    "            \n",
    "            last_day = last[last[\"ParticipantId\"]==pid][\"Last Day\"].item()\n",
    "            last_day_gt_1 = last_gt_1[last_gt_1[\"ParticipantId\"]==pid][\"Last Day > 1\"].item()\n",
    "            \n",
    "            if last_day-last_day_gt_1 >= 3:\n",
    "                is_not_pid = keep[\"ParticipantId\"] != pid\n",
    "                is_lt_day  = keep[\"RelDay\"] <= last_day_gt_1\n",
    "                keep = keep[is_not_pid | is_lt_day]\n",
    "\n",
    "        return keep\n",
    "\n",
    "    emotions_df = pd.read_csv(f'{data_dir}/Emotions.csv')\n",
    "    participant_df = pd.read_csv(f'{data_dir}/Participants.csv')\n",
    "\n",
    "    emotions_df = emotions_df[emotions_df[\"WatchDataQuality\"] == \"Good\"]\n",
    "\n",
    "    emotions_df[\"State Anxiety\"] = pd.to_numeric(emotions_df[\"State Anxiety\"], errors='coerce')\n",
    "    emotions_df[\"ER Interest\"] = pd.to_numeric(emotions_df[\"ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Phone ER Interest\"] = pd.to_numeric(emotions_df[\"Phone ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Response Time (min)\"] = (emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])/60\n",
    "    emotions_df[\"Response Time (sec)\"] = (emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])\n",
    "    emotions_df[\"Response Time (log min)\"] = np.log((1+ emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])/60)\n",
    "    emotions_df[\"Response Time (log sec)\"] = np.log((1+ emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"]))\n",
    "\n",
    "    emotions_df[\"State Anxiety (z)\"] = float('nan')\n",
    "    emotions_df[\"ER Interest (z)\"] = float('nan')\n",
    "\n",
    "    for pid in set(emotions_df[\"ParticipantId\"].tolist()):\n",
    "        is_pid = emotions_df[\"ParticipantId\"] == pid\n",
    "        is_anx = emotions_df[\"State Anxiety\"] > 1\n",
    "        emotions_df.loc[is_pid,[\"ER Interest (z)\"]] = StandardScaler().fit_transform(emotions_df.loc[is_pid,[\"ER Interest\"]])\n",
    "        emotions_df.loc[is_pid&is_anx,[\"State Anxiety (z)\"]] = StandardScaler().fit_transform(emotions_df.loc[is_pid&is_anx,[\"State Anxiety\"]])\n",
    "\n",
    "    emotions_df = add_day_columns(emotions_df, \"DeliveredTimestampUtc\", participant_df)\n",
    "\n",
    "    emotions_df = emotions_df[emotions_df[\"RelDay\"] < 11]\n",
    "\n",
    "    return drop_all1_ends(emotions_df)\n",
    "\n",
    "emotions_df = make_emotions_df()\n",
    "\n",
    "class TheoryGridCellSpatialRelationEncoder:\n",
    "    #https://arxiv.org/pdf/2003.00824\n",
    "    def __init__(self, coord_dim = 2, frequency_num = 16, max_radius = 10000,  min_radius = 1000, freq_init = \"geometric\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coord_dim: the dimention of space, 2D, 3D, or other\n",
    "            frequency_num: the number of different sinusoidal with different frequencies/wavelengths\n",
    "            max_radius: the largest context radius this model can handle\n",
    "        \"\"\"\n",
    "\n",
    "        self.frequency_num = frequency_num\n",
    "        self.coord_dim = coord_dim \n",
    "        self.max_radius = max_radius\n",
    "        self.min_radius = min_radius\n",
    "        self.freq_init = freq_init\n",
    "\n",
    "        # the frequency we use for each block, alpha in ICLR paper\n",
    "        self.cal_freq_list()\n",
    "        \n",
    "        # freq_mat shape: (frequency_num, 1)\n",
    "        freq_mat = np.expand_dims(self.freq_list, axis = 1)\n",
    "        # self.freq_mat shape: (frequency_num, 6)\n",
    "        self.freq_mat = np.repeat(freq_mat, 6, axis = 1)\n",
    "\n",
    "        # there unit vectors which is 120 degree apart from each other\n",
    "        self.unit_vec1 = np.asarray([1.0, 0.0])                        # 0\n",
    "        self.unit_vec2 = np.asarray([-1.0/2.0, np.sqrt(3)/2.0])      # 120 degree\n",
    "        self.unit_vec3 = np.asarray([-1.0/2.0, -np.sqrt(3)/2.0])     # 240 degree\n",
    "\n",
    "        # compute the dimention of the encoded spatial relation embedding\n",
    "        self.input_embed_dim = int(6 * self.frequency_num)\n",
    "        \n",
    "    def cal_freq_list(self):\n",
    "        if self.freq_init == \"random\":\n",
    "            self.freq_list = np.random.random(size=[self.frequency_num]) * self.max_radius\n",
    "        elif self.freq_init == \"geometric\":\n",
    "            log_timescale_increment = (np.log(float(self.max_radius) / float(self.min_radius)) /(self.frequency_num*1.0 - 1))\n",
    "            timescales = self.min_radius * np.exp(np.arange(self.frequency_num).astype(float) * log_timescale_increment)\n",
    "            self.freq_list = 1.0/timescales\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "    def make_input_embeds(self, coords):\n",
    "        if type(coords) == np.ndarray:\n",
    "            assert self.coord_dim == np.shape(coords)[2]\n",
    "            coords = list(coords)\n",
    "        elif type(coords) == list:\n",
    "            coords = [[c] for c in coords]\n",
    "            assert self.coord_dim == len(coords[0][0])\n",
    "        else:\n",
    "            raise Exception(\"Unknown coords data type for GridCellSpatialRelationEncoder\")\n",
    "\n",
    "        # (batch_size, num_context_pt, coord_dim)\n",
    "        coords_mat = np.asarray(coords).astype(float)\n",
    "        batch_size = coords_mat.shape[0]\n",
    "        num_context_pt = coords_mat.shape[1]\n",
    "\n",
    "        # compute the dot product between [deltaX, deltaY] and each unit_vec \n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat1 = np.expand_dims(np.matmul(coords_mat, self.unit_vec1), axis = -1)\n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat2 = np.expand_dims(np.matmul(coords_mat, self.unit_vec2), axis = -1)\n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat3 = np.expand_dims(np.matmul(coords_mat, self.unit_vec3), axis = -1)\n",
    "\n",
    "        # (batch_size, num_context_pt, 6)\n",
    "        angle_mat = np.concatenate([angle_mat1, angle_mat1, angle_mat2, angle_mat2, angle_mat3, angle_mat3], axis = -1)\n",
    "        # (batch_size, num_context_pt, 1, 6)\n",
    "        angle_mat = np.expand_dims(angle_mat, axis = -2)\n",
    "        # (batch_size, num_context_pt, frequency_num, 6)\n",
    "        angle_mat = np.repeat(angle_mat, self.frequency_num, axis = -2)\n",
    "        # (batch_size, num_context_pt, frequency_num, 6)\n",
    "        angle_mat = angle_mat * self.freq_mat\n",
    "        # (batch_size, num_context_pt, frequency_num*6)\n",
    "        spr_embeds = np.reshape(angle_mat, (batch_size, num_context_pt, -1))\n",
    "\n",
    "        # make sinuniod function\n",
    "        # sin for 2i, cos for 2i+1\n",
    "        # spr_embeds: (batch_size, num_context_pt, frequency_num*6=input_embed_dim)\n",
    "        spr_embeds[:, :, 0::2] = np.sin(spr_embeds[:, :, 0::2])  # dim 2i\n",
    "        spr_embeds[:, :, 1::2] = np.cos(spr_embeds[:, :, 1::2])  # dim 2i+1\n",
    "        \n",
    "        return spr_embeds.squeeze().tolist()\n",
    "\n",
    "def is_gt(values,val):\n",
    "    out = (values > val).astype(float)\n",
    "    out[values.isna()] = float('nan')\n",
    "    return out\n",
    "\n",
    "def is_lt(values,val):\n",
    "    out = (values < val).astype(float)\n",
    "    out[values.isna()] = float('nan')\n",
    "    return out\n",
    "\n",
    "def scale(values):\n",
    "    with warnings.catch_warnings():\n",
    "        # If a column has all nan then a warning is thrown\n",
    "        # We supress that warning because that can happen\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return StandardScaler().fit_transform(values).tolist()\n",
    "\n",
    "def add1(X):\n",
    "    for x,z in zip(X,np.isnan(X).all(axis=1).astype(int)):\n",
    "        x.append(z)\n",
    "    return X\n",
    "\n",
    "def wins(file_path, timestamps, window_len):\n",
    "    file = open(file_path) if Path(file_path).exists() else nullcontext()\n",
    "    rows = islice(csv.reader(file),1,None) if Path(file_path).exists() else [] #type: ignore\n",
    "\n",
    "    with file:\n",
    "        for timestamp in timestamps:\n",
    "            window = []\n",
    "            for row in rows:\n",
    "                if float(row[0]) < timestamp-window_len: continue\n",
    "                if float(row[0]) >= timestamp: break\n",
    "                data = map(float,row[1:])\n",
    "                window.append(next(data) if len(row) == 2 else tuple(data))\n",
    "            yield window\n",
    "\n",
    "def dems(pid, timestamps):\n",
    "    df = pd.read_csv(f'{data_dir}/Baseline.csv')\n",
    "    i = df[\"pid\"].tolist().index(pid)\n",
    "    return df.to_numpy()[[i]*len(timestamps), 1:].tolist()\n",
    "\n",
    "def cacher(sensor,pid,ts,maker,*args):\n",
    "    filename = f\"{sensor}_{int(sum(ts))}_{args}_{pid}.pkl\"\n",
    "    features = load_feats(filename)\n",
    "    if features: return features\n",
    "    features = maker(pid,ts,*args)\n",
    "    save_feats(filename,features)\n",
    "    return features\n",
    "\n",
    "def hrs(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/HeartRate.csv\", timestamps, secs):\n",
    "        w = list(filter(None,w))\n",
    "        if w: features.append([np.mean(w),np.std(w)])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    assert len(set(map(len,features))) == 1, 'hrs'\n",
    "    return scale(features)\n",
    "\n",
    "def scs1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    if features: return features\n",
    "\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/StepCount.csv\", timestamps, secs):\n",
    "        if len(w)>1: features.append([np.mean(np.diff(w)),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    assert len(set(map(len,features))) == 1, 'scs1'\n",
    "\n",
    "    return scale(features)\n",
    "\n",
    "def scs2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/StepCount.csv\", timestamps, secs):\n",
    "        if len(w)>1: features.append([np.max(w)-np.min(w)])\n",
    "        else: features.append([float('nan')])\n",
    "    assert len(set(map(len,features))) == 1, 'scs2'\n",
    "    return scale(features)\n",
    "\n",
    "def lins1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.var(w,axis=0),*np.percentile([np.linalg.norm(w,axis=1)],q=[10,50,90])])\n",
    "        else: features.append([float('nan')]*6)\n",
    "    assert len(set(map(len,features))) == 1, 'lins1'\n",
    "    return scale(features)\n",
    "\n",
    "def lins2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.var(w,axis=0),*np.percentile([np.linalg.norm(w,axis=1)],q=[10,50,90])])\n",
    "        else: features.append([float('nan')]*6)\n",
    "    assert len(set(map(len,features))) == 1, 'lins2'\n",
    "    return scale(features)\n",
    "\n",
    "def lins3(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([np.mean(np.linalg.norm(w,axis=1)), *np.std(w,axis=0)])\n",
    "        else: features.append([float('nan')]*4)\n",
    "    assert len(set(map(len,features))) == 1, 'lins3'\n",
    "    return scale(features)\n",
    "\n",
    "def lins4(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([np.mean(np.linalg.norm(w,axis=1)), *np.std(w,axis=0)])\n",
    "        else: features.append([float('nan')]*4)\n",
    "    assert len(set(map(len,features))) == 1, 'lins2'\n",
    "    return scale(features)\n",
    "\n",
    "def bats1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Battery.csv\", timestamps, secs):\n",
    "        w = [float(w)/100 for w in w]\n",
    "        if len(w)==1: features.append([0,float('nan'),float('nan')])\n",
    "        elif len(w)>1: features.append([np.max(w)-np.min(w),np.mean(np.diff(w)),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*3)\n",
    "        assert len(set(map(len,features))) == 1, 'bats1'\n",
    "    return features\n",
    "\n",
    "def bats2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Battery.csv\", timestamps, secs):\n",
    "        w = [float(w)/100 for w in w]\n",
    "        if w: features.append([np.mean(w),np.max(w)-np.min(w)])\n",
    "        else: features.append([float('nan')]*2)\n",
    "        assert len(set(map(len,features))) == 1, 'bats2'\n",
    "    return features\n",
    "\n",
    "def peds1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Pedometer.csv\", timestamps, secs):\n",
    "        if len(w)==1: features.append([float('nan'),0,float('nan')])\n",
    "        elif len(w)>1: features.append([np.mean(np.diff(w)),np.max(w)-np.min(w),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*3)\n",
    "        assert len(set(map(len,features))) == 1, 'peds1'\n",
    "    return scale(features)\n",
    "\n",
    "def peds2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Pedometer.csv\", timestamps, secs):\n",
    "        if len(w)>1: features.append([np.max(w)-np.min(w)])\n",
    "        else: features.append([float('nan')])\n",
    "        assert len(set(map(len,features))) == 1, 'peds2'\n",
    "    return scale(features)\n",
    "\n",
    "def locs1(pid, timestamps, secs, freq, lmin, lmax):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Location.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.mean(w,axis=0)[1:]])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    out = TheoryGridCellSpatialRelationEncoder(frequency_num=freq,min_radius=lmin,max_radius=lmax,freq_init='geometric').make_input_embeds(features)\n",
    "    return [out] if len(timestamps) == 1 else out\n",
    "\n",
    "def locs2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Location.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.mean(w,axis=0)[1:]])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    return features\n",
    "\n",
    "def tims(timestamps,tzs):\n",
    "    hour, day = 60*60, 60*60*24\n",
    "    for timestamp,tz in zip(timestamps,tzs):\n",
    "        if np.isnan(timestamp): \n",
    "            yield [float('nan')]*3\n",
    "        else:\n",
    "            if   tz == \"-04:00\": timestamp -= 4*hour\n",
    "            elif tz == \"-05:00\": timestamp -= 5*hour\n",
    "            time_of_day = (timestamp/day) % 1\n",
    "            day_of_week = (int(timestamp/day)+4) % 7\n",
    "            is_weekend = day_of_week in [0,6]\n",
    "            is_weekday = day_of_week in [1,2,3,4,5]\n",
    "            yield [time_of_day,int(is_weekend),int(is_weekday)]\n",
    "\n",
    "def save_feats(filename,feats):\n",
    "    if not Path(f\"{data_dir}/feats/{filename}\").exists():\n",
    "        with open(f\"{data_dir}/feats/{filename}\", \"wb\") as f: # Use \"wb\" for binary write mode\n",
    "            pickle.dump(feats, f)\n",
    "\n",
    "def load_feats(filename):\n",
    "    if not Path(f\"{data_dir}/feats/{filename}\").exists():\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            with open(f\"{data_dir}/feats/{filename}\", \"rb\") as f: # Use \"wb\" for binary write mode\n",
    "                return pickle.load(f)\n",
    "        except:\n",
    "            Path(f\"{data_dir}/feats/{filename}\").unlink()\n",
    "            return None\n",
    "\n",
    "def make_xyg2(work_item):\n",
    "    (pid,ts,tz,ys,args,secs) = work_item\n",
    "\n",
    "    fs = []\n",
    "\n",
    "    if secs == None:\n",
    "        if args[1]: fs.append(list(cacher(\"hrs\"  ,pid,ts,hrs  ,args[1])))\n",
    "        if args[2]: fs.append(list(cacher(\"scs2\" ,pid,ts,scs2 ,args[2])))\n",
    "        if args[3]: fs.append(list(cacher(\"lins3\",pid,ts,lins3,args[3])))\n",
    "        if args[4]: fs.append(list(cacher(\"lins4\",pid,ts,lins4,args[4])))\n",
    "        if args[5]: fs.append(list(cacher(\"bats2\",pid,ts,bats2,args[5])))\n",
    "        if args[6]: fs.append(list(cacher(\"peds2\",pid,ts,peds2,args[6])))\n",
    "        if args[7]: fs.append(list(cacher(\"locs1\",pid,ts,locs1,*args[7])))\n",
    "\n",
    "        if args[9]:\n",
    "            for f in fs: add1(f)\n",
    "\n",
    "        if args[0]: fs.append(list(tims(ts,tz)))\n",
    "        if args[8]: fs.append(list(dems(pid,ts)))\n",
    "\n",
    "        _xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "        _ys = list(ys)\n",
    "        _gs = [pid]*len(ys)\n",
    "    \n",
    "    else:\n",
    "        _xs,_ys,_gs = [],[],[]\n",
    "        for sec in (secs or []):\n",
    "            nts = [t+sec for t in ts]\n",
    "            nxs,nys,ngs = make_xyg2((pid,nts,tz,ys,args,None))\n",
    "            _xs += nxs\n",
    "            _ys += nys\n",
    "            _gs += ngs\n",
    "\n",
    "    return _xs,_ys,_gs\n",
    "\n",
    "can_predict = emotions_df.copy().sort_values([\"ParticipantId\",\"DeliveredTimestampUtc\"])\n",
    "\n",
    "def work_items(tims:bool,hrs:int,scs:int,lins1:int,lins2:int,bats:int,peds:int,locs1,dems:bool,add1:bool,event:str,secs=[0]):\n",
    "\n",
    "    df = can_predict[~can_predict[\"SubmissionTimestampUtc\"].isna()]\n",
    "\n",
    "    if not isinstance(secs,(list,tuple)): secs = [secs]\n",
    "\n",
    "    for pid in sorted(df[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        ptc  = df[df.ParticipantId == pid]\n",
    "        tss  = ptc[\"SubmissionTimestampUtc\" if event == \"sub\" else \"DeliveredTimestampUtc\"].tolist() \n",
    "        tzs  = ptc[\"LocalTimeZone\"].tolist()\n",
    "\n",
    "        y0s = torch.tensor(is_gt(ptc[\"ER Interest (z)\"],0).tolist())\n",
    "        y1s = torch.tensor(is_lt(ptc['Response Time (min)'],10).tolist())\n",
    "        y2s = torch.tensor(is_gt(ptc[\"State Anxiety (z)\"],0).tolist())\n",
    "        y3s = torch.tensor(is_gt(ptc[\"State Anxiety\"], 1).tolist())\n",
    "        y4s = torch.tensor(ptc[\"Response Time (log sec)\"].tolist())\n",
    "        y5s = torch.tensor(ptc[\"ER Interest (z)\"].tolist())\n",
    "        y6s = torch.tensor(ptc[\"State Anxiety (z)\"].tolist())\n",
    "\n",
    "        ys = torch.hstack((\n",
    "            y0s.unsqueeze(1),\n",
    "            y1s.unsqueeze(1),\n",
    "            y2s.unsqueeze(1),\n",
    "            y3s.unsqueeze(1),\n",
    "            y4s.unsqueeze(1),\n",
    "            y5s.unsqueeze(1),\n",
    "            y6s.unsqueeze(1)\n",
    "        )).tolist()\n",
    "\n",
    "        args = [tims,hrs,scs,lins1,lins2,bats,peds,locs1,dems,add1]\n",
    "\n",
    "        yield pid,tss,tzs,ys,args,secs\n",
    "\n",
    "#with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "X,Y,G = zip(*map(make_xyg2, work_items(True,0,0,300,0,300,300,[300,2,1,2],False,True,\"del\")))\n",
    "\n",
    "Y = torch.tensor(list(chain.from_iterable(Y))).float()\n",
    "G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "G = G[~torch.isnan(Y[:,[0,1]]).any(dim=1)]\n",
    "\n",
    "testable_G = cb.CobaRandom(1).shuffle([k for k,n in Counter(G.tolist()).items() if n >= 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88788c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "send, nosend = (1,0), (0,1)\n",
    "\n",
    "class VWR:\n",
    "    def __init__(self, args):\n",
    "        self._args = args\n",
    "        self.params = {'vwr_args':args}\n",
    "        self.vw = cb.VowpalMediator()\n",
    "        self.is_logistic = \"logistic\" in args\n",
    "    def predict(self, features):\n",
    "        if not self.vw.is_initialized: self.vw.init_learner(self._args, 1)\n",
    "        return self.vw.predict(self.vw.make_example({\"x\":features})) #type: ignore\n",
    "    def learn(self, features, value, prob):\n",
    "        if not self.vw.is_initialized: self.vw.init_learner(self._args, 1)\n",
    "        if self.is_logistic:\n",
    "            assert value in [0,1]\n",
    "            value = -1 if value == 0 else 1\n",
    "        self.vw.learn(self.vw.make_example({\"x\":features}, f\"{value} {1/prob}\")) #type: ignore\n",
    "\n",
    "class MyLearner1:\n",
    "    def __init__(self, vwr: VWR, gamma, gamma_update):\n",
    "        self.rng = cb.CobaRandom()\n",
    "        self.vwr = vwr\n",
    "        self.t = 0\n",
    "        self.gamma = gamma\n",
    "        self.gamma_update = gamma_update\n",
    "        self.params = {**vwr.params,\"gamma\":gamma,\"gamma_up\":gamma_update.__name__}\n",
    "\n",
    "    def predict(self, context, actions):\n",
    "        gamma = self.gamma + self.gamma_update(self.t)\n",
    "        p1 = 1/(1+np.exp(-self.vwr.predict(context)))\n",
    "        p0 = 1-p1\n",
    "\n",
    "        minp = 1/(2+gamma*(abs(p1-p0)))\n",
    "\n",
    "        if (self.rng.random() < minp):\n",
    "            return (actions[0] if p1 <= p0 else actions[1]),minp\n",
    "        else:\n",
    "            return (actions[0] if p1 >= p0 else actions[1]),1-minp\n",
    "\n",
    "    def learn(self, context, action, reward, probability):\n",
    "        if action == (1,0):\n",
    "            self.t+=1\n",
    "            assert reward in [0,1]\n",
    "            self.vwr.learn(context,reward,probability)\n",
    "\n",
    "class MyLearner2:\n",
    "    \"\"\"An implementation of equation 7 and algorithm 1 in https://openreview.net/forum?id=l6pYRbuHpO\"\"\"\n",
    "\n",
    "    def __init__(self, vwr: VWR, gamma, gamma_update):\n",
    "        self.vwr = vwr\n",
    "        self.rng = cb.CobaRandom()\n",
    "        self.t = 0\n",
    "        self.gamma=gamma\n",
    "        self.gamma_update = gamma_update\n",
    "        self.params = {**vwr.params,\"gamma\":gamma,\"gamma_up\":gamma_update.__name__}\n",
    "\n",
    "    def predict(self,context,actions):\n",
    "        gamma = self.gamma + self.gamma_update(self.t)\n",
    "\n",
    "        f1 = 1/(1+np.exp(-self.vwr.predict(context)))\n",
    "        f2 = 1-f1\n",
    "\n",
    "        #Invert the values so our equations exactly match eq(7)\n",
    "        #in the original paper. This reduces the chance for bugs.\n",
    "        f1 = -f1\n",
    "        f2 = -f2\n",
    "\n",
    "        p1 = 1 if f1 <= f2 else 1/(4+gamma*(f1-f2))\n",
    "        p2 = 1-p1\n",
    "\n",
    "        if self.rng.random() <= p1:\n",
    "            return actions[0],p1\n",
    "        else:\n",
    "            return actions[1],p2\n",
    "\n",
    "    def learn(self,context,action,reward,probability):\n",
    "        if action == (1,0):\n",
    "            self.t+=1\n",
    "            assert reward in [0,1]\n",
    "            self.vwr.learn(context,reward,probability)\n",
    "\n",
    "class MethodR:\n",
    "    required_pretrain = []\n",
    "    def __init__(self):\n",
    "        self._cb_learner = cb.SafeLearner(cb.RandomLearner())\n",
    "\n",
    "    def set_s(self,s):\n",
    "        self._cb_learner.learner._rng = cb.CobaRandom(self._cb_learner.learner._rng.seed+s)\n",
    "\n",
    "    def set_t(self,t):\n",
    "        pass\n",
    "\n",
    "    def predict(self,x0,x1,x2):\n",
    "        return self._cb_learner.predict(x2, [send, nosend])\n",
    "\n",
    "    def learn(self,x0,x1,x2,y0,y1,y2,a,p,**kwargs):\n",
    "        pass\n",
    "\n",
    "class MethodF:\n",
    "    required_pretrain = []\n",
    "    def __init__(self, pmf):\n",
    "        self._cb_learner = cb.SafeLearner(cb.FixedLearner(pmf))\n",
    "        self.params = {\"pmf\": pmf}\n",
    "\n",
    "    def set_s(self,s):\n",
    "        self._cb_learner.learner._pred._pmfrng = cb.CobaRandom(self._cb_learner.learner._pred._pmfrng.seed+s)\n",
    "\n",
    "    def set_t(self,t):\n",
    "        pass\n",
    "\n",
    "    def predict(self,x0,x1,x2):\n",
    "        return self._cb_learner.predict(x0, [send, nosend])\n",
    "\n",
    "    def learn(self,x0,x1,x2,y0,y1,y2,a,p,**kwargs):\n",
    "        pass\n",
    "\n",
    "class Method0:\n",
    "    required_pretrain = [2]\n",
    "    def __init__(self, cb_learner):\n",
    "        from copy import deepcopy\n",
    "        self._cb_learner = deepcopy(cb.SafeLearner(cb_learner))\n",
    "        self.params = {\"method\":0, **cb_learner.params}\n",
    "\n",
    "    def set_s(self,s):\n",
    "        pass\n",
    "\n",
    "    def set_t(self,t):\n",
    "        try:\n",
    "            self._cb_learner.t = t #type: ignore\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def predict(self,x0,x1,x2):\n",
    "        return self._cb_learner.predict(x2, [send, nosend])\n",
    "\n",
    "    def learn(self,x0,x1,x2,y0,y1,y2,a,p,**kwargs):\n",
    "        r = y2 if a==send else .5\n",
    "        self._cb_learner.learn(x2,a,r,p,**kwargs)\n",
    "\n",
    "class Method1:\n",
    "    \"\"\"\n",
    "    train a cb learner for y0 and y1 then pick send if they both choose send\n",
    "    \"\"\"\n",
    "    required_pretrain = [0,1]\n",
    "    def __init__(self, cb_learner):\n",
    "        from copy import deepcopy\n",
    "        self._cb_learner0 = deepcopy(cb.SafeLearner(cb_learner))\n",
    "        self._cb_learner1 = deepcopy(cb.SafeLearner(cb_learner))\n",
    "        self.params = {\"method\":1, **cb_learner.params}\n",
    "\n",
    "    def set_s(self,s):\n",
    "        pass\n",
    "\n",
    "    def set_t(self,t):\n",
    "        try:\n",
    "            self._cb_learner0.t = t #type: ignore\n",
    "            self._cb_learner1.t = t #type: ignore\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def predict(self,x0,x1,x2):\n",
    "        a0,p0,o0 = self._cb_learner0.predict(x0, [send, nosend])\n",
    "        a1,p1,o1 = self._cb_learner1.predict(x1, [send, nosend])\n",
    "        p0send = p0 if a0 == send else 1-p0\n",
    "        p1send = p1 if a1 == send else 1-p1\n",
    "        a = send if a0==send and a1==send else nosend\n",
    "        p = p0send*p1send if a == send else 1-p0send*p1send\n",
    "        return a,p,{\"o0\":o0,\"o1\":o1}\n",
    "\n",
    "    def learn(self,x0,x1,x2,y0,y1,y2,a,p,**kwargs):\n",
    "        r0 = y0 if a==send else .5\n",
    "        r1 = y1 if a==send else .5\n",
    "        if not np.isnan(r0): self._cb_learner0.learn(x0,a,r0,p,**kwargs.get(\"o0\",{}))\n",
    "        self._cb_learner1.learn(x1,a,r1,p,**kwargs.get(\"o1\",{}))\n",
    "\n",
    "class Method2:\n",
    "    \"\"\"\n",
    "    train a regression learner for y0 and y1 and then pass these in as features to cb_learner\n",
    "    \"\"\"\n",
    "    required_pretrain = [0,1,2]\n",
    "    def __init__(self, reg_learner: VWR, cb_learner):\n",
    "        from copy import deepcopy\n",
    "        self._reg_learner0 = deepcopy(reg_learner)\n",
    "        self._reg_learner1 = deepcopy(reg_learner)\n",
    "        self._cb_learner   = deepcopy(cb.SafeLearner(cb_learner))\n",
    "        self.params = {\"method\":2, **cb_learner.params, **reg_learner.params}\n",
    "\n",
    "    def set_s(self,s):\n",
    "        pass\n",
    "\n",
    "    def set_t(self,t):\n",
    "        try:\n",
    "            self._cb_learner.t = t #type: ignore\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def make_feats(self,x0,x1,x2):\n",
    "        p0 = self._reg_learner0.predict(x0)\n",
    "        p1 = self._reg_learner1.predict(x1)\n",
    "        return [*x2,p0,p1,p0*p1]\n",
    "\n",
    "    def predict(self,x0,x1,x2):\n",
    "        return self._cb_learner.predict(self.make_feats(x0,x1,x2), [send, nosend])\n",
    "\n",
    "    def learn(self,x0,x1,x2,y0,y1,y2,a,p,**kwargs):\n",
    "        r = y2 if a==send else .5\n",
    "        self._cb_learner.learn(self.make_feats(x0,x1,x2),a,r,p,**kwargs)\n",
    "        if not np.isnan(y0): self._reg_learner0.learn(x0,y0,p)\n",
    "        self._reg_learner1.learn(x1,y1,p)\n",
    "\n",
    "class Method3:\n",
    "    required_pretrain = [0,1]\n",
    "    def __init__(self, reg_learner:VWR, gamma, gamma_update):\n",
    "        from copy import deepcopy\n",
    "        self._reg_learner0 = deepcopy(reg_learner)\n",
    "        self._reg_learner1 = deepcopy(reg_learner)\n",
    "        self.t = 0\n",
    "        self.gamma = gamma\n",
    "        self.gamma_update = gamma_update\n",
    "        self.rng = cb.CobaRandom(1)\n",
    "        self.params = {\"method\":3, **reg_learner.params, \"gamma\": gamma, \"gamma_up\":gamma_update.__name__}\n",
    "\n",
    "    def set_s(self,s):\n",
    "        pass\n",
    "\n",
    "    def set_t(self,t):\n",
    "        self.t = t\n",
    "\n",
    "    def predict(self,x0,x1,x2):\n",
    "        gamma = self.gamma + self.gamma_update(self.t)\n",
    "\n",
    "        p0 = self._reg_learner0.predict(x0)\n",
    "        p1 = self._reg_learner1.predict(x1)\n",
    "\n",
    "        f1 = p0*p1\n",
    "        f2 = 1-f1\n",
    "\n",
    "        #Invert the values so our equations exactly match eq(7)\n",
    "        #in the original paper. This reduces the chance for bugs.\n",
    "        f1 = -f1\n",
    "        f2 = -f2\n",
    "\n",
    "        p1 = 1 if f1 <= f2 else 1/(4+gamma*(f1-f2))\n",
    "        p2 = 1-p1\n",
    "\n",
    "        if self.rng.random() <= p1:\n",
    "            return send,p1\n",
    "        else:\n",
    "            return nosend,p2\n",
    "\n",
    "    def learn(self,x0,x1,x2,y0,y1,y2,a,p):\n",
    "        if a == send:\n",
    "            if not np.isnan(y0): self._reg_learner0.learn(x0,y0,p)\n",
    "            self._reg_learner1.learn(x1,y1,p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869cb2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    class ForceOneModule(torch.nn.Module):\n",
    "        def forward(self,X):\n",
    "            return torch.ones(size=(X.shape[0],1)).float()\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if spec == '1':\n",
    "            return FeedForward.ForceOneModule(), 1\n",
    "        if isinstance(spec,list):\n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, a0, a1, a2, g, rng):\n",
    "        self.params = {'trn0':a0, 'trn1':a1, 'trn2':a2, 'g':g, 'rng': rng}\n",
    "        self.X0 = None\n",
    "        self.X1 = None\n",
    "        self.X2 = None\n",
    "        self.Y  = None\n",
    "        self.G  = None\n",
    "        self.a0 = list(a0)\n",
    "        self.a1 = list(a1)\n",
    "        self.a2 = list(a2)\n",
    "        self.g = g\n",
    "        self.a0[7] = [a0[7],2,1,10] if a0[7] else None\n",
    "        self.a1[7] = [a1[7],2,1,10] if a1[7] else None\n",
    "        self.a2[7] = [a2[7],2,1,10] if a2[7] else None\n",
    "\n",
    "    def get_data(self):\n",
    "        import torch\n",
    "        import itertools as it\n",
    "\n",
    "        if self.X0 is not None: return self.X0,self.X1,self.X2,self.Y,self.G\n",
    "\n",
    "        X0,Y,G = zip(*map(make_xyg2, work_items(*self.a0)))\n",
    "        X1,Y,G = zip(*map(make_xyg2, work_items(*self.a1)))\n",
    "        X2,Y,G = zip(*map(make_xyg2, work_items(*self.a2)))\n",
    "\n",
    "        X0 = torch.tensor(list(it.chain.from_iterable(X0))).float()\n",
    "        X1 = torch.tensor(list(it.chain.from_iterable(X1))).float()\n",
    "        X2 = torch.tensor(list(it.chain.from_iterable(X2))).float()\n",
    "        Y  = torch.tensor(list(it.chain.from_iterable(Y ))).float()\n",
    "        G  = torch.tensor(list(it.chain.from_iterable(G ))).int()\n",
    "\n",
    "        self.X0,self.X1,self.X2,self.Y,self.G = X0,X1,X2,Y,G\n",
    "\n",
    "        if X0.shape[0] == 0: return\n",
    "\n",
    "        all_na = torch.isnan(Y[:,[0,1]]).all(dim=1)\n",
    "        X0 = X0[~all_na]\n",
    "        X1 = X1[~all_na]\n",
    "        X2 = X2[~all_na]\n",
    "        Y  = Y[~all_na].float()\n",
    "        G  = G[~all_na]\n",
    "\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(X0)))\n",
    "\n",
    "        self.X0,self.X1,self.X2,self.Y,self.G = X0[rng_indexes],X1[rng_indexes],X2[rng_indexes],Y[rng_indexes],G[rng_indexes]\n",
    "\n",
    "        return self.X0,self.X1,self.X2,self.Y,self.G\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ssl_samps, ssl_negs, ssl_tau, ssl_v, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae\n",
    "        self.s2  = s2  #ssl\n",
    "        self.s3  = s3  #basis\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ssl_samps = ssl_samps\n",
    "        self.ssl_negs  = ssl_negs\n",
    "        self.ssl_tau   = ssl_tau\n",
    "        self.ssl_v     = ssl_v\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        \n",
    "        self.learn  = (pers_lrn_cnt > 0)\n",
    "        self.replay = (pers_lrn_cnt > 0) and (pers_mem_cnt > 0)\n",
    "\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2': s2, 's3': s3, 'dae': (dae_steps,dae_dropn), 'ssl': (ssl_samps,ssl_negs,ssl_tau,ssl_v), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl), 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weights(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        def get_trn_tst(G,g):\n",
    "            is_tst = sum(G == i for i in g).bool() #type: ignore\n",
    "            return ~is_tst, is_tst\n",
    "\n",
    "        def sslv1(X,Y,G,nsamps,nnegs):\n",
    "            from itertools import compress, repeat, chain\n",
    "            from operator import eq, ne\n",
    "\n",
    "            rng = cb.CobaRandom(1)\n",
    "            rng_order = rng.shuffle(range(len(X)))\n",
    "\n",
    "            X = X.tolist()\n",
    "            Y = list(map(tuple,Y.tolist()))\n",
    "            G = G.tolist()\n",
    "\n",
    "            X = list(map(X.__getitem__,rng_order))\n",
    "            Y = list(map(Y.__getitem__,rng_order))\n",
    "            G = list(map(G.__getitem__,rng_order))\n",
    "\n",
    "            Xg = {}\n",
    "            Yg = {}\n",
    "\n",
    "            for g in set(G):\n",
    "                Xg[g] = list(compress(X,map(eq,G,repeat(g))))\n",
    "                Yg[g] = list(compress(Y,map(eq,G,repeat(g))))\n",
    "\n",
    "            eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "            ne_class  = {y: list(compress(X,map(ne,Y,repeat(y)))) for y in set(Y)}\n",
    "\n",
    "            def choose_unique(items,given_i):\n",
    "                if len(items) == 1:  return items[0]\n",
    "                for i in rng.randints(100,0,len(items)-1):\n",
    "                    if i != given_i:\n",
    "                        return items[i]\n",
    "\n",
    "            def choose_n(items,n):\n",
    "                add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "                for i in rng.randints(100,0,len(items)-1):\n",
    "                    add_to_index(i)\n",
    "                    if len(indexes)==n:\n",
    "                        return [items[i] for i in indexes]\n",
    "\n",
    "            for i in range(nsamps):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for g in set(G):\n",
    "                    x,y = Xg[g][i%len(Xg[g])],Yg[g][i%len(Yg[g])]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],nnegs))\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        def sslv2(X,Y,G,nsamps,nnegs):\n",
    "            from itertools import compress, repeat, chain\n",
    "            from operator import eq, ne\n",
    "\n",
    "            rng = cb.CobaRandom(1)\n",
    "            rng_order = rng.shuffle(range(len(X)))\n",
    "\n",
    "            X = X.tolist()\n",
    "            Y = list(map(tuple,Y.tolist()))\n",
    "            G = G.tolist()\n",
    "\n",
    "            X = list(map(X.__getitem__,rng_order))\n",
    "            Y = list(map(Y.__getitem__,rng_order))\n",
    "            G = list(map(G.__getitem__,rng_order))\n",
    "\n",
    "            Xg = {}\n",
    "            Yg = {}\n",
    "            eq_class = {}\n",
    "            ne_class = {}\n",
    "\n",
    "            for g in set(G):\n",
    "                _X = list(compress(X,map(eq,G,repeat(g))))\n",
    "                _Y = list(compress(Y,map(eq,G,repeat(g))))\n",
    "\n",
    "                Xg[g] = _X\n",
    "                Yg[g] = _Y\n",
    "\n",
    "                eq_class[g] = {y: list(compress(_X,map(eq,_Y,repeat(y)))) for y in set(_Y)}\n",
    "                ne_class[g] = {y: list(compress(_X,map(ne,_Y,repeat(y)))) for y in set(_Y)}\n",
    "\n",
    "            def choose_unique(items,given_i):\n",
    "                if len(items) == 1:  return items[0]\n",
    "                for i in rng.randints(100,0,len(items)-1):\n",
    "                    if i != given_i:\n",
    "                        return items[i]\n",
    "\n",
    "            def choose_n(items,n):\n",
    "                add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "                for i in rng.randints(100,0,len(items)-1):\n",
    "                    add_to_index(i)\n",
    "                    if len(indexes)==n:\n",
    "                        return [items[i] for i in indexes]\n",
    "\n",
    "            for i in range(nsamps):\n",
    "                anchor, positive, negative = [], [], []    \n",
    "                for g in rng.shuffle(set(G)):\n",
    "                    if len(set(Yg[g])) == 1: continue\n",
    "\n",
    "                    x,y = Xg[g][i%len(Xg[g])],Yg[g][i%len(Yg[g])]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[g][y],i%len(Yg[g])))\n",
    "                    negative.append(choose_n     (ne_class[g][y],nnegs))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        def replace_x(layers,n_feats,n_persons):\n",
    "            new_layers = []\n",
    "            for l in layers:\n",
    "                if l == 'x': new_layers.append(n_feats)\n",
    "                elif l == '-x': new_layers.append(n_feats*n_persons)\n",
    "                elif isinstance(l,(tuple,list)): new_layers.append(replace_x(l,n_feats,n_persons))\n",
    "                else: new_layers.append(l)\n",
    "            return new_layers\n",
    "\n",
    "        def remove_na(X,Y,G=None):\n",
    "            any_na = torch.isnan(Y)\n",
    "            if Y.ndim==2: any_na = any_na.any(dim=1)\n",
    "            if G == None: return X[~any_na],Y[~any_na]\n",
    "            else: return X[~any_na],Y[~any_na],G[~any_na]\n",
    "\n",
    "        def pretrain(trn_X,trn_Y,trn_G):\n",
    "            n_feats = trn_X.shape[1]\n",
    "            n_persons = len(set(trn_G.tolist()))\n",
    "            n_tasks = trn_Y.shape[1]\n",
    "\n",
    "            _s1 = replace_x(self.s1,n_feats,n_persons)\n",
    "            _s2 = replace_x(self.s2,n_feats,n_persons)\n",
    "            _s3 = replace_x(self.s3,n_feats,n_persons)\n",
    "\n",
    "            _s1 = [n_tasks if f == 'y' else f for f in _s1]\n",
    "            _s2 = [n_tasks if f == 'y' else f for f in _s2]\n",
    "            _s3 = [n_tasks if f == 'y' else f for f in _s3]\n",
    "\n",
    "            if _s3 and _s3[-1] == -1: _s3 = (*(_s3)[:-1], n_persons*n_tasks)\n",
    "\n",
    "            s1 = FeedForward(_s1)\n",
    "            s2 = FeedForward(_s2)\n",
    "            s3 = FeedForward(_s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s3_children = list(s3.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s3_children[len(s3_children)-self.ws_drop0:])\n",
    "            s3 = torch.nn.Sequential(*s3_children[:len(s3_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,s3,sb]\n",
    "            opts = [s1opt,saopt,s2opt,s3opt,sbopt]\n",
    "\n",
    "            for m in mods: m.train()\n",
    "\n",
    "            [s1,sa,s2,s3,sb] = mods\n",
    "            [s1opt,saopt,s2opt,s3opt,sbopt] = opts\n",
    "\n",
    "            if _s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,G,W = trn_X,trn_G,make_weights(trn_G)\n",
    "\n",
    "                if _s1[-1] != n_feats*n_persons:\n",
    "                    Z = X\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                    R = torch.arange(len(X)).unsqueeze(1)\n",
    "                    Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                    Z[R,I] = X\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_z,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ssl_samps:\n",
    "                X, Y, G = trn_X,trn_Y,trn_G\n",
    "\n",
    "                if self.ssl_negs == 0: raise Exception(\"neg can't be 0\")\n",
    "                if self.ssl_tau == 0: raise Exception(\"Tau can't be 0\")\n",
    "\n",
    "                ssl = sslv1 if self.ssl_v == 1 else sslv2\n",
    "\n",
    "                for A,P,N in ssl(X, Y, G, self.ssl_samps, self.ssl_negs):\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(A,P,N)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=4,drop_last=False,shuffle=True)\n",
    "\n",
    "                    for _A, _P, _N in torch_loader:\n",
    "                        #https://arxiv.org/pdf/2002.05709\n",
    "                        _A = s2(s1(_A.nan_to_num()))\n",
    "                        _P = s2(s1(_P.nan_to_num()))\n",
    "                        _N = s2(s1(_N.nan_to_num()))\n",
    "\n",
    "                        p = torch.einsum(\"bi,bi->b\",_A,_P)\n",
    "                        n = torch.einsum(\"bi,bji->bj\",_A,_N)\n",
    "\n",
    "                        p /= (torch.linalg.norm(_A,dim=1)*torch.linalg.norm(_P,dim=1))\n",
    "                        n /= (torch.linalg.norm(_A,dim=1).unsqueeze(1)*torch.linalg.norm(_N,dim=2))\n",
    "\n",
    "                        p = torch.exp(p/self.ssl_tau)\n",
    "                        n = torch.exp(n/self.ssl_tau)\n",
    "\n",
    "                        if s2opt: s2opt.zero_grad()\n",
    "                        (-torch.log(p/(p+n.sum(dim=1)))).mean().backward()\n",
    "                        if s2opt: s2opt.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,s3opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                if _s3[-1] != n_tasks*n_persons:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_tasks + torch.arange(n_tasks).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*n_tasks), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s3(s2(s1(_X.nan_to_num()))))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            for m in mods: m.eval()\n",
    "\n",
    "            return lambda x: s3(s2(s1(x.nan_to_num())))\n",
    "\n",
    "        def get_scores(env):\n",
    "            from copy import deepcopy\n",
    "            from statistics import mean\n",
    "\n",
    "            X0,X1,X2,Y,G = env.get_data()\n",
    "\n",
    "            g = env.g\n",
    "            Y0 = Y[:,[0  ]]\n",
    "            Y1 = Y[:,[  1]]\n",
    "            Y2 = Y[:,[0,1]]\n",
    "\n",
    "            torch.manual_seed(env.params['rng'])\n",
    "\n",
    "            is_trn, is_tst = get_trn_tst(G,g)\n",
    "\n",
    "            if sum(is_tst) < 20: return\n",
    "\n",
    "            trn_X0, trn_X1, trn_X2 = X0[is_trn], X1[is_trn], X2[is_trn]\n",
    "            trn_Y0, trn_Y1, trn_Y2 = Y0[is_trn], Y1[is_trn], Y2[is_trn]\n",
    "\n",
    "            trn_G = G[is_trn]\n",
    "\n",
    "            tst_X0, tst_X1, tst_X2 = X0[is_tst], X1[is_tst], X2[is_tst]\n",
    "            tst_Y0, tst_Y1, tst_Y2 = Y0[is_tst], Y1[is_tst], Y2[is_tst]\n",
    "\n",
    "            pretrain0 = lambda x:x\n",
    "            pretrain1 = lambda x:x\n",
    "            pretrain2 = lambda x:x\n",
    "\n",
    "            if 0 in lrn.required_pretrain: pretrain0 = pretrain(*remove_na(trn_X0,trn_Y0,trn_G))\n",
    "            if 1 in lrn.required_pretrain: pretrain1 = pretrain(*remove_na(trn_X1,trn_Y1,trn_G))\n",
    "            if 2 in lrn.required_pretrain: pretrain2 = pretrain(*remove_na(trn_X2,trn_Y2,trn_G))\n",
    "\n",
    "            rng = cb.CobaRandom(seed=env.params['rng'])\n",
    "            rng_indexes = rng.shuffle(range(len(tst_X0)))\n",
    "\n",
    "            X0,X1,X2 = tst_X0[rng_indexes], tst_X1[rng_indexes], tst_X2[rng_indexes]\n",
    "            Y0,Y1,Y2 = tst_Y0[rng_indexes], tst_Y1[rng_indexes], tst_Y2[rng_indexes]\n",
    "\n",
    "            inconclusive = (Y0.isnan() & (Y1==1)).squeeze()\n",
    "\n",
    "            X0 = X0[~inconclusive]\n",
    "            X1 = X1[~inconclusive]\n",
    "            X2 = X2[~inconclusive]\n",
    "            Y0 = Y0[~inconclusive]\n",
    "            Y1 = Y1[~inconclusive]\n",
    "            Y2 = Y2[~inconclusive]\n",
    "\n",
    "            Y0 = Y0.squeeze()\n",
    "            Y1 = Y1.squeeze()\n",
    "            Y2 = (Y2==1).all(dim=1).float()\n",
    "\n",
    "            send, nosend = (1,0), (0,1)\n",
    "\n",
    "            scores = []\n",
    "            for j in range(30):\n",
    "\n",
    "                rng = cb.CobaRandom(seed=j+env.params['rng'])\n",
    "                rng_indexes = rng.shuffle(range(len(X0)))\n",
    "\n",
    "                X0,X1,X2 = X0[rng_indexes], X1[rng_indexes], X2[rng_indexes]\n",
    "                Y0,Y1,Y2 = Y0[rng_indexes], Y1[rng_indexes], Y2[rng_indexes]\n",
    "\n",
    "                lrn_ = deepcopy(lrn)\n",
    "\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    rng_indexes = rng.shuffle(range(len(trn_X2)))\n",
    "\n",
    "                    X0_, Y0_ = trn_X0[rng_indexes], trn_Y0[rng_indexes]\n",
    "                    X1_, Y1_ = trn_X1[rng_indexes], trn_Y1[rng_indexes]\n",
    "                    X2_, Y2_ = trn_X2[rng_indexes], trn_Y2[rng_indexes]\n",
    "\n",
    "                    inconclusive = (Y2_[:,0].isnan() & (Y2_[:,1]==1)).squeeze()\n",
    "\n",
    "                    X0_, Y0_ = X0_[~inconclusive].nan_to_num(), Y0_[~inconclusive].squeeze()\n",
    "                    X1_, Y1_ = X1_[~inconclusive].nan_to_num(), Y1_[~inconclusive].squeeze()\n",
    "                    X2_, Y2_ = X2_[~inconclusive].nan_to_num(), Y2_[~inconclusive]\n",
    "\n",
    "                    X0_, Y0_ = pretrain0(X0_).tolist(), Y0_.tolist()\n",
    "                    X1_, Y1_ = pretrain1(X1_).tolist(), Y1_.tolist()\n",
    "                    X2_, Y2_ = pretrain2(X2_).tolist(), (Y2_==1).all(dim=1).float().tolist()\n",
    "\n",
    "                    for x0,x1,x2,y0,y1,y2 in zip(X0_,X1_,X2_,Y0_,Y1_,Y2_):\n",
    "                        lrn_.learn(x0,x1,x2,y0,y1,y2,send,1)\n",
    "\n",
    "                lrn_.set_s(env.params['rng']+j)\n",
    "                lrn_.set_t(0)\n",
    "\n",
    "                tp,fp,tn,fn = 0,0,0,0\n",
    "\n",
    "                mems = []\n",
    "                bits = []\n",
    "\n",
    "                scores.append([])\n",
    "                for i in range(min([len(Y0),45])):\n",
    "\n",
    "                    a,p,y = None,None,None\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        x0, y0  = pretrain0(X0[i].nan_to_num()).tolist(), float(Y0[i])\n",
    "                        x1, y1  = pretrain1(X1[i].nan_to_num()).tolist(), float(Y1[i])\n",
    "                        x2, y2  = pretrain2(X2[i].nan_to_num()).tolist(), float(Y2[i])\n",
    "\n",
    "                        y     = y2\n",
    "                        a,p,o = lrn_.predict(x0,x1,x2)\n",
    "                        bit   = (x0,x1,x2,y0,y1,y2,a,p,o)\n",
    "                        mem   = (x0,x1,x2,y0,y1,y2,a,1,o)\n",
    "\n",
    "                    if self.learn : bits.append(bit)\n",
    "                    if self.replay: mems.append((mem,self.pers_mem_rpt))\n",
    "\n",
    "                    if self.replay and len(mems) >= self.pers_mem_cnt:\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            mem,n = mems[j]\n",
    "                            bits.append(mem)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [mem,n-1]\n",
    "\n",
    "                    if self.learn and len(bits) >= self.pers_lrn_cnt:\n",
    "                        for x0,x1,x2,y0,y1,y2,a,p,o in bits[:self.pers_lrn_cnt]:\n",
    "                            lrn_.learn(x0,x1,x2,y0,y1,y2,a,p,**o)\n",
    "                        del bits[:self.pers_lrn_cnt]\n",
    "\n",
    "                    tp += int(a==send and y==1)\n",
    "                    fp += int(a==send and y==0)\n",
    "                    fn += int(a!=send and y==1)\n",
    "                    tn += int(a!=send and y==0)\n",
    "\n",
    "                    r = y if a==send else .5\n",
    "                    c = y if a==send else 1-y\n",
    "\n",
    "                    tpr = tp/((tp+fn) or 1)\n",
    "                    fpr = fp/((tn+fp) or 1)\n",
    "\n",
    "                    scores[-1].append({\n",
    "                        'reward':r,\n",
    "                        'correct':c,\n",
    "                        'f1':2*tp/((2*tp+fp+tn) or 1),\n",
    "                        'tp': tp,\n",
    "                        'fp': fp,\n",
    "                        'tn': tn,\n",
    "                        'fn': fn,\n",
    "                        'pr':tp/(fp or 1), #positive likelihood ratio\n",
    "                        'plr':tpr/(fpr or 1), #positive likelihood ratio\n",
    "                        'acc':(tp+tn)/(tp+fp+fn+tn),\n",
    "                        'fpr':fp/((tn+fp) or 1),\n",
    "                        'tpr':tp/((tp+fn) or 1), #sensitivity, recall\n",
    "                        'tnr':tn/((tn+fp) or 1), #specificity\n",
    "                        'ppv':tp/((tp+fp) or 1), #positive precision, positive predictive value\n",
    "                        'ppn':tn/((tn+fn) or 1), #negative precision, negative predictive value\n",
    "                        'bac':(tp/((tp+fn) or 1) + tn/((tn+fp) or 1))/2\n",
    "                    })\n",
    "\n",
    "            for s in zip(*scores):\n",
    "                yield {k:mean([s_[k] for s_ in s if k in s_]) for k in set().union(*s)}\n",
    "\n",
    "        yield from get_scores(env)\n",
    "\n",
    "def sqrt_t(t):\n",
    "    return .25*t**(1/2)\n",
    "\n",
    "a0 = (True, 300, 300,   0,   0, 300, 0, 0, False, True, 'del') #for [0]\n",
    "a1 = (True, 0, 300, 300, 300, 0  , 0, 300, False, True, 'del') #for [1]\n",
    "a2 = (True, 0, 300, 0, 300, 300, 300, 300, False, True, 'del') #for [0,1]\n",
    "\n",
    "envs = [ MyEnvironment(a0,a1,a2,[g],rng) for g in testable_G for rng in range(4) ]\n",
    "lrns = [\n",
    "    MethodR(),\n",
    "    Method0(MyLearner1(VWR(\"--loss_function logistic --quiet --coin\"), 1, sqrt_t)),\n",
    "    Method0(MyLearner2(VWR(\"--loss_function logistic --quiet --coin\"), 1, sqrt_t)),\n",
    "    Method1(MyLearner1(VWR(\"--loss_function logistic --quiet --coin\"), 1, sqrt_t)), #current best\n",
    "]\n",
    "\n",
    "vals = [\n",
    "    # ssl (v=2) + basis\n",
    "    #MyEvaluator((), ('x','l','r',40,'l','r',90), (90,'l','r',-1), 0, 0, 120, 4, .5, 2, 4, 1, 0, 1, 0, 0, 0, []),\n",
    "    #MyEvaluator((), ('x','l','r',40,'l','r',90), (90,'l','r',-1), 0, 0, 120, 4, .5, 2, 4, 1, 0, 3, 2, 2, 2, []),\n",
    "    MyEvaluator((), ('x','l','r',40,'l','r',90), (90,'l','r',-1), 0, 0, 120, 4, .5, 2, 4, 1, 1, 3, 2, 2, 2, []),\n",
    "]\n",
    "\n",
    "res = cb.Experiment(envs,lrns,vals).run(processes=32,quiet=True) #type: ignore\n",
    "res.plot_learners(x='g',y=\"pr\",l=['full_name','evaluator_id'],span=1,xticks=False)\n",
    "res.plot_contrast(0,1,l='learner_id',x='g',y='pr',span=1,xticks=False)\n",
    "res.plot_learners(x='g',y=\"tp\",l=['full_name','evaluator_id'],span=1,xticks=False)\n",
    "res.plot_learners(x='g',y=\"fp\",l=['full_name','evaluator_id'],span=1,xticks=False)\n",
    "res.plot_learners(y=\"pr\",l=['full_name','evaluator_id'],span=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
