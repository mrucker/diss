{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice, chain, count, product, repeat\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "data_dir = \"../data\"\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import coba as cb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from parameterfree import COCOB\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    torch.set_num_threads(3)\n",
    "    torch.set_num_interop_threads(3)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "c0 = \"#444\"\n",
    "c1 = \"#0072B2\"\n",
    "c2 = \"#E69F00\"\n",
    "c3 = \"#009E73\"\n",
    "c4 = \"#56B4E9\"\n",
    "c5 = \"#D55E00\"\n",
    "c6 = \"#F0E442\"\n",
    "c7 = \"#CC79A7\"\n",
    "c8 = \"#000000\"\n",
    "c9 = \"#332288\"\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "plt.rc('font', **{'size': 20})\n",
    "\n",
    "def make_emotions_df():\n",
    "\n",
    "    def add_day_columns(df, timestamp_col, participant_df):\n",
    "        return add_rel_day(add_start_day(add_day(df, timestamp_col), participant_df))\n",
    "\n",
    "    def add_day(df, timestamp_col):\n",
    "        df = df.copy()\n",
    "        df[\"Day\"] = (df[timestamp_col]/(60*60*24)).apply(np.floor)\n",
    "        return df\n",
    "\n",
    "    def add_start_day(df, participant_df):\n",
    "        participant_df = participant_df.copy()\n",
    "        participant_df[\"StartDay\"] = (participant_df[\"DataStartStampUtc\"]/(60*60*24)).apply(np.floor)\n",
    "        return pd.merge(df, participant_df[['ParticipantId',\"StartDay\"]])\n",
    "\n",
    "    def add_rel_day(df):\n",
    "        df = df.copy()\n",
    "        df[\"RelDay\"] = df[\"Day\"]-df[\"StartDay\"]\n",
    "        return df\n",
    "\n",
    "    def drop_all1_ends(df):\n",
    "        last, last_gt_1, keep = df.copy(),df.copy(), df.copy()\n",
    "        \n",
    "        last_gt_1 = last_gt_1[last_gt_1[\"State Anxiety\"]!= 1]\n",
    "        last_gt_1 = last_gt_1.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        last_gt_1 = last_gt_1.rename(columns={\"RelDay\":\"Last Day > 1\"})\n",
    "\n",
    "        last = last.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        last = last.rename(columns={\"RelDay\":\"Last Day\"})\n",
    "\n",
    "        for pid in last[\"ParticipantId\"]:\n",
    "            \n",
    "            last_day = last[last[\"ParticipantId\"]==pid][\"Last Day\"].item()\n",
    "            last_day_gt_1 = last_gt_1[last_gt_1[\"ParticipantId\"]==pid][\"Last Day > 1\"].item()\n",
    "            \n",
    "            if last_day-last_day_gt_1 >= 3:\n",
    "                is_not_pid = keep[\"ParticipantId\"] != pid\n",
    "                is_lt_day  = keep[\"RelDay\"] <= last_day_gt_1\n",
    "                keep = keep[is_not_pid | is_lt_day]\n",
    "\n",
    "        return keep\n",
    "\n",
    "    runs_df = pd.read_csv(f'{data_dir}/Runs.csv')\n",
    "    states_df = pd.read_csv(f'{data_dir}/States.csv')\n",
    "    emotions_df = pd.read_csv(f'{data_dir}/Emotions.csv')\n",
    "    participant_df = pd.read_csv(f'{data_dir}/Participants.csv')\n",
    "\n",
    "    emotions_df = emotions_df[emotions_df[\"WatchDataQuality\"] == \"Good\"]\n",
    "\n",
    "    emotions_df[\"ER Interest\"] = pd.to_numeric(emotions_df[\"ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Phone ER Interest\"] = pd.to_numeric(emotions_df[\"Phone ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Response Time (min)\"] = (emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])/60\n",
    "\n",
    "    runs_df = add_day_columns(runs_df, \"DeliveredTimestampUtc\", participant_df)\n",
    "    states_df = add_day_columns(states_df, \"TimestampUtc\", participant_df)\n",
    "    emotions_df = add_day_columns(emotions_df, \"DeliveredTimestampUtc\", participant_df)\n",
    "\n",
    "    runs_df = runs_df[runs_df[\"RelDay\"] < 11]\n",
    "    states_df = states_df[states_df[\"RelDay\"] < 11]\n",
    "    emotions_df = emotions_df[emotions_df[\"RelDay\"] < 11]\n",
    "\n",
    "    return drop_all1_ends(emotions_df)\n",
    "\n",
    "emotions_df = make_emotions_df()\n",
    "\n",
    "class TheoryGridCellSpatialRelationEncoder:\n",
    "    #https://arxiv.org/pdf/2003.00824\n",
    "    def __init__(self, coord_dim = 2, frequency_num = 16, max_radius = 10000,  min_radius = 1000, freq_init = \"geometric\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coord_dim: the dimention of space, 2D, 3D, or other\n",
    "            frequency_num: the number of different sinusoidal with different frequencies/wavelengths\n",
    "            max_radius: the largest context radius this model can handle\n",
    "        \"\"\"\n",
    "\n",
    "        self.frequency_num = frequency_num\n",
    "        self.coord_dim = coord_dim \n",
    "        self.max_radius = max_radius\n",
    "        self.min_radius = min_radius\n",
    "        self.freq_init = freq_init\n",
    "\n",
    "        # the frequency we use for each block, alpha in ICLR paper\n",
    "        self.cal_freq_list()\n",
    "        \n",
    "        # freq_mat shape: (frequency_num, 1)\n",
    "        freq_mat = np.expand_dims(self.freq_list, axis = 1)\n",
    "        # self.freq_mat shape: (frequency_num, 6)\n",
    "        self.freq_mat = np.repeat(freq_mat, 6, axis = 1)\n",
    "\n",
    "        # there unit vectors which is 120 degree apart from each other\n",
    "        self.unit_vec1 = np.asarray([1.0, 0.0])                        # 0\n",
    "        self.unit_vec2 = np.asarray([-1.0/2.0, np.sqrt(3)/2.0])      # 120 degree\n",
    "        self.unit_vec3 = np.asarray([-1.0/2.0, -np.sqrt(3)/2.0])     # 240 degree\n",
    "\n",
    "        # compute the dimention of the encoded spatial relation embedding\n",
    "        self.input_embed_dim = int(6 * self.frequency_num)\n",
    "        \n",
    "    def cal_freq_list(self):\n",
    "        if self.freq_init == \"random\":\n",
    "            self.freq_list = np.random.random(size=[self.frequency_num]) * self.max_radius\n",
    "        elif self.freq_init == \"geometric\":\n",
    "            log_timescale_increment = (np.log(float(self.max_radius) / float(self.min_radius)) /(self.frequency_num*1.0 - 1))\n",
    "            timescales = self.min_radius * np.exp(np.arange(self.frequency_num).astype(float) * log_timescale_increment)\n",
    "            self.freq_list = 1.0/timescales\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "    def make_input_embeds(self, coords):\n",
    "        if type(coords) == np.ndarray:\n",
    "            assert self.coord_dim == np.shape(coords)[2]\n",
    "            coords = list(coords)\n",
    "        elif type(coords) == list:\n",
    "            coords = [[c] for c in coords]\n",
    "            assert self.coord_dim == len(coords[0][0])\n",
    "        else:\n",
    "            raise Exception(\"Unknown coords data type for GridCellSpatialRelationEncoder\")\n",
    "\n",
    "        # (batch_size, num_context_pt, coord_dim)\n",
    "        coords_mat = np.asarray(coords).astype(float)\n",
    "        batch_size = coords_mat.shape[0]\n",
    "        num_context_pt = coords_mat.shape[1]\n",
    "\n",
    "        # compute the dot product between [deltaX, deltaY] and each unit_vec \n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat1 = np.expand_dims(np.matmul(coords_mat, self.unit_vec1), axis = -1)\n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat2 = np.expand_dims(np.matmul(coords_mat, self.unit_vec2), axis = -1)\n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat3 = np.expand_dims(np.matmul(coords_mat, self.unit_vec3), axis = -1)\n",
    "\n",
    "        # (batch_size, num_context_pt, 6)\n",
    "        angle_mat = np.concatenate([angle_mat1, angle_mat1, angle_mat2, angle_mat2, angle_mat3, angle_mat3], axis = -1)\n",
    "        # (batch_size, num_context_pt, 1, 6)\n",
    "        angle_mat = np.expand_dims(angle_mat, axis = -2)\n",
    "        # (batch_size, num_context_pt, frequency_num, 6)\n",
    "        angle_mat = np.repeat(angle_mat, self.frequency_num, axis = -2)\n",
    "        # (batch_size, num_context_pt, frequency_num, 6)\n",
    "        angle_mat = angle_mat * self.freq_mat\n",
    "        # (batch_size, num_context_pt, frequency_num*6)\n",
    "        spr_embeds = np.reshape(angle_mat, (batch_size, num_context_pt, -1))\n",
    "\n",
    "        # make sinuniod function\n",
    "        # sin for 2i, cos for 2i+1\n",
    "        # spr_embeds: (batch_size, num_context_pt, frequency_num*6=input_embed_dim)\n",
    "        spr_embeds[:, :, 0::2] = np.sin(spr_embeds[:, :, 0::2])  # dim 2i\n",
    "        spr_embeds[:, :, 1::2] = np.cos(spr_embeds[:, :, 1::2])  # dim 2i+1\n",
    "        \n",
    "        return spr_embeds.squeeze().tolist()\n",
    "\n",
    "def wins(file_path, timestamps, window_len):\n",
    "    file = open(file_path) if Path(file_path).exists() else nullcontext()\n",
    "    rows = islice(csv.reader(file),1,None) if Path(file_path).exists() else [] #type: ignore\n",
    "\n",
    "    with file:\n",
    "        for timestamp in timestamps:\n",
    "            window = []\n",
    "            for row in rows:\n",
    "                if float(row[0]) < timestamp-window_len: continue\n",
    "                if float(row[0]) > timestamp: break\n",
    "                data = map(float,row[1:])\n",
    "                window.append(next(data) if len(row) == 2 else tuple(data))\n",
    "            yield window\n",
    "\n",
    "def dems(pid, timestamps):\n",
    "    df = pd.read_csv(f'{data_dir}/Baseline.csv')\n",
    "    i = df[\"pid\"].tolist().index(pid)\n",
    "    return df.to_numpy()[[i]*len(timestamps), 1:].tolist()\n",
    "\n",
    "def add1(X):\n",
    "    for x,z in zip(X,np.isnan(X).all(axis=1).astype(int)):\n",
    "        x.append(z)\n",
    "    return X\n",
    "\n",
    "def hrs(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/HeartRate.csv\", timestamps, secs):\n",
    "        w = list(filter(None,w))\n",
    "        if w: features.append([np.mean(w),np.std(w)])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    assert len(set(map(len,features))) == 1, 'hrs'\n",
    "    return StandardScaler().fit_transform(features).tolist() #type: ignore\n",
    "\n",
    "def scs(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/StepCount.csv\", timestamps, secs):\n",
    "        if w: features.append([np.mean(np.diff(w)),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    assert len(set(map(len,features))) == 1, 'scs'\n",
    "    return StandardScaler().fit_transform(features).tolist() #type: ignore\n",
    "\n",
    "def lins1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.var(w,axis=0),*np.percentile([np.linalg.norm(w,axis=1)],q=[10,50,90])])\n",
    "        else: features.append([float('nan')]*6)\n",
    "    assert len(set(map(len,features))) == 1, 'lins1'\n",
    "    return StandardScaler().fit_transform(features).tolist() #type: ignore\n",
    "\n",
    "def lins2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.var(w,axis=0),*np.percentile([np.linalg.norm(w,axis=1)],q=[10,50,90])])\n",
    "        else: features.append([float('nan')]*6)\n",
    "    assert len(set(map(len,features))) == 1, 'lins2'\n",
    "    return StandardScaler().fit_transform(features).tolist() #type: ignore\n",
    "\n",
    "def bats(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Battery.csv\", timestamps, secs):\n",
    "        w = [float(w)/100 for w in w]\n",
    "        if w: features.append([np.max(w)-np.min(w),np.mean(np.diff(w)),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*3)\n",
    "        assert len(set(map(len,features))) == 1, 'bats'\n",
    "    return features\n",
    "\n",
    "def peds(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Pedometer.csv\", timestamps, secs):\n",
    "        if w: features.append([np.mean(np.diff(w)),np.max(w)-np.min(w),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*3)\n",
    "        assert len(set(map(len,features))) == 1, 'peds'\n",
    "    return StandardScaler().fit_transform(features).tolist() #type: ignore\n",
    "\n",
    "def locs1(pid, timestamps, secs, init, freq, lmin, lmax):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Location.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.mean(w,axis=0)[1:]])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    out = TheoryGridCellSpatialRelationEncoder(frequency_num=freq,max_radius=lmax,min_radius=lmin,freq_init=init).make_input_embeds(features)\n",
    "    return [out] if len(timestamps) == 1 else out\n",
    "\n",
    "def locs2(pid, timestamps, secs, freq, lmax, lmin, init):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Location.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.mean(w,axis=0)[1:]])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    return features\n",
    "\n",
    "def tims(timestamps,tzs):\n",
    "    hour, day = 60*60, 60*60*24\n",
    "    for timestamp,tz in zip(timestamps,tzs):\n",
    "        if   tz == \"-04:00\": timestamp -= 4*hour\n",
    "        elif tz == \"-05:00\": timestamp -= 5*hour\n",
    "        time_of_day = (timestamp/day) % 1\n",
    "        day_of_week = (int(timestamp/day)+4) % 7\n",
    "        is_weekend = day_of_week in [0,6]\n",
    "        is_weekday = day_of_week in [1,2,3,4,5]\n",
    "        yield [time_of_day,int(is_weekend),int(is_weekday)]\n",
    "\n",
    "def make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        locs1(pid,ts,*args[5]),\n",
    "        lins1(pid,ts,*args[2]),\n",
    "        tims(ts,tz),\n",
    "        bats(pid,ts,*args[3]),\n",
    "        peds(pid,ts,*args[4])\n",
    "    ]\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def make_xyg2(work_item):\n",
    "    (pid,ts,tz,ys,args) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        locs1(pid,ts,*args[5]),\n",
    "        lins1(pid,ts,*args[2]),\n",
    "        lins2(pid,ts,*args[2]),\n",
    "        scs(pid,ts,*args[1]),\n",
    "        hrs(pid,ts,*args[0]),\n",
    "        tims(ts,tz),\n",
    "        bats(pid,ts,*args[3]),\n",
    "        peds(pid,ts,*args[4])\n",
    "    ]\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        locs1(pid,ts,*args[5]),\n",
    "        lins1(pid,ts,*args[2]),\n",
    "        scs(pid,ts,*args[1]),\n",
    "        hrs(pid,ts,*args[0]),\n",
    "        tims(ts,tz),\n",
    "        bats(pid,ts,*args[3]),\n",
    "        peds(pid,ts,*args[4])\n",
    "    ]\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def make_xyg4(work_item):\n",
    "    (pid,ts,tz,ys,args) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        locs1(pid,ts,*args[5]),\n",
    "        lins1(pid,ts,*args[2]),\n",
    "        tims(ts,tz),\n",
    "        bats(pid,ts,*args[3]),\n",
    "        peds(pid,ts,*args[4]),\n",
    "        dems(pid,ts)\n",
    "    ]\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def make_xyg5(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        locs1(pid,ts,*args[5]),\n",
    "        lins1(pid,ts,*args[2]),\n",
    "        tims(ts,tz),\n",
    "        bats(pid,ts,*args[3]),\n",
    "        peds(pid,ts,*args[4])\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "can_predict = emotions_df[~emotions_df[\"ER Interest\"].isna()].copy().sort_values([\"ParticipantId\",\"SubmissionTimestampUtc\"])\n",
    "\n",
    "def work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        [float(y<np.mean(ys)) for y in ys]\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X1,Y1,G1 = zip(*executor.map(make_xyg1, work_items(30,30,30,3600,30,3600,'geometric',2,.1,100)))\n",
    "\n",
    "X1 = torch.tensor(list(chain.from_iterable(X1))).float()\n",
    "Y1 = torch.tensor(list(chain.from_iterable(Y1))).float().unsqueeze(1)\n",
    "G1 = torch.tensor(list(chain.from_iterable(G1))).int()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X2,Y2,G2 = zip(*executor.map(make_xyg2, work_items(30,30,30,3600,30,3600,'geometric',2,.1,100)))\n",
    "\n",
    "X2 = torch.tensor(list(chain.from_iterable(X2))).float()\n",
    "Y2 = torch.tensor(list(chain.from_iterable(Y2))).float().unsqueeze(1)\n",
    "G2 = torch.tensor(list(chain.from_iterable(G2))).int()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X3,Y3,G3 = zip(*executor.map(make_xyg3, work_items(30,30,30,3600,30,3600,'geometric',2,.1,100)))\n",
    "\n",
    "X3 = torch.tensor(list(chain.from_iterable(X3))).float()\n",
    "Y3 = torch.tensor(list(chain.from_iterable(Y3))).float().unsqueeze(1)\n",
    "G3 = torch.tensor(list(chain.from_iterable(G3))).int()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X4,Y4,G4 = zip(*executor.map(make_xyg4, work_items(30,30,30,3600,30,3600,'geometric',2,.1,100)))\n",
    "\n",
    "X4 = torch.tensor(list(chain.from_iterable(X4))).float()\n",
    "Y4 = torch.tensor(list(chain.from_iterable(Y4))).float().unsqueeze(1)\n",
    "G4 = torch.tensor(list(chain.from_iterable(G4))).int()\n",
    "\n",
    "# with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "#     X5,Y5,G5 = zip(*executor.map(make_xyg1, work_items(30,30,30,3600,30,3600,'geometric',2,.1,100)))\n",
    "\n",
    "# X5 = torch.tensor(list(chain.from_iterable(X5))).float()\n",
    "# Y5 = torch.tensor(list(chain.from_iterable(Y5))).float().unsqueeze(1)\n",
    "# G5 = torch.tensor(list(chain.from_iterable(G5))).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a68d209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2193, 27])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6034de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def local():\n",
    "    def add_day_columns(df, timestamp_col, participant_df):\n",
    "        return add_rel_day(add_start_day(add_day(df, timestamp_col), participant_df))\n",
    "\n",
    "    def add_day(df, timestamp_col):\n",
    "        df = df.copy()\n",
    "        df[\"Day\"] = (df[timestamp_col]/(60*60*24)).apply(np.floor)\n",
    "        return df\n",
    "\n",
    "    def add_start_day(df, participant_df):\n",
    "        participant_df = participant_df.copy()\n",
    "        participant_df[\"StartDay\"] = (participant_df[\"DataStartStampUtc\"]/(60*60*24)).apply(np.floor)\n",
    "        return pd.merge(df, participant_df[['ParticipantId',\"StartDay\"]])\n",
    "\n",
    "    def add_rel_day(df):\n",
    "        df = df.copy()\n",
    "        df[\"RelDay\"] = df[\"Day\"]-df[\"StartDay\"]\n",
    "        return df\n",
    "\n",
    "    def drop_all1_ends(df):\n",
    "        drop, keep = df.copy(), df.copy()\n",
    "        \n",
    "        drop = drop[drop[\"State Anxiety\"]!= 1]\n",
    "        drop = drop.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        drop = drop.rename(columns={\"RelDay\":\"Last Day With Anxiety > 1\"})\n",
    "        drop = drop[drop[\"Last Day With Anxiety > 1\"] <= 8 ]\n",
    "        \n",
    "        for pid,day in drop.itertuples(index=False):\n",
    "            is_not_pid = keep[\"ParticipantId\"] != pid\n",
    "            is_lt_day  = keep[\"RelDay\"] < day\n",
    "            keep = keep[is_not_pid | is_lt_day]\n",
    "\n",
    "        return keep\n",
    "\n",
    "    runs_df = pd.read_csv(f'{data_dir}/Runs.csv')\n",
    "    states_df = pd.read_csv(f'{data_dir}/States.csv')\n",
    "    emotions_df = pd.read_csv(f'{data_dir}/Emotions.csv')\n",
    "    participant_df = pd.read_csv(f'{data_dir}/Participants.csv')\n",
    "\n",
    "    emotions_df = emotions_df[emotions_df[\"WatchDataQuality\"] == \"Good\"]\n",
    "\n",
    "    emotions_df[\"ER Interest\"] = pd.to_numeric(emotions_df[\"ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Phone ER Interest\"] = pd.to_numeric(emotions_df[\"Phone ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Response Time (min)\"] = (emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])/60\n",
    "\n",
    "    runs_df = add_day_columns(runs_df, \"DeliveredTimestampUtc\", participant_df)\n",
    "    states_df = add_day_columns(states_df, \"TimestampUtc\", participant_df)\n",
    "    emotions_df = add_day_columns(emotions_df, \"DeliveredTimestampUtc\", participant_df)\n",
    "\n",
    "    runs_df = runs_df[runs_df[\"RelDay\"] < 11]\n",
    "    states_df = states_df[states_df[\"RelDay\"] < 11]\n",
    "    emotions_df = emotions_df[emotions_df[\"RelDay\"] < 11]\n",
    "\n",
    "    #emotions_df = drop_all1_ends(emotions_df)\n",
    "    \n",
    "    #return emotions_df.groupby(\"ParticipantId\")[\"RelDay\"].min().tolist()\n",
    "\n",
    "    return len(set(emotions_df[\"ParticipantId\"].tolist()))\n",
    "\n",
    "local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ad43280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[413, 414, 415, 416, 417, 418, 420, 421, 423, 424, 425, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 480, 482, 483, 484, 485, 486, 487, 488, 490, 491, 492, 493, 494, 497, 498]'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sorted(set(G1.tolist())))\n",
    "\n",
    "#len(set(emotions_df['ParticipantId'].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143794ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,30,'1'),make_envs(X2,Y2,G2,30,'2')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/1.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a877d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "w = 30\n",
    "r = lambda w:  ['l', 'r', w, 'l', 'r', w]\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,30,'1')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',1) , ()    , 1, 1, 4, 0, 2, 0,  0, 0, 0, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), ()                , (90,1), 1, 1, 0, 0, 0, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/2.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for d,w,s,n in product([.1,.2,.3],[60,90,120],[1,2,3,4],[1,2]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for d,w,s,n in product([.1,.2,.3],[60,90,120],[1,2,3,4],[2]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 4, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for d,w,s,n in product([.4],[60,90,120],[1,2,3,4],[1,2]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for d,w,s,n in product([.4],[60,90,120],[1,2,3,4],[2]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 4, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for d,w,s,n in product([.5],[60,90,120],[1,2,3,4],[2]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 4, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for d,w,s,n in product([.5],[60,90,120],[1,2,3,4],[1]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for d,w,s,n in product([.5],[60,90,120],[1,2,3,4],[2]):\n",
    "    vals.append(MyEvaluator(('x',d,*([w,'l','r']*n),'x'), (w,90,'l','r',-1), (90,1), s, 1, 4, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/3.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for w,s,l in product([60,90,120],[1,2,3,4],[1,2]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,*([w,'l','r']*l),-1), (w,1), 2, 4, s, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "    vals.append(MyEvaluator(('x',.4, 60,'l','r','x'), (60,*([w,'l','r']*l),-1), (w,1), 1, 1, s, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for w,s,l in product([60,90,120],[1,2,3,4],[2]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,*([w,'l','r']*l),-1), (w,1), 2, 4, s, 4, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "    vals.append(MyEvaluator(('x',.4, 60,'l','r','x'), (60,*([w,'l','r']*l),-1), (w,1), 1, 1, s, 4, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/4.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc1e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for s in [4,5,6]:\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, s, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "    vals.append(MyEvaluator(('x',.4, 60,'l','r','x'), (60,90,'l','r',-1), (90,1), 1, 1, s, 1, 2, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/5.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for s in [1,2,3,4,5]:\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, s, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "    vals.append(MyEvaluator(('x',.4, 60,'l','r','x'), (60,90,'l','r',-1), (90,1), 1, 1, 4, 1, s, 3, 10, 2, 2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/6.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e354881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for b,m,r1,r2 in product([1,2,3],[5,10],[1,2,3],[1,2,3]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for b,m,r1,r2 in product([2,3],[3],[1,2],[2,3]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for b,m,r1,r2 in product([2,3],[1,2],[1,2],[2,3]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for b,m,r1,r2 in product([4],[2,3],[1,2],[2,3]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/7.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e49ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for e,w in product([1,2,3],[[2],[3],[2,3]]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], e, w))\n",
    "\n",
    "for e,w in product([1,2,3],[[]]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], e, w))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/8.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572da241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "i, envs = 0, []\n",
    "with ProcessPoolExecutor(max_workers=35) as executor:\n",
    "    for f,l,w1,w2 in product([2,4],[.01,.1,1],[.5*minute,1*minute,2*minute,4*minute,8*minute],[1*minute,5*minute,30*minute,1*hour,4*hour]):\n",
    "        print(i:=i+1)\n",
    "\n",
    "        X,Y,G = zip(*executor.map(make_xyg1, work_items(w1,w1,w1,w2,w1,w2,'geometric',f,l,10)))\n",
    "        X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "        G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "        envs.extend(make_envs(X,Y,G,10,('xyg1',l,f,w1,w2)))\n",
    "\n",
    "        X,Y,G = zip(*executor.map(make_xyg2, work_items(w1,w1,w1,w2,w1,w2,'geometric',f,l,10)))\n",
    "        X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "        G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "        envs.extend(make_envs(X,Y,G,10,('xyg2',l,f,w1,w2)))\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [ MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]) ]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/10.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d13fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,('x1',.1,2,30,3600)),make_envs(X2,Y2,G2,10,('x2',.1,2,30,3600)),make_envs(X3,Y3,G3,10,('x3',.1,2,30,3600))))\n",
    "lrns = [ None ]\n",
    "vals = [ MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]) ]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/11.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "envs = list(make_envs(X1,Y1,G1,10,('x1',.1,2,30,3600)))\n",
    "lrns = [ None ]\n",
    "vals = [ \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 3, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 5, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 2, 4, 3, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 2, 4, 5, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "\n",
    "    MyEvaluator(('x',.4,60,'l','r','x'), (60,90,'l','r',-1), (90,1), 1, 1, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.4,60,'l','r','x'), (60,90,'l','r', 1), (90,1), 1, 1, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 1, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 3, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 1, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 3, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "\n",
    "\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 3, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/12.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for b,m,r1,r2 in product([2,3,4],[2,3,4],[1,2,3],[1,2,3]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[]))\n",
    "\n",
    "for b,m,r1,r2 in product([3,4],[2,3,4],[1,2],[1,2]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[3,]))\n",
    "\n",
    "for b,m,r1,r2 in product([2],[2,3],[1],[1]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[3,]))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/13.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = []\n",
    "for b,m,r1,r2 in product([3,4],[2,3,4],[1,2],[1,2]):\n",
    "    vals.append(MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',1), (90,1), 1, 4, 4, 1, 4, b, m, r1, r2, 0, [0], 1, weighted=[3]))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/14.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06b220",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 15\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 1, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, weighted=[3])\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/15.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb51e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,10,'1')))\n",
    "\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 1, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, weighted=[3])\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/16.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e429ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1'),make_envs(X3,Y3,G3,5,'3')))\n",
    "\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[3]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/17.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e6beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1')))\n",
    "\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 2, weighted=[3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[2,3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 3, weighted=[3]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/18.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e65261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=[1,'x']) -> None:\n",
    "\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W):\n",
    "        rng = cb.CobaRandom(1)\n",
    "        W = W or torch.ones(len(X))\n",
    "        for vw in self._vws:\n",
    "            shuffle = rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = deepcopy(mods_opts)\n",
    "\n",
    "        for i in range(90):\n",
    "\n",
    "            mods_opts = deepcopy(unchanged_mods_opts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=i).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())                \n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods, _ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "                        if s3opt: s3opt.zero_grad()\n",
    "                        loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                        if s3opt: s3opt.step()\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1')))\n",
    "\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 2, weighted=[3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, weighted=[2,3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 3, weighted=[3]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/18.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=[1,'x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = deepcopy(unchanged_mods_opts_emts)\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                    if self.pers_mem_cnt: \n",
    "                        mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                    if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                        rng.shuffle(mems, inplace=True)\n",
    "                        for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                            if j >= len(mems): continue\n",
    "                            x,y,n = mems[j]\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "                            if n == 1: mems.pop(j)\n",
    "                            else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                    if self.pers_lrn_cnt and len(lrnx) >= self.pers_lrn_cnt:\n",
    "                        x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                        y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                        if emt:\n",
    "                            with torch.no_grad():\n",
    "                                emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                        if s3opt:\n",
    "                            if s3opt: s3opt.zero_grad()\n",
    "                            loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                            if s3opt: s3opt.step()\n",
    "\n",
    "                        del lrnx[:self.pers_lrn_cnt]\n",
    "                        del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 0, 0, 0, 0, [0], 1, [3], 1, 25),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 3, 2, 1, 0, [0], 1, [3], 1, 25),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 0, 1, 0, 0, 0, 0, [0], 1, [3], 1, 25),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 0, 1, 0, 0, 0, 0, [0], 1, [3], 5, 25),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/19.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4464707",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 0, 0, 0, 0, [0], 1, [3], 1, 25),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 0, 0, 0, 0, [0], 1, [3], 1, 100),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 0, 0, 0, 0, [0], 1, [3], 1, 200),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 0, 0, 0, 0, [0], 1, [3], 1, 400),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 1, 0, 0, 0, 0, [0], 1, [3], 1, 800),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 0, 0, 0, 0, 0, [0], 1, [3], 10, 400),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 0, 0, 0, 0, 0, [0], 1, [3], 10, 800),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/20.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471d999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 0, 0, 0, 0, 0, [0], 1, [3], 5, 400),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/21.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1'),make_envs(X4,Y4,G4,5,'4')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    #MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 0, 0, 0, 0, 0, [0], 1, [3], 5, 400),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/22.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cf6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X4,Y4,G4,5,'4'),make_envs(X1,Y1,G1,5,'1')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r', 1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    #MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (), 2, 4, 4, 1, 4, 0, 0, 0, 0, 0, [0], 1, [3], 5, 400),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/22.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26987896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X4,Y4,G4,5,'4')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,150,'l','r',150,'l','r','x'), (150,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/23.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1807e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,'l','r',90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 4, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',90,'l','r',-1), (90,'l','r',90,1), 2, 4, 4, 4, 4, 3, 2, 1, 1, 4, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,'l','r',2,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,'l','r',2,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 4, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,100,'l','r',-1), (100,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (-1,1), 2, 4, 4, 0, 4, 3, 2, 1, 1, 8, [0], 4, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (-1,1), 2, 4, 4, 0, 4, 3, 2, 1, 1, 0, [0], 4, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 4, [3], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,'l','r',2,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 4, [3], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/24.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb053c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [456]\n",
      "All Equal, no environment added for [414, 428, 453, 456, 459, 487]\n",
      "{'Learners': 1, 'Environments': 400, 'Interactions': 8400}\n"
     ]
    }
   ],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    return [ roc_auc_score(Y[:,i],predict(X)[:,i]) for i,y in enumerate(self.y)]\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield { f'auc{i}': auc for i,auc in zip(self.y,torch.tensor(s).mean(dim=0).tolist()) }\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,Y1,G1,5,'1'),make_envs(X5,Y5,G5,5,'5')))\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/25.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c01770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [456]\n",
      "{'Learners': 1, 'Environments': 200, 'Interactions': 8400}\n"
     ]
    }
   ],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.train_X)))\n",
    "        return self.train_X[rng_indexes,:], self.train_Y[rng_indexes,:], self.train_G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,1-Y1,G1,5,'1'))) #type: ignore\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 4, [3], 0, 0),\n",
    "    MyEvaluator((),(),('x',1), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0], 1, [], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/26.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c37ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [456]\n",
      "{'Learners': 1, 'Environments': 40, 'Interactions': 1680}\n"
     ]
    }
   ],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        #return self\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,1-Y1,G1,1,'1'))) #type: ignore\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator((),(),('x',1), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0], 1, [], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/27.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c9fc6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [456]\n",
      "{'Learners': 1, 'Environments': 40, 'Interactions': 1680}\n"
     ]
    }
   ],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        #sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        #_I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y#[_I.squeeze()]\n",
    "        _X = self.train_X#[_I.squeeze()]\n",
    "        _G = self.train_G#[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(Y,G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            Y = Y.squeeze()\n",
    "            g_weights = Counter(G.tolist())\n",
    "            y_weights = Counter(Y.tolist())\n",
    "            for g,w in g_weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            for y,w in y_weights.items():\n",
    "                W[Y==y] = W[Y==y] * 1/w\n",
    "            return (W / W.mean())\n",
    "\n",
    "        mods_opts_emts = []\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,Y,G = env.train()\n",
    "                W = make_weighted(Y,G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(Y,G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(Y,G)\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,1-Y1,G1,1,'1'))) #type: ignore\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [3], 0, 0),\n",
    "    MyEvaluator((),(),('x',1), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0], 1, [], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/28.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        #sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        #_I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y#[_I.squeeze()]\n",
    "        _X = self.train_X#[_I.squeeze()]\n",
    "        _G = self.train_G#[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_Y_weights(Y):\n",
    "            W = torch.zeros((len(Y),1))\n",
    "            Y = Y.squeeze()\n",
    "            y_weights = Counter(Y.tolist())\n",
    "            for y,w in y_weights.items():\n",
    "                W[Y==y] = 1/w\n",
    "            return W\n",
    "        def make_G_weights(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            g_weights = Counter(G.tolist())\n",
    "            for g,w in g_weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return W/W.mean()\n",
    "        \n",
    "        def make_YG_weights(Y,G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            g_weights = Counter(G.tolist())\n",
    "            for g,w in g_weights.items():\n",
    "                W[G==g] = 1/w * make_Y_weights(Y[G==g])\n",
    "            return W\n",
    "        \n",
    "        def make_W(Y,G,weight):\n",
    "            if weight == 'YG':\n",
    "                return make_YG_weights(Y,G)\n",
    "            elif weight == 'Y':\n",
    "                return make_Y_weights(Y)\n",
    "            elif weight == 'G':\n",
    "                return make_G_weights(G)\n",
    "            else:\n",
    "                return torch.ones((len(G),1))\n",
    "\n",
    "        mods_opts_emts = []\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_G_weights(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_W(Y,G,self.weighted[0])\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze())\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_W(Y,G,self.weighted[1])\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "        Y = env.train()[1]\n",
    "        C = Counter(Y.squeeze().tolist())\n",
    "        W = torch.tensor(C[0]/C[1]) if self.weighted[2] == 'Y' else None\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss(pos_weight=W)\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,1-Y1,G1,1,'1'))) #type: ignore\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y',None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G',None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG',None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','YG',None], 0, 0),\n",
    "    MyEvaluator((),(),('x',1), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0], 1, [None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'YG','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y',None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','YG','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G',None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','YG','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG',None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','YG','Y'], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/29.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "722e6b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Equal, no environment added for [456]\n",
      "{'Learners': 1, 'Environments': 400, 'Interactions': 277200}\n"
     ]
    }
   ],
   "source": [
    "class EMT:\n",
    "\n",
    "    def __init__(self, N: int = 1, split:int = 100, scorer:str=\"self_consistent_rank\", bound:int=0, features=['x'], rng: int = 1) -> None:\n",
    "\n",
    "        self.rng = cb.CobaRandom(rng)\n",
    "        self.params = {'split':split, 'scorer':scorer, 'bound':bound, 'features':features}\n",
    "\n",
    "        feat_args = []\n",
    "        if 1   not in features: feat_args.append('--noconstant')\n",
    "        if 'a' not in features: feat_args.append('--ignore_linear a')\n",
    "        if 'x' not in features: feat_args.append('--ignore_linear x')\n",
    "        feat_args += [ f'--interactions {f}' for f in features if f not in {1, 'a', 'x'} ]\n",
    "\n",
    "        vw_args = [\n",
    "            \"--emt\",\n",
    "            f\"--emt_tree {bound}\",\n",
    "            f\"--emt_leaf {split}\",\n",
    "            f\"--emt_scorer {scorer}\",\n",
    "            f\"--emt_router {'eigen'}\",\n",
    "            f\"-b {26}\",\n",
    "            \"--min_prediction 0\",\n",
    "            \"--max_prediction 3\",\n",
    "            \"--coin\",\n",
    "            \"--initial_weight 0\",\n",
    "            *feat_args,\n",
    "            '--quiet',\n",
    "            '--random_seed 1337'\n",
    "        ]\n",
    "\n",
    "        self._fns = None\n",
    "        self._vws = [ cb.VowpalMediator() for _ in range(N)]\n",
    "        for vw in self._vws: vw.init_learner(' '.join(vw_args), label_type=2)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = torch.tensor(0)\n",
    "        for vw in self._vws:\n",
    "            preds = preds + torch.tensor([[float(vw.predict(vw.make_example({'x':x}, None)))] for x in X.tolist()]) #type:ignore\n",
    "        return preds/len(self._vws)\n",
    "\n",
    "    def learn(self, X, Y, W = None):\n",
    "        if W is None: \n",
    "            W = torch.ones((len(X),1))\n",
    "        for vw in self._vws:\n",
    "            shuffle = self.rng.shuffle(range(len(X)))\n",
    "            for x,y,w in zip(X[shuffle].tolist(),Y[shuffle].squeeze(1).tolist(),W[shuffle].squeeze(1).tolist()):\n",
    "                vw.learn(vw.make_example({'x':x}, f\"{int(y)} {w}\")) #type:ignore\n",
    "\n",
    "    def __deepcopy__(self,memo):\n",
    "        from copy import copy\n",
    "\n",
    "        if not self._fns:\n",
    "            from uuid import uuid4\n",
    "            from pathlib import Path\n",
    "            Path(\"./emt/\").mkdir(exist_ok=True)\n",
    "            self._fns = [f\"./emt/{uuid4()}\" for _ in range(len(self._vws))]\n",
    "            for fn,old_vw in zip(self._fns,self._vws):\n",
    "                old_vw._vw.save(fn) #type:ignore\n",
    "                old_vw._vw.finish() #type:ignore\n",
    "            self._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]        \n",
    "\n",
    "        copied = copy(self)\n",
    "        copied._vws = [cb.VowpalMediator().init_learner(f'-i {fn} --quiet', label_type=2) for fn in self._fns]\n",
    "        return copied\n",
    "\n",
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        #sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        #_I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y#[_I.squeeze()]\n",
    "        _X = self.train_X#[_I.squeeze()]\n",
    "        _G = self.train_G#[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "\n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted, emt_N, emt_bound):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.emt_N = emt_N\n",
    "        self.emt_bound = emt_bound\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted, 'emt': (emt_N,emt_bound) }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "\n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_Y_weights(Y):\n",
    "            W = torch.zeros((len(Y),1))\n",
    "            Y = Y.squeeze()\n",
    "            y_weights = Counter(Y.tolist())\n",
    "            for y,w in y_weights.items():\n",
    "                W[Y==y] = 1/w\n",
    "            return W\n",
    "        def make_G_weights(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            g_weights = Counter(G.tolist())\n",
    "            for g,w in g_weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return W/W.mean()\n",
    "        \n",
    "        def make_YG_weights(Y,G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            g_weights = Counter(G.tolist())\n",
    "            for g,w in g_weights.items():\n",
    "                W[G==g] = 1/w * make_Y_weights(Y[G==g])\n",
    "            return W\n",
    "        \n",
    "        def make_W(Y,G,weight):\n",
    "            if weight == 'YG':\n",
    "                return make_YG_weights(Y,G)\n",
    "            elif weight == 'Y':\n",
    "                return make_Y_weights(Y)\n",
    "            elif weight == 'G':\n",
    "                return make_G_weights(G)\n",
    "            else:\n",
    "                return torch.ones((len(G),1))\n",
    "\n",
    "        mods_opts_emts = []\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods_opts_emts.append([\n",
    "                [s1,sa,s2,sb,s3],\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt],\n",
    "                EMT(self.emt_N, bound=self.emt_bound, features=['x'], rng=_) if self.emt_N else None\n",
    "            ])\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts,emt in mods_opts_emts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_G_weights(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_W(Y,G,self.weighted[0])\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze())\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_W(Y,G,self.weighted[1])\n",
    "\n",
    "                if opts:\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "                \n",
    "                if emt: emt.learn(s2(s1(X.nan_to_num())), Y, W if 3 in self.weighted else None)\n",
    "\n",
    "        for mods,_,_ in mods_opts_emts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts_emts = mods_opts_emts\n",
    "        Y = env.train()[1]\n",
    "        C = Counter(Y.squeeze().tolist())\n",
    "        W = torch.tensor(C[0]/C[1]) if self.weighted[2] == 'Y' else None\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts_emts = []\n",
    "            for mods,opts,emt in unchanged_mods_opts_emts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                emt  = deepcopy(emt) if self.pers_lrn_cnt else emt\n",
    "                mods_opts_emts.append([mods,opts,emt])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts,_ in mods_opts_emts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts_emts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts_emts))]\n",
    "            memss = [[] for _ in range(len(mods_opts_emts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_,emt in mods_opts_emts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                    if emt: preds = preds + emt.predict(s2(s1(X.nan_to_num())))\n",
    "                return preds/len(mods_opts_emts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss(pos_weight=W)\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts,emt) in zip(lrnxs,lrnys,memss,mods_opts_emts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if emt:\n",
    "                                with torch.no_grad():\n",
    "                                    emt.learn(s2(s1(x.nan_to_num())),y)\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 25)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "lrns = [ None ]\n",
    "envs = list(chain(make_envs(X1,1-Y1,G1,10,'1'))) #type: ignore\n",
    "vals = [\n",
    "    MyEvaluator((),(),('x',1), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, [0], 1, [None,None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y',None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G',None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG',None,None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','G',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','Y',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','YG',None], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, [None,'YG','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y',None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['Y','YG','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G',None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['G','YG','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG',None,'Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','G','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','Y','Y'], 0, 0),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'),(120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 1, 1, 0, [0], 1, ['YG','YG','Y'], 0, 0),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/30.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6aa4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 14400, 'Interactions': 302400}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        if self.s1: self.s1 = [n_feats, *self.s1[1:-1] ,n_feats]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "i, envs = 0, []\n",
    "with ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    for f,l,w1,w2 in product([2,4],[.01,.1,1],[.5*minute,1*minute,2*minute,4*minute],[1*minute,5*minute,30*minute,1*hour]):\n",
    "\n",
    "        print(i:=i+1,flush=True)\n",
    "\n",
    "        X,Y,G = zip(*executor.map(make_xyg1, work_items(w1,w1,w1,w2,w1,w2,'geometric',f,l,10)))\n",
    "        X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "        G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "        envs.extend(make_envs(X,Y,G,5,('xyg1',l,f,w1,w2)))\n",
    "\n",
    "        X,Y,G = zip(*executor.map(make_xyg3, work_items(w1,w1,w1,w2,w1,w2,'geometric',f,l,10)))\n",
    "        X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "        G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "        envs.extend(make_envs(X,Y,G,5,('xyg3',l,f,w1,w2)))\n",
    "        clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [ MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]) ]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/31.log',processes=30,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59f375f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 150, 'Interactions': 6300}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 50)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg3, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,False)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,5,('xyg3',1,2,30,3600,False)))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg3, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,5,('xyg3',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [ \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',120,90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]) \n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/32.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0741ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 180, 'Interactions': 26460}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 30)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg3, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,False)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,5,('xyg3',1,2,30,3600,False)))\n",
    "\n",
    "# with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "#     X,Y,G = zip(*executor.map(_make_xyg3, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "#     X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "#     Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "#     G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "#     envs.extend(make_envs(X,Y,G,5,('xyg3',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [ \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',-1), (90,1), 1, 1, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',-1), (90,1), 2, 1, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 3, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,90,'l','r','x'), (90,90,'l','r',-1), (90,1), 3, 1, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',120,90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]) \n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/33.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "670986f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 1080, 'Interactions': 90720}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 30)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,False)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,False)))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [ \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]), \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3])\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/34.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "246ba0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 1080, 'Interactions': 90720}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        #sampler = RandomUnderSampler(random_state=self.params['rng'])\n",
    "        #_I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y#[_I.squeeze()]\n",
    "        _X = self.train_X#[_I.squeeze()]\n",
    "        _G = self.train_G#[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "\n",
    "        self.s1 = [f if f != 'x' else n_feats for f in self.s1]\n",
    "        self.s2 = [f if f != 'x' else n_feats for f in self.s2]\n",
    "        self.s3 = [f if f != 'x' else n_feats for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], len(set(env.train()[2].tolist()))*len(self.y))\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (len(set(env.train()[2].tolist()))*len(self.y), *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_X.isnan()],_X[~_X.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] in [1,2]:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) + torch.arange(len(self.y)).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*len(self.y)), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "            \n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 30)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,False)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,False)))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [ \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]), \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3])\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/35.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732abcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 1080, 'Interactions': 136080}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomOverSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        n_persons = len(set(env.train()[2].tolist()))\n",
    "        n_y = len(self.y)\n",
    "\n",
    "        self.s1 = [n_feats if f == 'x' else f if f != '-x' else n_feats*n_persons for f in self.s1]\n",
    "        self.s2 = [n_feats if f == 'x' else f for f in self.s2]\n",
    "        self.s3 = [n_feats if f == 'x' else f for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], n_persons*n_y)\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (n_persons*n_y, *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s1[-1] != n_feats*n_persons:\n",
    "                    Z = X\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                    R = torch.arange(len(X)).unsqueeze(1)\n",
    "                    Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                    Z[R,I] = X\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_z,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] != n_y*n_persons:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_y + torch.arange(n_y).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*n_y), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 30)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,False)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,False)))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]), \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',90,'l','r',1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','-x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/36.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741e5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 1080, 'Interactions': 136080}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        new_id = defaultdict(lambda c= count(0):next(c))\n",
    "        sampler = RandomOverSampler(random_state=self.params['rng'])\n",
    "        _Z = [ new_id[z] for z in zip(self.train_Y.squeeze().tolist(),self.train_G.squeeze().tolist()) ]\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),_Z)[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        n_persons = len(set(env.train()[2].tolist()))\n",
    "        n_y = len(self.y)\n",
    "\n",
    "        self.s1 = [n_feats if f == 'x' else f if f != '-x' else n_feats*n_persons for f in self.s1]\n",
    "        self.s2 = [n_feats if f == 'x' else f for f in self.s2]\n",
    "        self.s3 = [n_feats if f == 'x' else f for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], n_persons*n_y)\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (n_persons*n_y, *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s1[-1] != n_feats*n_persons:\n",
    "                    Z = X\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                    R = torch.arange(len(X)).unsqueeze(1)\n",
    "                    Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                    Z[R,I] = X\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_z,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] != n_y*n_persons:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_y + torch.arange(n_y).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*n_y), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()                        \n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 30)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,False)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,False)))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []),\n",
    "    MyEvaluator((), ('x',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []), \n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','x'), (120,90,'l','r',1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []),\n",
    "    MyEvaluator((), ('x',90,'l','r',1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []),\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []),\n",
    "    MyEvaluator(('x',.3,120,'l','r',120,'l','r','-x'), (120,90,'l','r',-1), (90,1), 2, 4, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/37.log',processes=35,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b29534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 1080, 'Interactions': 45360}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, train_X, train_Y, train_G, test_X, test_Y, trn, g, rng):\n",
    "        self.params = {'pid': g, 'rng': rng, 'trn':trn}\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y.float()\n",
    "        self.train_G = train_G\n",
    "        self.test_X = test_X\n",
    "        self.test_Y = test_Y.float()\n",
    "\n",
    "    def ssl(self,neg,sr,yi):\n",
    "        from itertools import compress, repeat, chain\n",
    "        from operator import eq\n",
    "\n",
    "        rng = cb.CobaRandom(self.params['rng'])\n",
    "        rng_order = rng.shuffle(range(len(self.train_X)))\n",
    "\n",
    "        X = self.train_X.tolist()\n",
    "        Y = self.train_Y[:,yi]\n",
    "        Y = list(map(tuple,Y.tolist()))\n",
    "\n",
    "        X = list(map(X.__getitem__,rng_order))\n",
    "        Y = list(map(Y.__getitem__,rng_order))\n",
    "\n",
    "        eq_class  = {y: list(compress(X,map(eq,Y,repeat(y)))) for y in set(Y)}\n",
    "        ne_class  = {y: list(chain(*[v for k,v in eq_class.items() if k != y ])) for y in set(Y)}\n",
    "\n",
    "        def choose_unique(items,given_i):\n",
    "            if len(items) == 1:  return items[0]\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                if i != given_i:\n",
    "                    return items[i]\n",
    "\n",
    "        def choose_n(items,n):\n",
    "            add_to_index = (indexes := set()).add if len(items) > n else (indexes := []).append\n",
    "            for i in rng.randints(None,0,len(items)-1):\n",
    "                add_to_index(i)\n",
    "                if len(indexes)==n:\n",
    "                    return [items[i] for i in indexes]\n",
    "\n",
    "        if sr < 1:\n",
    "            anchor, positive, negative = [], [], []\n",
    "\n",
    "            for i in range(int(len(X)*sr)):\n",
    "                x,y = X[i],Y[i]\n",
    "                anchor.append(x)\n",
    "                positive.append(choose_unique(eq_class[y],i))\n",
    "                negative.append(choose_n     (ne_class[y],neg))\n",
    "            yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "        else:\n",
    "            for _ in range(sr):\n",
    "                anchor, positive, negative = [], [], []\n",
    "                for i in range(len(X)):\n",
    "                    x,y = X[i],Y[i]\n",
    "                    anchor.append(x)\n",
    "                    positive.append(choose_unique(eq_class[y],i))\n",
    "                    negative.append(choose_n     (ne_class[y],neg))\n",
    "\n",
    "                yield torch.tensor(anchor).float(), torch.tensor(positive).float(), torch.tensor(negative).float()\n",
    "\n",
    "    def train(self):\n",
    "        sampler = RandomOverSampler(random_state=self.params['rng'])\n",
    "        _I = sampler.fit_resample(torch.arange(len(self.train_X)).unsqueeze(1),self.train_Y.squeeze())[0] #type: ignore\n",
    "        _Y = self.train_Y[_I.squeeze()]\n",
    "        _X = self.train_X[_I.squeeze()]\n",
    "        _G = self.train_G[_I.squeeze()]\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(_X)))\n",
    "        return _X[rng_indexes,:], _Y[rng_indexes,:], _G[rng_indexes]\n",
    "    \n",
    "    def test(self):\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(self.test_X)))\n",
    "        return self.test_X[rng_indexes,:], self.test_Y[rng_indexes]\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, y, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.y = y\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'y': y, 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from statistics import mean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from copy import deepcopy\n",
    "        from collections import Counter\n",
    "        import peft\n",
    "        \n",
    "        rng = cb.CobaRandom(env.params['rng'])\n",
    "        torch.manual_seed(env.params['rng'])\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weighted(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "\n",
    "        mods_opts = []\n",
    "        opts = []\n",
    "\n",
    "        n_feats = env.test()[0].shape[1]\n",
    "        n_persons = len(set(env.train()[2].tolist()))\n",
    "        n_y = len(self.y)\n",
    "\n",
    "        self.s1 = [n_feats if f == 'x' else f if f != '-x' else n_feats*n_persons for f in self.s1]\n",
    "        self.s2 = [n_feats if f == 'x' else f for f in self.s2]\n",
    "        self.s3 = [n_feats if f == 'x' else f for f in self.s3]\n",
    "\n",
    "        if self.s2 and self.s2[-1] == -1: self.s2 = (*(self.s2)[:-1], n_persons*n_y)\n",
    "        if self.s3 and self.s3[ 0] == -1: self.s3 = (n_persons*n_y, *(self.s3)[1:])\n",
    "\n",
    "        for _ in range(self.n_models):\n",
    "            s1 = FeedForward(self.s1)\n",
    "            s2 = FeedForward(self.s2)\n",
    "            s3 = FeedForward(self.s3)\n",
    "\n",
    "            s1_children = list(s1.children())\n",
    "            s2_children = list(s2.children())\n",
    "\n",
    "            sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "            s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "            sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "            s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "            s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "            saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "            s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "            sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "            s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "            mods = [s1,sa,s2,sb,s3]\n",
    "            opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "            mods_opts.append([mods,opts])\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.train()\n",
    "\n",
    "        for mods,opts in mods_opts:\n",
    "            [s1,sa,s2,sb,s3] = mods\n",
    "            [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "            if self.s1 and self.dae_steps:\n",
    "                opts = list(filter(None,[s1opt,saopt]))\n",
    "                X,_,G = env.train()\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s1[-1] != n_feats*n_persons:\n",
    "                    Z = X\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                    R = torch.arange(len(X)).unsqueeze(1)\n",
    "                    Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                    Z[R,I] = X\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.L1Loss()\n",
    "                for _ in range(self.dae_steps):\n",
    "                    for (_X,_z,_w) in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps0:\n",
    "                opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                if self.s2[-1] != n_y*n_persons:\n",
    "                    Z = Y\n",
    "                else:\n",
    "                    i = defaultdict(lambda c= count(0):next(c))\n",
    "                    I = torch.tensor([[i[g]] for g in G.tolist()]) * n_y + torch.arange(n_y).unsqueeze(0)\n",
    "                    R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                    Z = torch.full((len(G),len(i)*n_y), float('nan'))\n",
    "                    Z[R,I] = Y\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                for _ in range(self.ws_steps0):\n",
    "                    for _X,_z,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                        loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "            if self.ws_steps1:\n",
    "                opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                for o in opts: o.zero_grad()\n",
    "\n",
    "                X, Y, G = env.train()\n",
    "                Y = Y[:,self.y]\n",
    "                W = make_weighted(G)\n",
    "\n",
    "                torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for _ in range(self.ws_steps1):\n",
    "                    for _X,_y,_w in torch_loader:\n",
    "                        for o in opts: o.zero_grad()\n",
    "                        loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                        loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                        for o in opts: o.step()\n",
    "\n",
    "        for mods,_ in mods_opts:\n",
    "            for l in mods: l.eval()\n",
    "\n",
    "        N = 20\n",
    "        scores = [ [] for _ in range(N+1) ]\n",
    "        unchanged_mods_opts = mods_opts\n",
    "\n",
    "        for j in range(90):\n",
    "\n",
    "            mods_opts = []\n",
    "            for mods,opts in unchanged_mods_opts:\n",
    "                mods = deepcopy(mods)\n",
    "                opts = deepcopy(opts)\n",
    "                mods_opts.append([mods,opts])\n",
    "\n",
    "            X, Y = env.test()\n",
    "            trn,tst = next(StratifiedShuffleSplit(1,train_size=N/len(X),random_state=j).split(X,Y))\n",
    "            X = X[np.hstack([trn,tst])]\n",
    "            Y = Y[np.hstack([trn,tst]),:][:,self.y]\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                if not self.pers_rank:\n",
    "                    opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "                else:\n",
    "                    targets  = [ n for n, m in mods[-1].named_modules() if isinstance(m,torch.nn.Linear)]\n",
    "                    config   = peft.LoraConfig(r=self.pers_rank, target_modules=targets)\n",
    "                    mods[-1] = peft.get_peft_model(mods[-1], config)\n",
    "                    opts[-1] = COCOB(mods[-1].parameters())\n",
    "\n",
    "            lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "            lrnys = [[] for _ in range(len(mods_opts))]\n",
    "            memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                with torch.no_grad():\n",
    "                    out = {}\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i,y in enumerate(self.y):\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,y]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,y]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,y]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = roc_auc_score(Y[:,y],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = balanced_accuracy_score(Y[:,y],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = f1_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = precision_score(Y[:,y],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = recall_score(Y[:,y],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = f1_score(Y[:,y],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = f1_score(Y[:,y],preds[:,i],average='weighted')\n",
    "\n",
    "                    return out\n",
    "\n",
    "            scores[0].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()\n",
    "            for i in range(N):\n",
    "\n",
    "                for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                    [s1,_,s2,_,s3 ] = mods\n",
    "                    [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                    x,y = X[i,:], Y[i,:]\n",
    "\n",
    "                    if self.pers_lrn_cnt:\n",
    "                        lrnx.append(x)\n",
    "                        lrny.append(y)\n",
    "\n",
    "                        if self.pers_mem_cnt: \n",
    "                            mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                        if len(mems) > self.pers_mem_cnt and self.pers_mem_rcl > rng.random():\n",
    "                            rng.shuffle(mems, inplace=True)\n",
    "                            for j in reversed(range(1 if self.pers_mem_rcl < 1 else self.pers_mem_rcl)):\n",
    "                                if j >= len(mems): continue\n",
    "                                x,y,n = mems[j]\n",
    "                                lrnx.append(x)\n",
    "                                lrny.append(y)\n",
    "                                if n == 1: mems.pop(j)\n",
    "                                else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                        if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                            x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                            y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                            if s3opt:\n",
    "                                if s3opt: s3opt.zero_grad()\n",
    "                                loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                if s3opt: s3opt.step()\n",
    "\n",
    "                            del lrnx[:self.pers_lrn_cnt]\n",
    "                            del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                scores[i+1].append(score(X[N:,:], Y[N:,:]))\n",
    "\n",
    "        for s in scores:\n",
    "            yield {k:mean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "def make_envs(X, Y, G, R, feats):\n",
    "\n",
    "    too_short = set(g for g in set(G.tolist()) if (g==G).sum() < 30)\n",
    "    all_equal = set(g for g in set(G.tolist()) if any(len(set(y.tolist()))==1 for y in Y[g==G].T))\n",
    "\n",
    "    if any(all_equal): print(f\"All Equal, no environment added for {sorted(all_equal)}\")\n",
    "\n",
    "    for rng,g in product(range(R),sorted(set(G.tolist())-all_equal-too_short)):\n",
    "        try:\n",
    "            next(StratifiedShuffleSplit(1,random_state=rng).split(X[g==G], Y[g==G]))\n",
    "            yield MyEnvironment(X[g!=G], Y[g!=G], G[g!=G], X[g==G], Y[g==G], feats, g, rng)\n",
    "        except ValueError as e:\n",
    "            if 'The least populated class in y has only 1 member' in str(e): continue\n",
    "            raise\n",
    "\n",
    "def _work_items(hrs,scs,lins,bats,peds,locs,init,freq,lmin,lmax,ind):\n",
    "    for pid in sorted(can_predict[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        sub  = can_predict[can_predict.ParticipantId == pid]\n",
    "        tss  = sub[\"SubmissionTimestampUtc\"].tolist()\n",
    "        tzs  = sub[\"LocalTimeZone\"].tolist()\n",
    "        ys   = sub[\"ER Interest\"].tolist()\n",
    "        args = [[hrs],[scs],[lins],[bats],[peds],[locs,init,freq,lmin,lmax]]\n",
    "        yield pid,tss,tzs,ys,args,ind\n",
    "\n",
    "def _make_xyg3(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(scs(pid,ts,*args[1])),\n",
    "        list(hrs(pid,ts,*args[0])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4]))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "def _make_xyg4(work_item):\n",
    "    (pid,ts,tz,ys,args,ind) = work_item\n",
    "    #hrs, scs, lins, bats, peds, locs\n",
    "    fs = [\n",
    "        list(locs1(pid,ts,*args[5])),\n",
    "        list(lins1(pid,ts,*args[2])),\n",
    "        list(tims(ts,tz)),\n",
    "        list(bats(pid,ts,*args[3])),\n",
    "        list(peds(pid,ts,*args[4])),\n",
    "        list(dems(pid,ts))\n",
    "    ]\n",
    "\n",
    "    if ind:\n",
    "        for f in fs:\n",
    "            add1(f)\n",
    "\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    ys = [float(y<np.mean(ys)) for y in ys]\n",
    "    gs = [pid]*len(ys)\n",
    "    return xs,ys,gs\n",
    "\n",
    "hour=60*(minute:=60)\n",
    "envs = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg1, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg1',1,2,30,3600,True)))\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "    X,Y,G = zip(*executor.map(_make_xyg4, _work_items(30,30,30,3600,30,3600,'geometric',2,1,10,True)))\n",
    "    X = torch.tensor(list(chain.from_iterable(X))).float()\n",
    "    Y = torch.tensor(list(chain.from_iterable(Y))).float().unsqueeze(1)\n",
    "    G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "    envs.extend(make_envs(X,Y,G,15,('xyg4',1,2,30,3600,True)))\n",
    "\n",
    "clear_output()\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, []),\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,1), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, [0], 1, [3]),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/4/38.log',processes=30,quiet=True) #type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
