{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "203f7238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Code\n",
    "\n",
    "import csv\n",
    "import warnings\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice, chain, count, product\n",
    "from contextlib import nullcontext\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "data_dir = \"../data\"\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import coba as cb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit\n",
    "from parameterfree import COCOB\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "try:\n",
    "    torch.set_num_threads(3)\n",
    "    torch.set_num_interop_threads(3)\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "c0 = \"#444\"\n",
    "c1 = \"#0072B2\"\n",
    "c2 = \"#E69F00\"\n",
    "c3 = \"#009E73\"\n",
    "c4 = \"#56B4E9\"\n",
    "c5 = \"#D55E00\"\n",
    "c6 = \"#F0E442\"\n",
    "c7 = \"#CC79A7\"\n",
    "c8 = \"#000000\"\n",
    "c9 = \"#332288\"\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "plt.rc('font', **{'size': 20})\n",
    "\n",
    "def make_emotions_df():\n",
    "\n",
    "    def add_day_columns(df, timestamp_col, participant_df):\n",
    "        return add_rel_day(add_start_day(add_day(df, timestamp_col), participant_df))\n",
    "\n",
    "    def add_day(df, timestamp_col):\n",
    "        df = df.copy()\n",
    "        df[\"Day\"] = (df[timestamp_col]/(60*60*24)).apply(np.floor)\n",
    "        return df\n",
    "\n",
    "    def add_start_day(df, participant_df):\n",
    "        participant_df = participant_df.copy()\n",
    "        participant_df[\"StartDay\"] = (participant_df[\"DataStartStampUtc\"]/(60*60*24)).apply(np.floor)\n",
    "        return pd.merge(df, participant_df[['ParticipantId',\"StartDay\"]])\n",
    "\n",
    "    def add_rel_day(df):\n",
    "        df = df.copy()\n",
    "        df[\"RelDay\"] = df[\"Day\"]-df[\"StartDay\"]\n",
    "        return df\n",
    "\n",
    "    def drop_all1_ends(df):\n",
    "        last, last_gt_1, keep = df.copy(),df.copy(), df.copy()\n",
    "        \n",
    "        last_gt_1 = last_gt_1[last_gt_1[\"State Anxiety\"]!= 1]\n",
    "        last_gt_1 = last_gt_1.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        last_gt_1 = last_gt_1.rename(columns={\"RelDay\":\"Last Day > 1\"})\n",
    "\n",
    "        last = last.groupby(\"ParticipantId\")[\"RelDay\"].max().reset_index()\n",
    "        last = last.rename(columns={\"RelDay\":\"Last Day\"})\n",
    "\n",
    "        for pid in last[\"ParticipantId\"]:\n",
    "            \n",
    "            last_day = last[last[\"ParticipantId\"]==pid][\"Last Day\"].item()\n",
    "            last_day_gt_1 = last_gt_1[last_gt_1[\"ParticipantId\"]==pid][\"Last Day > 1\"].item()\n",
    "            \n",
    "            if last_day-last_day_gt_1 >= 3:\n",
    "                is_not_pid = keep[\"ParticipantId\"] != pid\n",
    "                is_lt_day  = keep[\"RelDay\"] <= last_day_gt_1\n",
    "                keep = keep[is_not_pid | is_lt_day]\n",
    "\n",
    "        return keep\n",
    "\n",
    "    emotions_df = pd.read_csv(f'{data_dir}/Emotions.csv')\n",
    "    participant_df = pd.read_csv(f'{data_dir}/Participants.csv')\n",
    "\n",
    "    emotions_df = emotions_df[emotions_df[\"WatchDataQuality\"] == \"Good\"]\n",
    "\n",
    "    emotions_df[\"State Anxiety\"] = pd.to_numeric(emotions_df[\"State Anxiety\"], errors='coerce')\n",
    "    emotions_df[\"ER Interest\"] = pd.to_numeric(emotions_df[\"ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Phone ER Interest\"] = pd.to_numeric(emotions_df[\"Phone ER Interest\"], errors='coerce')\n",
    "    emotions_df[\"Response Time (min)\"] = (emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])/60\n",
    "    emotions_df[\"Response Time (sec)\"] = (emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])\n",
    "    emotions_df[\"Response Time (log min)\"] = np.log((1+ emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"])/60)\n",
    "    emotions_df[\"Response Time (log sec)\"] = np.log((1+ emotions_df[\"SubmissionTimestampUtc\"] - emotions_df[\"DeliveredTimestampUtc\"]))\n",
    "\n",
    "    emotions_df[\"State Anxiety (z)\"] = float('nan')\n",
    "    emotions_df[\"ER Interest (z)\"] = float('nan')\n",
    "\n",
    "    for pid in set(emotions_df[\"ParticipantId\"].tolist()):\n",
    "        is_pid = emotions_df[\"ParticipantId\"] == pid\n",
    "        is_anx = emotions_df[\"State Anxiety\"] > 1\n",
    "        emotions_df.loc[is_pid,[\"ER Interest (z)\"]] = StandardScaler().fit_transform(emotions_df.loc[is_pid,[\"ER Interest\"]])\n",
    "        emotions_df.loc[is_pid&is_anx,[\"State Anxiety (z)\"]] = StandardScaler().fit_transform(emotions_df.loc[is_pid&is_anx,[\"State Anxiety\"]])\n",
    "\n",
    "    emotions_df = add_day_columns(emotions_df, \"DeliveredTimestampUtc\", participant_df)\n",
    "\n",
    "    emotions_df = emotions_df[emotions_df[\"RelDay\"] < 11]\n",
    "\n",
    "    return drop_all1_ends(emotions_df)\n",
    "\n",
    "emotions_df = make_emotions_df()\n",
    "\n",
    "class TheoryGridCellSpatialRelationEncoder:\n",
    "    #https://arxiv.org/pdf/2003.00824\n",
    "    def __init__(self, coord_dim = 2, frequency_num = 16, max_radius = 10000,  min_radius = 1000, freq_init = \"geometric\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coord_dim: the dimention of space, 2D, 3D, or other\n",
    "            frequency_num: the number of different sinusoidal with different frequencies/wavelengths\n",
    "            max_radius: the largest context radius this model can handle\n",
    "        \"\"\"\n",
    "\n",
    "        self.frequency_num = frequency_num\n",
    "        self.coord_dim = coord_dim \n",
    "        self.max_radius = max_radius\n",
    "        self.min_radius = min_radius\n",
    "        self.freq_init = freq_init\n",
    "\n",
    "        # the frequency we use for each block, alpha in ICLR paper\n",
    "        self.cal_freq_list()\n",
    "        \n",
    "        # freq_mat shape: (frequency_num, 1)\n",
    "        freq_mat = np.expand_dims(self.freq_list, axis = 1)\n",
    "        # self.freq_mat shape: (frequency_num, 6)\n",
    "        self.freq_mat = np.repeat(freq_mat, 6, axis = 1)\n",
    "\n",
    "        # there unit vectors which is 120 degree apart from each other\n",
    "        self.unit_vec1 = np.asarray([1.0, 0.0])                        # 0\n",
    "        self.unit_vec2 = np.asarray([-1.0/2.0, np.sqrt(3)/2.0])      # 120 degree\n",
    "        self.unit_vec3 = np.asarray([-1.0/2.0, -np.sqrt(3)/2.0])     # 240 degree\n",
    "\n",
    "        # compute the dimention of the encoded spatial relation embedding\n",
    "        self.input_embed_dim = int(6 * self.frequency_num)\n",
    "        \n",
    "    def cal_freq_list(self):\n",
    "        if self.freq_init == \"random\":\n",
    "            self.freq_list = np.random.random(size=[self.frequency_num]) * self.max_radius\n",
    "        elif self.freq_init == \"geometric\":\n",
    "            log_timescale_increment = (np.log(float(self.max_radius) / float(self.min_radius)) /(self.frequency_num*1.0 - 1))\n",
    "            timescales = self.min_radius * np.exp(np.arange(self.frequency_num).astype(float) * log_timescale_increment)\n",
    "            self.freq_list = 1.0/timescales\n",
    "        else:\n",
    "            raise Exception()\n",
    "\n",
    "    def make_input_embeds(self, coords):\n",
    "        if type(coords) == np.ndarray:\n",
    "            assert self.coord_dim == np.shape(coords)[2]\n",
    "            coords = list(coords)\n",
    "        elif type(coords) == list:\n",
    "            coords = [[c] for c in coords]\n",
    "            assert self.coord_dim == len(coords[0][0])\n",
    "        else:\n",
    "            raise Exception(\"Unknown coords data type for GridCellSpatialRelationEncoder\")\n",
    "\n",
    "        # (batch_size, num_context_pt, coord_dim)\n",
    "        coords_mat = np.asarray(coords).astype(float)\n",
    "        batch_size = coords_mat.shape[0]\n",
    "        num_context_pt = coords_mat.shape[1]\n",
    "\n",
    "        # compute the dot product between [deltaX, deltaY] and each unit_vec \n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat1 = np.expand_dims(np.matmul(coords_mat, self.unit_vec1), axis = -1)\n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat2 = np.expand_dims(np.matmul(coords_mat, self.unit_vec2), axis = -1)\n",
    "        # (batch_size, num_context_pt, 1)\n",
    "        angle_mat3 = np.expand_dims(np.matmul(coords_mat, self.unit_vec3), axis = -1)\n",
    "\n",
    "        # (batch_size, num_context_pt, 6)\n",
    "        angle_mat = np.concatenate([angle_mat1, angle_mat1, angle_mat2, angle_mat2, angle_mat3, angle_mat3], axis = -1)\n",
    "        # (batch_size, num_context_pt, 1, 6)\n",
    "        angle_mat = np.expand_dims(angle_mat, axis = -2)\n",
    "        # (batch_size, num_context_pt, frequency_num, 6)\n",
    "        angle_mat = np.repeat(angle_mat, self.frequency_num, axis = -2)\n",
    "        # (batch_size, num_context_pt, frequency_num, 6)\n",
    "        angle_mat = angle_mat * self.freq_mat\n",
    "        # (batch_size, num_context_pt, frequency_num*6)\n",
    "        spr_embeds = np.reshape(angle_mat, (batch_size, num_context_pt, -1))\n",
    "\n",
    "        # make sinuniod function\n",
    "        # sin for 2i, cos for 2i+1\n",
    "        # spr_embeds: (batch_size, num_context_pt, frequency_num*6=input_embed_dim)\n",
    "        spr_embeds[:, :, 0::2] = np.sin(spr_embeds[:, :, 0::2])  # dim 2i\n",
    "        spr_embeds[:, :, 1::2] = np.cos(spr_embeds[:, :, 1::2])  # dim 2i+1\n",
    "        \n",
    "        return spr_embeds.squeeze().tolist()\n",
    "\n",
    "def is_gt(values,val):\n",
    "    out = (values > val).astype(float)\n",
    "    out[values.isna()] = float('nan')\n",
    "    return out\n",
    "\n",
    "def is_lt(values,val):\n",
    "    out = (values < val).astype(float)\n",
    "    out[values.isna()] = float('nan')\n",
    "    return out\n",
    "\n",
    "def scale(values):\n",
    "    with warnings.catch_warnings():\n",
    "        # If a column has all nan then a warning is thrown\n",
    "        # We supress that warning because that can happen\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return StandardScaler().fit_transform(values).tolist()\n",
    "\n",
    "def add1(X):\n",
    "    for x,z in zip(X,np.isnan(X).all(axis=1).astype(int)):\n",
    "        x.append(z)\n",
    "    return X\n",
    "\n",
    "def wins(file_path, timestamps, window_len):\n",
    "    file = open(file_path) if Path(file_path).exists() else nullcontext()\n",
    "    rows = islice(csv.reader(file),1,None) if Path(file_path).exists() else [] #type: ignore\n",
    "\n",
    "    with file:\n",
    "        for timestamp in timestamps:\n",
    "            window = []\n",
    "            for row in rows:\n",
    "                if float(row[0]) < timestamp-window_len: continue\n",
    "                if float(row[0]) >= timestamp: break\n",
    "                data = map(float,row[1:])\n",
    "                window.append(next(data) if len(row) == 2 else tuple(data))\n",
    "            yield window\n",
    "\n",
    "def dems(pid, timestamps):\n",
    "    df = pd.read_csv(f'{data_dir}/Baseline.csv')\n",
    "    i = df[\"pid\"].tolist().index(pid)\n",
    "    return df.to_numpy()[[i]*len(timestamps), 1:].tolist()\n",
    "\n",
    "def cacher(sensor,pid,ts,maker,*args):\n",
    "    filename = f\"{sensor}_{int(sum(ts))}_{args}_{pid}.pkl\"\n",
    "    features = load_feats(filename)\n",
    "    if features: return features\n",
    "    features = maker(pid,ts,*args)\n",
    "    save_feats(filename,features)\n",
    "    return features\n",
    "\n",
    "def hrs(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/HeartRate.csv\", timestamps, secs):\n",
    "        w = list(filter(None,w))\n",
    "        if w: features.append([np.mean(w),np.std(w)])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    assert len(set(map(len,features))) == 1, 'hrs'\n",
    "    return scale(features)\n",
    "\n",
    "def scs1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    if features: return features\n",
    "\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/StepCount.csv\", timestamps, secs):\n",
    "        if len(w)>1: features.append([np.mean(np.diff(w)),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    assert len(set(map(len,features))) == 1, 'scs1'\n",
    "\n",
    "    return scale(features)\n",
    "\n",
    "def scs2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/StepCount.csv\", timestamps, secs):\n",
    "        if len(w)>1: features.append([np.max(w)-np.min(w)])\n",
    "        else: features.append([float('nan')])\n",
    "    assert len(set(map(len,features))) == 1, 'scs2'\n",
    "    return scale(features)\n",
    "\n",
    "def lins1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.var(w,axis=0),*np.percentile([np.linalg.norm(w,axis=1)],q=[10,50,90])])\n",
    "        else: features.append([float('nan')]*6)\n",
    "    assert len(set(map(len,features))) == 1, 'lins1'\n",
    "    return scale(features)\n",
    "\n",
    "def lins2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.var(w,axis=0),*np.percentile([np.linalg.norm(w,axis=1)],q=[10,50,90])])\n",
    "        else: features.append([float('nan')]*6)\n",
    "    assert len(set(map(len,features))) == 1, 'lins2'\n",
    "    return scale(features)\n",
    "\n",
    "def lins3(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([np.mean(np.linalg.norm(w,axis=1)), *np.std(w,axis=0)])\n",
    "        else: features.append([float('nan')]*4)\n",
    "    assert len(set(map(len,features))) == 1, 'lins3'\n",
    "    return scale(features)\n",
    "\n",
    "def lins4(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/watch/{pid}/LinearAcceleration.csv\", timestamps, secs):\n",
    "        if w: features.append([np.mean(np.linalg.norm(w,axis=1)), *np.std(w,axis=0)])\n",
    "        else: features.append([float('nan')]*4)\n",
    "    assert len(set(map(len,features))) == 1, 'lins2'\n",
    "    return scale(features)\n",
    "\n",
    "def bats1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Battery.csv\", timestamps, secs):\n",
    "        w = [float(w)/100 for w in w]\n",
    "        if len(w)==1: features.append([0,float('nan'),float('nan')])\n",
    "        elif len(w)>1: features.append([np.max(w)-np.min(w),np.mean(np.diff(w)),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*3)\n",
    "        assert len(set(map(len,features))) == 1, 'bats1'\n",
    "    return features\n",
    "\n",
    "def bats2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Battery.csv\", timestamps, secs):\n",
    "        w = [float(w)/100 for w in w]\n",
    "        if w: features.append([np.mean(w),np.max(w)-np.min(w)])\n",
    "        else: features.append([float('nan')]*2)\n",
    "        assert len(set(map(len,features))) == 1, 'bats2'\n",
    "    return features\n",
    "\n",
    "def peds1(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Pedometer.csv\", timestamps, secs):\n",
    "        if len(w)==1: features.append([float('nan'),0,float('nan')])\n",
    "        elif len(w)>1: features.append([np.mean(np.diff(w)),np.max(w)-np.min(w),np.std(np.diff(w))])\n",
    "        else: features.append([float('nan')]*3)\n",
    "        assert len(set(map(len,features))) == 1, 'peds1'\n",
    "    return scale(features)\n",
    "\n",
    "def peds2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Pedometer.csv\", timestamps, secs):\n",
    "        if len(w)>1: features.append([np.max(w)-np.min(w)])\n",
    "        else: features.append([float('nan')])\n",
    "        assert len(set(map(len,features))) == 1, 'peds2'\n",
    "    return scale(features)\n",
    "\n",
    "def locs1(pid, timestamps, secs, freq, lmin, lmax):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Location.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.mean(w,axis=0)[1:]])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    out = TheoryGridCellSpatialRelationEncoder(frequency_num=freq,min_radius=lmin,max_radius=lmax,freq_init='geometric').make_input_embeds(features)\n",
    "    return [out] if len(timestamps) == 1 else out\n",
    "\n",
    "def locs2(pid, timestamps, secs):\n",
    "    features = []\n",
    "    for w in wins(f\"{data_dir}/phone/{pid}/Location.csv\", timestamps, secs):\n",
    "        if w: features.append([*np.mean(w,axis=0)[1:]])\n",
    "        else: features.append([float('nan')]*2)\n",
    "    return features\n",
    "\n",
    "def tims(timestamps,tzs):\n",
    "    hour, day = 60*60, 60*60*24\n",
    "    for timestamp,tz in zip(timestamps,tzs):\n",
    "        if np.isnan(timestamp): \n",
    "            yield [float('nan')]*3\n",
    "        else:\n",
    "            if   tz == \"-04:00\": timestamp -= 4*hour\n",
    "            elif tz == \"-05:00\": timestamp -= 5*hour\n",
    "            time_of_day = (timestamp/day) % 1\n",
    "            day_of_week = (int(timestamp/day)+4) % 7\n",
    "            is_weekend = day_of_week in [0,6]\n",
    "            is_weekday = day_of_week in [1,2,3,4,5]\n",
    "            yield [time_of_day,int(is_weekend),int(is_weekday)]\n",
    "\n",
    "def save_feats(filename,feats):\n",
    "    if not Path(f\"{data_dir}/feats/{filename}\").exists():\n",
    "        with open(f\"{data_dir}/feats/{filename}\", \"wb\") as f: # Use \"wb\" for binary write mode\n",
    "            pickle.dump(feats, f)\n",
    "\n",
    "def load_feats(filename):\n",
    "    if not Path(f\"{data_dir}/feats/{filename}\").exists():\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            with open(f\"{data_dir}/feats/{filename}\", \"rb\") as f: # Use \"wb\" for binary write mode\n",
    "                return pickle.load(f)\n",
    "        except:\n",
    "            Path(f\"{data_dir}/feats/{filename}\").unlink()\n",
    "            return None\n",
    "\n",
    "def make_xyg1(work_item):\n",
    "    (pid,ts,tz,ys,args,secs) = work_item\n",
    "\n",
    "    fs = []\n",
    "\n",
    "    if args[0]: fs.append(list(tims(ts,tz)))\n",
    "    if args[1]: fs.append(list(cacher(\"hrs\"  ,pid,ts,hrs  ,args[1])))\n",
    "    if args[2]: fs.append(list(cacher(\"scs1\" ,pid,ts,scs1 ,args[2])))\n",
    "    if args[3]: fs.append(list(cacher(\"lins1\",pid,ts,lins1,args[3])))\n",
    "    if args[4]: fs.append(list(cacher(\"lins2\",pid,ts,lins2,args[4])))\n",
    "    if args[5]: fs.append(list(cacher(\"bats1\",pid,ts,bats1,args[5])))\n",
    "    if args[6]: fs.append(list(cacher(\"peds1\",pid,ts,peds1,args[6])))\n",
    "    if args[7]: fs.append(list(cacher(\"locs1\",pid,ts,locs1,*args[7])))\n",
    "    if args[8]: fs.append(list(cacher(\"locs2\",pid,ts,locs2,args[8])))\n",
    "\n",
    "    if args[10]:\n",
    "        for f in fs: add1(f)\n",
    "\n",
    "    if args[9]: fs.append(list(dems(pid,ts)))\n",
    "\n",
    "    os = list(ys)\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    gs = [pid]*len(ys)\n",
    "\n",
    "    for sec in (secs or []):\n",
    "        nts = [t-sec for t in ts]\n",
    "        nxs,nys,ngs = make_xyg2((pid,nts,tz,os,args,0))\n",
    "        xs += nxs\n",
    "        ys += nys\n",
    "        gs += ngs\n",
    "\n",
    "    return xs,ys,gs\n",
    "\n",
    "def make_xyg2(work_item):\n",
    "    (pid,ts,tz,ys,args,secs) = work_item\n",
    "\n",
    "    fs = []\n",
    "\n",
    "    if args[0]: fs.append(list(tims(ts,tz)))\n",
    "    if args[1]: fs.append(list(cacher(\"hrs\"  ,pid,ts,hrs  ,args[1])))\n",
    "    if args[2]: fs.append(list(cacher(\"scs2\" ,pid,ts,scs2 ,args[2])))\n",
    "    if args[3]: fs.append(list(cacher(\"lins3\",pid,ts,lins3,args[3])))\n",
    "    if args[4]: fs.append(list(cacher(\"lins4\",pid,ts,lins4,args[4])))\n",
    "    if args[5]: fs.append(list(cacher(\"bats2\",pid,ts,bats2,args[5])))\n",
    "    if args[6]: fs.append(list(cacher(\"peds2\",pid,ts,peds2,args[6])))\n",
    "    if args[7]: fs.append(list(cacher(\"locs1\",pid,ts,locs1,*args[7])))\n",
    "    if args[8]: fs.append(list(cacher(\"locs2\",pid,ts,locs2,args[8])))\n",
    "\n",
    "    if args[10]:\n",
    "        for f in fs: add1(f)\n",
    "\n",
    "    if args[9]: fs.append(list(dems(pid,ts)))\n",
    "\n",
    "    os = list(ys)\n",
    "    xs = [list(chain.from_iterable(feats)) for feats in zip(*fs)]\n",
    "    gs = [pid]*len(ys)\n",
    "\n",
    "    for sec in (secs or []):\n",
    "        if sec != 0:\n",
    "            nts = [t-sec for t in ts]\n",
    "            nxs,nys,ngs = make_xyg2((pid,nts,tz,os,args,0))\n",
    "            xs += nxs\n",
    "            ys += nys\n",
    "            gs += ngs\n",
    "\n",
    "    return xs,ys,gs\n",
    "\n",
    "can_predict = emotions_df.copy().sort_values([\"ParticipantId\",\"DeliveredTimestampUtc\"])\n",
    "\n",
    "def work_items(tims:bool,hrs:int,scs:int,lins1:int,lins2:int,bats:int,peds:int,locs1,locs2:int,dems:bool,add1:bool,event:str,secs=[]):\n",
    "\n",
    "    df = can_predict[~can_predict[\"SubmissionTimestampUtc\"].isna()]\n",
    "\n",
    "    for pid in sorted(df[\"ParticipantId\"].drop_duplicates().tolist()):\n",
    "        ptc  = df[df.ParticipantId == pid]\n",
    "        tss  = ptc[\"SubmissionTimestampUtc\" if event == \"sub\" else \"DeliveredTimestampUtc\"].tolist() \n",
    "        tzs  = ptc[\"LocalTimeZone\"].tolist()\n",
    "\n",
    "        y0s = torch.tensor(is_gt(ptc[\"ER Interest (z)\"],0).tolist())\n",
    "        y1s = torch.tensor(is_lt(ptc['Response Time (min)'],10).tolist())\n",
    "        y2s = torch.tensor(is_gt(ptc[\"State Anxiety (z)\"],0).tolist())\n",
    "        y3s = torch.tensor(is_gt(ptc[\"State Anxiety\"], 1).tolist())\n",
    "        y4s = torch.tensor(ptc[\"Response Time (log sec)\"].tolist())\n",
    "        y5s = torch.tensor(ptc[\"ER Interest (z)\"].tolist())\n",
    "        y6s = torch.tensor(ptc[\"State Anxiety (z)\"].tolist())\n",
    "\n",
    "        ys = torch.hstack((\n",
    "            y0s.unsqueeze(1),\n",
    "            y1s.unsqueeze(1),\n",
    "            y2s.unsqueeze(1),\n",
    "            y3s.unsqueeze(1),\n",
    "            y4s.unsqueeze(1),\n",
    "            y5s.unsqueeze(1),\n",
    "            y6s.unsqueeze(1)\n",
    "        )).tolist()\n",
    "\n",
    "        args = [tims,hrs,scs,lins1,lins2,bats,peds,locs1,locs2,dems,add1]\n",
    "\n",
    "        yield pid,tss,tzs,ys,args,secs\n",
    "\n",
    "#with ProcessPoolExecutor(max_workers=20) as executor:\n",
    "X,Y,G = zip(*map(make_xyg2, work_items(True,0,0,300,0,300,300,[300,2,1,2],0,False,True,\"del\")))\n",
    "\n",
    "Y = torch.tensor(list(chain.from_iterable(Y))).float()\n",
    "G = torch.tensor(list(chain.from_iterable(G))).int()\n",
    "G = G[~torch.isnan(Y[:,[0,1]]).any(dim=1)]\n",
    "\n",
    "testable_G = cb.CobaRandom(1).shuffle([k for k,n in Counter(G.tolist()).items() if n >= 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 4786, 'Interactions': 352779}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "        \n",
    "    class ForceOneModule(torch.nn.Module):\n",
    "        def forward(self,X):\n",
    "            return torch.ones(size=(X.shape[0],1)).float()\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if spec == '1':\n",
    "            return FeedForward.ForceOneModule(), 1\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, a, L, g, rng, v=2):\n",
    "        self.params = {'rng': rng, 'trn':a, 'l':L, 'v':v, 'g':g }\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.G = None\n",
    "        self.a = list(a)\n",
    "        self.L = L\n",
    "        self.g = g\n",
    "        self.v = v\n",
    "        self.a[7] = [a[7],2,1,10] if a[7] else None\n",
    "\n",
    "    def get_data(self):\n",
    "        import torch\n",
    "        import itertools as it\n",
    "\n",
    "        if self.X is not None: return self.X,self.Y,self.G\n",
    "        make = make_xyg1 if self.v == 1 else make_xyg2\n",
    "\n",
    "        X,Y,G = zip(*map(make, work_items(*self.a)))\n",
    "\n",
    "        X = torch.tensor(list(it.chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(it.chain.from_iterable(Y))).float()\n",
    "        G = torch.tensor(list(it.chain.from_iterable(G))).int()\n",
    "\n",
    "        self.X,self.Y,self.G = X,Y,G\n",
    "\n",
    "        if X.shape[0] == 0: return\n",
    "\n",
    "        any_na = torch.isnan(Y[:,[0,1]]).any(dim=1)\n",
    "        X = X[~any_na]\n",
    "        Y = Y[~any_na].float()\n",
    "        G = G[~any_na]\n",
    "\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(X)))\n",
    "\n",
    "        self.X,self.Y,self.G = X[rng_indexes],Y[rng_indexes],G[rng_indexes]\n",
    "\n",
    "        return self.X,self.Y,self.G\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from copy import deepcopy\n",
    "        from numpy import nanmean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from collections import Counter\n",
    "\n",
    "        X,Y,G = env.get_data()\n",
    "        if len(X) == 0: return\n",
    "\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weights(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "        \n",
    "        def get_trn_tst(G,g):\n",
    "            is_tst = sum(G == i for i in g).bool() #type: ignore\n",
    "            return ~is_tst, is_tst\n",
    "\n",
    "        def get_scores(X,Y,G,g,N):\n",
    "\n",
    "            torch.manual_seed(env.params['rng'])\n",
    "\n",
    "            is_trn, is_tst = get_trn_tst(G,g)\n",
    "            trn_X, trn_Y, trn_G = X[is_trn], Y[is_trn], G[is_trn]\n",
    "            tst_X, tst_Y, tst_G = X[is_tst], Y[is_tst], G[is_tst]\n",
    "\n",
    "            try:\n",
    "                next(StratifiedShuffleSplit(1,train_size=N).split(tst_X,tst_Y))\n",
    "            except:\n",
    "                return\n",
    "            \n",
    "            if len(set(tst_Y.squeeze().tolist())) == 1:\n",
    "                return\n",
    "\n",
    "            n_feats = X.shape[1]\n",
    "            n_persons = len(set(trn_G.tolist()))\n",
    "            n_tasks = Y.shape[1]\n",
    "\n",
    "            _s1 = [n_feats if f == 'x' else f if f != '-x' else n_feats*n_persons for f in self.s1]\n",
    "            _s2 = [n_feats if f == 'x' else f                                     for f in self.s2]\n",
    "            _s3 = [n_feats if f == 'x' else f                                     for f in self.s3]\n",
    "\n",
    "            _s1 = [n_tasks if f == 'y' else f for f in _s1]\n",
    "            _s2 = [n_tasks if f == 'y' else f for f in _s2]\n",
    "            _s3 = [n_tasks if f == 'y' else f for f in _s3]\n",
    "\n",
    "            if _s2 and _s2[-1] == -1: _s2 = (*(_s2)[:-1], n_persons*n_tasks)\n",
    "            if _s3 and _s3[ 0] == -1: _s3 = (n_persons*n_tasks, *(_s3)[1:])\n",
    "\n",
    "            mods_opts = []\n",
    "            opts = []\n",
    "\n",
    "            for _ in range(self.n_models):\n",
    "                s1 = FeedForward(_s1)\n",
    "                s2 = FeedForward(_s2)\n",
    "                s3 = FeedForward(_s3)\n",
    "\n",
    "                s1_children = list(s1.children())\n",
    "                s2_children = list(s2.children())\n",
    "\n",
    "                sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "                s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "                sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "                s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "                s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "                saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "                s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "                sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "                s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "                mods = [s1,sa,s2,sb,s3]\n",
    "                opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "                mods_opts.append([mods,opts])\n",
    "\n",
    "            for mods,_ in mods_opts:\n",
    "                for m in mods: m.train()\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                [s1,sa,s2,sb,s3] = mods\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "                if _s1 and self.dae_steps:\n",
    "                    opts = list(filter(None,[s1opt,saopt]))\n",
    "                    X,G,W = trn_X,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    if _s1[-1] != n_feats*n_persons:\n",
    "                        Z = X\n",
    "                    else:\n",
    "                        i = defaultdict(lambda c= count(0):next(c))\n",
    "                        I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                        R = torch.arange(len(X)).unsqueeze(1)\n",
    "                        Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                        Z[R,I] = X\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.L1Loss()\n",
    "                    for _ in range(self.dae_steps):\n",
    "                        for (_X,_z,_w) in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                            for o in opts: o.step()\n",
    "\n",
    "                if self.ws_steps0:\n",
    "                    opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                    for o in opts: o.zero_grad()\n",
    "\n",
    "                    X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    if _s2[-1] != n_tasks*n_persons:\n",
    "                        Z = Y\n",
    "                    else:\n",
    "                        i = defaultdict(lambda c= count(0):next(c))\n",
    "                        I = torch.tensor([[i[g]] for g in G.tolist()]) * n_tasks + torch.arange(n_tasks).unsqueeze(0)\n",
    "                        R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                        Z = torch.full((len(G),len(i)*n_tasks), float('nan'))\n",
    "                        Z[R,I] = Y\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "                if self.ws_steps1:\n",
    "                    opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                    for o in opts: o.zero_grad()\n",
    "\n",
    "                    X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            for mods,_ in mods_opts:\n",
    "                for m in mods: m.eval()\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                out = dict()\n",
    "                with torch.no_grad():\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i in range(n_tasks):\n",
    "\n",
    "                        one = len(set(Y[:,i].tolist())) == 1\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,i]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,i]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,i]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,i]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = float('nan') if one else roc_auc_score(Y[:,i],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = float('nan') if one else balanced_accuracy_score(Y[:,i],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = float('nan') if one else tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = float('nan') if one else tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = float('nan') if one else f1_score(Y[:,i],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = float('nan') if one else precision_score(Y[:,i],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = float('nan') if one else recall_score(Y[:,i],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = float('nan') if one else f1_score(Y[:,i],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = float('nan') if one else f1_score(Y[:,i],preds[:,i],average='weighted')\n",
    "\n",
    "                return out\n",
    "\n",
    "            scores = [ [] for _ in range(N+1) ]\n",
    "            unchanged_mods_opts = mods_opts\n",
    "\n",
    "            X,Y = tst_X, tst_Y\n",
    "\n",
    "            for j in range(15):\n",
    "\n",
    "                mods_opts = []\n",
    "                for mods,opts in unchanged_mods_opts:\n",
    "                    mods = deepcopy(mods)\n",
    "                    opts = deepcopy(opts)\n",
    "                    mods_opts.append([mods,opts])\n",
    "\n",
    "                trn,tst = next(StratifiedShuffleSplit(1,train_size=N,random_state=j).split(X,Y))\n",
    "                trn_X,trn_Y = X[trn],Y[trn]\n",
    "                scr_X,scr_Y = X[tst],Y[tst]\n",
    "\n",
    "                for mods,opts in mods_opts:\n",
    "                    if not self.pers_rank:\n",
    "                        opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "\n",
    "                lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "                lrnys = [[] for _ in range(len(mods_opts))]\n",
    "                memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "                scores[0].append(score(scr_X, scr_Y))\n",
    "\n",
    "                rng = cb.CobaRandom(1)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for i in range(N):\n",
    "\n",
    "                    for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                        [s1,_,s2,_,s3 ] = mods\n",
    "                        [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                        x,y = trn_X[i,:], trn_Y[i,:]\n",
    "\n",
    "                        if self.pers_lrn_cnt:\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "\n",
    "                            if self.pers_mem_cnt: \n",
    "                                mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                            if len(mems) > self.pers_mem_cnt:\n",
    "                                rng.shuffle(mems, inplace=True)\n",
    "                                for j in reversed(range(self.pers_mem_rcl)):\n",
    "                                    if j >= len(mems): continue\n",
    "                                    x,y,n = mems[j]\n",
    "                                    lrnx.append(x)\n",
    "                                    lrny.append(y)\n",
    "                                    if n == 1: mems.pop(j)\n",
    "                                    else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                            if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                                x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                                y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                                if s3opt:\n",
    "                                    if s3opt: s3opt.zero_grad()\n",
    "                                    loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                    if s3opt: s3opt.step()\n",
    "\n",
    "                                del lrnx[:self.pers_lrn_cnt]\n",
    "                                del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                    scores[i+1].append(score(scr_X, scr_Y))\n",
    "\n",
    "            for s in scores:\n",
    "                yield {k:nanmean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "        yield from get_scores(X,Y[:,env.L],G,env.g,20)\n",
    "\n",
    "testable_G = cb.CobaRandom(1).shuffle([k for k,n in Counter(G.tolist()).items() if n >= 30])\n",
    "\n",
    "x = [0,300]\n",
    "L = [[0],[1]]\n",
    "A = product([True],x,x,x,x,x,x,x,[0],[True,False],[True],[\"del\"])\n",
    "\n",
    "envs = [ MyEnvironment(a,l,[g],rng) for a in A for g in testable_G for rng in range(1) for l in L ]\n",
    "\n",
    "lrns = [ None ]\n",
    "vals = [\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,'y'), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, 1, []),\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r', 1), (90,'y'), 0, 0, 4, 1, 4, 3, 2, 2, 2, 0, 1, []),\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,'y'), 0, 0, 4, 1, 0, 3, 2, 2, 2, 0, 1, []),\n",
    "    MyEvaluator((), ('x',120,'l','r',90,'l','r', 1), (90,'y'), 0, 0, 4, 1, 0, 3, 2, 2, 2, 0, 1, []),\n",
    "]\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/2/onp-feats-fixed.log.gz',processes=38,quiet=True) #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8cefebff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [465], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [465], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [469], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [469], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [436], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [436], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [415], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [415], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [452], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [452], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [492], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [492], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [472], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [472], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [443], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [443], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [446], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [446], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [432], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [432], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [460], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [460], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [447], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [447], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [483], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [483], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [431], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [431], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [488], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [488], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [482], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [482], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [491], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [491], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [430], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [430], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [476], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [476], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [473], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [473], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [475], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [475], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [468], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [468], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [417], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [417], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [0], [413], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 0, 0, False, True, 'del'), [1], [413], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [421], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [421], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [480], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [480], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [457], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [457], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [434], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [434], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [416], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [416], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [470], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [470], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [450], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [450], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [440], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [440], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [463], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [463], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [490], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [490], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [466], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [466], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [439], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [439], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [498], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [498], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [465], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [465], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [469], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [469], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [436], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [436], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [415], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [415], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [452], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [452], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [492], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [492], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [472], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [472], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [443], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [443], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [446], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [446], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [432], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [432], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [460], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [460], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [447], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [447], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [483], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [483], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [431], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [431], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [488], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [488], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [482], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [482], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [491], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [491], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [430], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [430], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [476], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [476], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [473], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [473], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [475], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [475], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [468], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [468], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [417], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [417], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [0], [413], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, True, True, 'del'), [1], [413], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [421], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [421], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [480], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [480], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [457], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [457], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [434], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [434], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [416], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [416], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [470], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [470], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [450], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [450], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [440], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [440], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [463], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [463], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [490], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [490], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [466], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [466], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [439], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [439], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [498], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [498], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [465], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [465], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [469], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [469], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [436], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [436], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [415], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [415], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [452], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [452], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [492], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [492], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [472], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [472], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [443], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [443], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [446], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [446], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [432], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [432], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [460], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [460], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [447], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [447], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [483], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [483], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [431], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [431], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [488], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [488], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [482], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [482], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [491], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [491], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [430], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [430], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [476], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [476], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [473], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [473], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [475], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [475], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [468], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [468], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [417], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [417], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [0], [413], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 0, 300, 0, False, True, 'del'), [1], [413], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 300, 0, 0, True, True, 'del'), [0], [421], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 300, 0, 0, True, True, 'del'), [1], [421], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 300, 0, 0, True, True, 'del'), [0], [480], 0),\n",
       " ((True, 0, 0, 0, 0, 0, 300, 0, 0, True, True, 'del'), [1], [480], 0)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "        \n",
    "    class ForceOneModule(torch.nn.Module):\n",
    "        def forward(self,X):\n",
    "            return torch.ones(size=(X.shape[0],1)).float()\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if spec == '1':\n",
    "            return FeedForward.ForceOneModule(), 1\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, a, L, g, rng, v=2):\n",
    "        self.params = {'rng': rng, 'trn':a, 'l':L, 'v':v, 'g':g }\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.G = None\n",
    "        self.a = list(a)\n",
    "        self.L = L\n",
    "        self.g = g\n",
    "        self.v = v\n",
    "        self.a[7] = [a[7],2,1,10] if a[7] else None\n",
    "\n",
    "    def get_data(self):\n",
    "        import torch\n",
    "        import itertools as it\n",
    "\n",
    "        if self.X is not None: return self.X,self.Y,self.G\n",
    "        make = make_xyg1 if self.v == 1 else make_xyg2\n",
    "\n",
    "        X,Y,G = zip(*map(make, work_items(*self.a)))\n",
    "\n",
    "        X = torch.tensor(list(it.chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(it.chain.from_iterable(Y))).float()\n",
    "        G = torch.tensor(list(it.chain.from_iterable(G))).int()\n",
    "\n",
    "        self.X,self.Y,self.G = X,Y,G\n",
    "\n",
    "        if X.shape[0] == 0: return\n",
    "\n",
    "        any_na = torch.isnan(Y[:,[0,1]]).any(dim=1)\n",
    "        X = X[~any_na]\n",
    "        Y = Y[~any_na].float()\n",
    "        G = G[~any_na]\n",
    "\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(X)))\n",
    "\n",
    "        self.X,self.Y,self.G = X[rng_indexes],Y[rng_indexes],G[rng_indexes]\n",
    "\n",
    "        return self.X,self.Y,self.G\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from copy import deepcopy\n",
    "        from numpy import nanmean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from collections import Counter\n",
    "\n",
    "        X,Y,G = env.get_data()\n",
    "        if len(X) == 0: return\n",
    "\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weights(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "        \n",
    "        def get_trn_tst(G,g):\n",
    "            is_tst = sum(G == i for i in g).bool() #type: ignore\n",
    "            return ~is_tst, is_tst\n",
    "\n",
    "        def get_scores(X,Y,G,g,N):\n",
    "\n",
    "            torch.manual_seed(env.params['rng'])\n",
    "\n",
    "            is_trn, is_tst = get_trn_tst(G,g)\n",
    "            trn_X, trn_Y, trn_G = X[is_trn], Y[is_trn], G[is_trn]\n",
    "            tst_X, tst_Y, tst_G = X[is_tst], Y[is_tst], G[is_tst]\n",
    "\n",
    "            try:\n",
    "                next(StratifiedShuffleSplit(1,train_size=N).split(tst_X,tst_Y))\n",
    "            except:\n",
    "                return\n",
    "            \n",
    "            if len(set(tst_Y.squeeze().tolist())) == 1:\n",
    "                return\n",
    "\n",
    "            n_feats = X.shape[1]\n",
    "            n_persons = len(set(trn_G.tolist()))\n",
    "            n_tasks = Y.shape[1]\n",
    "\n",
    "            _s1 = [n_feats if f == 'x' else f if f != '-x' else n_feats*n_persons for f in self.s1]\n",
    "            _s2 = [n_feats if f == 'x' else f                                     for f in self.s2]\n",
    "            _s3 = [n_feats if f == 'x' else f                                     for f in self.s3]\n",
    "\n",
    "            _s1 = [n_tasks if f == 'y' else f for f in _s1]\n",
    "            _s2 = [n_tasks if f == 'y' else f for f in _s2]\n",
    "            _s3 = [n_tasks if f == 'y' else f for f in _s3]\n",
    "\n",
    "            if _s2 and _s2[-1] == -1: _s2 = (*(_s2)[:-1], n_persons*n_tasks)\n",
    "            if _s3 and _s3[ 0] == -1: _s3 = (n_persons*n_tasks, *(_s3)[1:])\n",
    "\n",
    "            mods_opts = []\n",
    "            opts = []\n",
    "\n",
    "            for _ in range(self.n_models):\n",
    "                s1 = FeedForward(_s1)\n",
    "                s2 = FeedForward(_s2)\n",
    "                s3 = FeedForward(_s3)\n",
    "\n",
    "                s1_children = list(s1.children())\n",
    "                s2_children = list(s2.children())\n",
    "\n",
    "                sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "                s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "                sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "                s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "                s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "                saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "                s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "                sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "                s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "                mods = [s1,sa,s2,sb,s3]\n",
    "                opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "                mods_opts.append([mods,opts])\n",
    "\n",
    "            for mods,_ in mods_opts:\n",
    "                for m in mods: m.train()\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                [s1,sa,s2,sb,s3] = mods\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "                if _s1 and self.dae_steps:\n",
    "                    opts = list(filter(None,[s1opt,saopt]))\n",
    "                    X,G,W = trn_X,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    if _s1[-1] != n_feats*n_persons:\n",
    "                        Z = X\n",
    "                    else:\n",
    "                        i = defaultdict(lambda c= count(0):next(c))\n",
    "                        I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                        R = torch.arange(len(X)).unsqueeze(1)\n",
    "                        Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                        Z[R,I] = X\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.L1Loss()\n",
    "                    for _ in range(self.dae_steps):\n",
    "                        for (_X,_z,_w) in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                            for o in opts: o.step()\n",
    "\n",
    "                if self.ws_steps0:\n",
    "                    opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                    for o in opts: o.zero_grad()\n",
    "\n",
    "                    X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    if _s2[-1] != n_tasks*n_persons:\n",
    "                        Z = Y\n",
    "                    else:\n",
    "                        i = defaultdict(lambda c= count(0):next(c))\n",
    "                        I = torch.tensor([[i[g]] for g in G.tolist()]) * n_tasks + torch.arange(n_tasks).unsqueeze(0)\n",
    "                        R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                        Z = torch.full((len(G),len(i)*n_tasks), float('nan'))\n",
    "                        Z[R,I] = Y\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "                if self.ws_steps1:\n",
    "                    opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                    for o in opts: o.zero_grad()\n",
    "\n",
    "                    X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            for mods,_ in mods_opts:\n",
    "                for m in mods: m.eval()\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                out = dict()\n",
    "                with torch.no_grad():\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i in range(n_tasks):\n",
    "\n",
    "                        one = len(set(Y[:,i].tolist())) == 1\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,i]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,i]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,i]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,i]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = float('nan') if one else roc_auc_score(Y[:,i],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = float('nan') if one else balanced_accuracy_score(Y[:,i],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = float('nan') if one else tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = float('nan') if one else tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = float('nan') if one else f1_score(Y[:,i],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = float('nan') if one else precision_score(Y[:,i],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = float('nan') if one else recall_score(Y[:,i],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = float('nan') if one else f1_score(Y[:,i],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = float('nan') if one else f1_score(Y[:,i],preds[:,i],average='weighted')\n",
    "\n",
    "                return out\n",
    "\n",
    "            scores = [ [] for _ in range(N+1) ]\n",
    "            unchanged_mods_opts = mods_opts\n",
    "\n",
    "            X,Y = tst_X, tst_Y\n",
    "\n",
    "            for j in range(15):\n",
    "\n",
    "                mods_opts = []\n",
    "                for mods,opts in unchanged_mods_opts:\n",
    "                    mods = deepcopy(mods)\n",
    "                    opts = deepcopy(opts)\n",
    "                    mods_opts.append([mods,opts])\n",
    "\n",
    "                trn,tst = next(StratifiedShuffleSplit(1,train_size=N,random_state=j).split(X,Y))\n",
    "                trn_X,trn_Y = X[trn],Y[trn]\n",
    "                scr_X,scr_Y = X[tst],Y[tst]\n",
    "\n",
    "                for mods,opts in mods_opts:\n",
    "                    if not self.pers_rank:\n",
    "                        opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "\n",
    "                lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "                lrnys = [[] for _ in range(len(mods_opts))]\n",
    "                memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "                scores[0].append(score(scr_X, scr_Y))\n",
    "\n",
    "                rng = cb.CobaRandom(1)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for i in range(N):\n",
    "\n",
    "                    for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                        [s1,_,s2,_,s3 ] = mods\n",
    "                        [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                        x,y = trn_X[i,:], trn_Y[i,:]\n",
    "\n",
    "                        if self.pers_lrn_cnt:\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "\n",
    "                            if self.pers_mem_cnt: \n",
    "                                mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                            if len(mems) > self.pers_mem_cnt:\n",
    "                                rng.shuffle(mems, inplace=True)\n",
    "                                for j in reversed(range(self.pers_mem_rcl)):\n",
    "                                    if j >= len(mems): continue\n",
    "                                    x,y,n = mems[j]\n",
    "                                    lrnx.append(x)\n",
    "                                    lrny.append(y)\n",
    "                                    if n == 1: mems.pop(j)\n",
    "                                    else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                            if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                                x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                                y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                                if s3opt:\n",
    "                                    if s3opt: s3opt.zero_grad()\n",
    "                                    loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                    if s3opt: s3opt.step()\n",
    "\n",
    "                                del lrnx[:self.pers_lrn_cnt]\n",
    "                                del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                    scores[i+1].append(score(scr_X, scr_Y))\n",
    "\n",
    "            for s in scores:\n",
    "                yield {k:nanmean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "        yield from get_scores(X,Y[:,env.L],G,env.g,20)\n",
    "\n",
    "testable_G = cb.CobaRandom(1).shuffle([k for k,n in Counter(G.tolist()).items() if n >= 30])\n",
    "\n",
    "x = [0,300]\n",
    "L = [[0],[1]]\n",
    "A = product([True],x,x,x,x,x,x,x,[0],[True,False],[True],[\"del\"])\n",
    "\n",
    "envs = [ (a,l,[g],rng) for a in A for g in testable_G for rng in range(1) for l in L ]\n",
    "\n",
    "envs[100:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1319be9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testable_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bf8d2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Learners': 1, 'Environments': 110, 'Interactions': 827442}\n"
     ]
    }
   ],
   "source": [
    "class FeedForward(torch.nn.Sequential):\n",
    "    \"\"\"A Generic implementation of Feedforward Neural Network\"\"\"\n",
    "\n",
    "    class SkipModule(torch.nn.Module):\n",
    "        def __init__(self, layers):\n",
    "            super().__init__()\n",
    "            self.layers = layers\n",
    "        def forward(self,X):\n",
    "            return X + self.layers(X)\n",
    "        \n",
    "    class ForceOneModule(torch.nn.Module):\n",
    "        def forward(self,X):\n",
    "            return torch.ones(size=(X.shape[0],1)).float()\n",
    "\n",
    "    def make_layer(self,curr_dim,spec):\n",
    "        if isinstance(spec,float):\n",
    "            return torch.nn.Dropout(spec), curr_dim\n",
    "        if curr_dim is None and isinstance(spec,int):\n",
    "            return None, spec\n",
    "        if isinstance(spec,int):\n",
    "            return torch.nn.Linear(curr_dim,spec),spec\n",
    "        if spec == 'r':\n",
    "            return torch.nn.ReLU(),curr_dim\n",
    "        if spec == 'l':\n",
    "            return torch.nn.LayerNorm(curr_dim),curr_dim\n",
    "        if spec == 'b':\n",
    "            return torch.nn.BatchNorm1d(curr_dim), curr_dim\n",
    "        if spec == 's':\n",
    "            return torch.nn.Sigmoid(),curr_dim\n",
    "        if spec == '1':\n",
    "            return FeedForward.ForceOneModule(), 1\n",
    "        if isinstance(spec,list):                \n",
    "            return FeedForward.SkipModule(FeedForward([curr_dim] + spec)), curr_dim\n",
    "        raise Exception(f\"Bad Layer: {spec}\")\n",
    "\n",
    "    def __init__(self, specs, rng=1):\n",
    "        \"\"\"Instantiate a Feedfoward network according to specifications.\n",
    "\n",
    "        Args:\n",
    "            specs: A sequence of layer specifications as follows:\n",
    "                -1 -- replaced with the input feature width\n",
    "                <int> -- a LinearLayer with output width equal to <int>\n",
    "                [0,1] -- a Dropout layer with the given probability\n",
    "                'l' -- a LayerNorm\n",
    "                'b' -- a BatchNorm1d\n",
    "                'r' -- a ReLU layer\n",
    "                's' -- a Sigmoid layer\n",
    "                [] -- a skip layer with the given specifications\n",
    "        \"\"\"\n",
    "\n",
    "        torch.manual_seed(rng)\n",
    "        layers,width = [],None\n",
    "        for spec in specs:\n",
    "            layer,width = self.make_layer(width,spec)\n",
    "            if layer: layers.append(layer)\n",
    "        super().__init__(*(layers or [torch.nn.Identity()]))\n",
    "        self.params = {\"specs\": specs, \"rng\": rng }\n",
    "\n",
    "class MyEnvironment:\n",
    "    def __init__(self, a, L, g, rng, v=2):\n",
    "        self.params = {'rng': rng, 'trn':a, 'l':L, 'v':v, 'g':g }\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.G = None\n",
    "        self.a = list(a)\n",
    "        self.L = L\n",
    "        self.g = g\n",
    "        self.v = v\n",
    "        self.a[7] = [a[7],2,1,10] if a[7] else None\n",
    "\n",
    "    def get_data(self):\n",
    "        import torch\n",
    "        import itertools as it\n",
    "\n",
    "        if self.X is not None: return self.X,self.Y,self.G\n",
    "        make = make_xyg1 if self.v == 1 else make_xyg2\n",
    "\n",
    "        X,Y,G = zip(*map(make, work_items(*self.a)))\n",
    "\n",
    "        X = torch.tensor(list(it.chain.from_iterable(X))).float()\n",
    "        Y = torch.tensor(list(it.chain.from_iterable(Y))).float()\n",
    "        G = torch.tensor(list(it.chain.from_iterable(G))).int()\n",
    "\n",
    "        self.X,self.Y,self.G = X,Y,G\n",
    "\n",
    "        if X.shape[0] == 0: return\n",
    "\n",
    "        any_na = torch.isnan(Y[:,[0,1]]).any(dim=1)\n",
    "        X = X[~any_na]\n",
    "        Y = Y[~any_na].float()\n",
    "        G = G[~any_na]\n",
    "\n",
    "        rng_indexes = cb.CobaRandom(self.params['rng']).shuffle(range(len(X)))\n",
    "\n",
    "        self.X,self.Y,self.G = X[rng_indexes],Y[rng_indexes],G[rng_indexes]\n",
    "\n",
    "        return self.X,self.Y,self.G\n",
    "\n",
    "class MyEvaluator:\n",
    "    def __init__(self, s1, s2, s3, dae_steps, dae_dropn, ws_steps0, ws_drop0, ws_steps1, pers_lrn_cnt, pers_mem_cnt, pers_mem_rpt, pers_mem_rcl, pers_rank, n_models, weighted):\n",
    "\n",
    "        self.s1  = s1  #dae sep-sl\n",
    "        self.s2  = s2  #sep-sl\n",
    "        self.s3  = s3  #one-sl pers\n",
    "\n",
    "        self.dae_steps = dae_steps\n",
    "        self.dae_dropn = dae_dropn\n",
    "\n",
    "        self.ws_steps0 = ws_steps0\n",
    "        self.ws_drop0  = ws_drop0\n",
    "        self.ws_steps1 = ws_steps1\n",
    "\n",
    "        self.pers_lrn_cnt = pers_lrn_cnt\n",
    "        self.pers_mem_cnt = pers_mem_cnt\n",
    "        self.pers_mem_rpt = pers_mem_rpt\n",
    "        self.pers_mem_rcl = pers_mem_rcl\n",
    "        self.pers_rank    = pers_rank\n",
    "\n",
    "        self.n_models = n_models\n",
    "        self.weighted = weighted\n",
    "\n",
    "        self.params = { 's1': s1, 's2':s2, 's3':s3, 'dae': (dae_steps,dae_dropn), 'ws': (ws_steps0,ws_drop0,ws_steps1), 'pers': (pers_lrn_cnt,pers_mem_cnt,pers_mem_rpt,pers_mem_rcl,pers_rank), 'n_models': n_models, 'weighted': weighted }\n",
    "\n",
    "    def evaluate(self, env, lrn):\n",
    "        from copy import deepcopy\n",
    "        from numpy import nanmean\n",
    "        from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "        from collections import Counter\n",
    "\n",
    "        X,Y,G = env.get_data()\n",
    "        if len(X) == 0: return\n",
    "\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        def make_weights(G):\n",
    "            W = torch.zeros((len(G),1))\n",
    "            weights = Counter(G.tolist())\n",
    "            for g,w in weights.items():\n",
    "                W[G==g] = 1/w\n",
    "            return (W / W.max())\n",
    "        \n",
    "        def get_trn_tst(G,g):\n",
    "            is_tst = sum(G == i for i in g).bool() #type: ignore\n",
    "            return ~is_tst, is_tst\n",
    "\n",
    "        def get_scores(X,Y,G,g,N):\n",
    "\n",
    "            torch.manual_seed(env.params['rng'])\n",
    "\n",
    "            is_trn, is_tst = get_trn_tst(G,g)\n",
    "            trn_X, trn_Y, trn_G = X[is_trn], Y[is_trn], G[is_trn]\n",
    "            tst_X, tst_Y, tst_G = X[is_tst], Y[is_tst], G[is_tst]\n",
    "\n",
    "            try:\n",
    "                next(StratifiedShuffleSplit(1,train_size=N).split(tst_X,tst_Y))\n",
    "            except:\n",
    "                return\n",
    "            \n",
    "            if len(set(tst_Y.squeeze().tolist())) == 1:\n",
    "                return\n",
    "\n",
    "            n_feats = X.shape[1]\n",
    "            n_persons = len(set(trn_G.tolist()))\n",
    "            n_tasks = Y.shape[1]\n",
    "\n",
    "            _s1 = [n_feats if f == 'x' else f if f != '-x' else n_feats*n_persons for f in self.s1]\n",
    "            _s2 = [n_feats if f == 'x' else f                                     for f in self.s2]\n",
    "            _s3 = [n_feats if f == 'x' else f                                     for f in self.s3]\n",
    "\n",
    "            _s1 = [n_tasks if f == 'y' else f for f in _s1]\n",
    "            _s2 = [n_tasks if f == 'y' else f for f in _s2]\n",
    "            _s3 = [n_tasks if f == 'y' else f for f in _s3]\n",
    "\n",
    "            if _s2 and _s2[-1] == -1: _s2 = (*(_s2)[:-1], n_persons*n_tasks)\n",
    "            if _s3 and _s3[ 0] == -1: _s3 = (n_persons*n_tasks, *(_s3)[1:])\n",
    "\n",
    "            mods_opts = []\n",
    "            opts = []\n",
    "\n",
    "            for _ in range(self.n_models):\n",
    "                s1 = FeedForward(_s1)\n",
    "                s2 = FeedForward(_s2)\n",
    "                s3 = FeedForward(_s3)\n",
    "\n",
    "                s1_children = list(s1.children())\n",
    "                s2_children = list(s2.children())\n",
    "\n",
    "                sa = torch.nn.Sequential(*s1_children[len(s1_children)-self.dae_dropn:])\n",
    "                s1 = torch.nn.Sequential(*s1_children[:len(s1_children)-self.dae_dropn])\n",
    "\n",
    "                sb = torch.nn.Sequential(*s2_children[len(s2_children)-self.ws_drop0:])\n",
    "                s2 = torch.nn.Sequential(*s2_children[:len(s2_children)-self.ws_drop0])\n",
    "\n",
    "                s1opt = COCOB(s1.parameters()) if list(s1.parameters()) else None\n",
    "                saopt = COCOB(sa.parameters()) if list(sa.parameters()) else None\n",
    "                s2opt = COCOB(s2.parameters()) if list(s2.parameters()) else None\n",
    "                sbopt = COCOB(sb.parameters()) if list(sb.parameters()) else None\n",
    "                s3opt = COCOB(s3.parameters()) if list(s3.parameters()) else None\n",
    "\n",
    "                mods = [s1,sa,s2,sb,s3]\n",
    "                opts = [s1opt,saopt,s2opt,sbopt,s3opt]\n",
    "                mods_opts.append([mods,opts])\n",
    "\n",
    "            for mods,_ in mods_opts:\n",
    "                for m in mods: m.train()\n",
    "\n",
    "            for mods,opts in mods_opts:\n",
    "                [s1,sa,s2,sb,s3] = mods\n",
    "                [s1opt,saopt,s2opt,sbopt,s3opt] = opts\n",
    "\n",
    "                if _s1 and self.dae_steps:\n",
    "                    opts = list(filter(None,[s1opt,saopt]))\n",
    "                    X,G,W = trn_X,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    if _s1[-1] != n_feats*n_persons:\n",
    "                        Z = X\n",
    "                    else:\n",
    "                        i = defaultdict(lambda c= count(0):next(c))\n",
    "                        I = torch.tensor([[i[g]] for g in G.tolist()]) * n_feats + torch.arange(n_feats).unsqueeze(0)\n",
    "                        R = torch.arange(len(X)).unsqueeze(1)\n",
    "                        Z = torch.full((len(X),len(i)*n_feats), float('nan'))\n",
    "                        Z[R,I] = X\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.L1Loss()\n",
    "                    for _ in range(self.dae_steps):\n",
    "                        for (_X,_z,_w) in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss(sa(s1(_X.nan_to_num()))[~_z.isnan()],_z[~_z.isnan()]).backward()                        \n",
    "                            for o in opts: o.step()\n",
    "\n",
    "                if self.ws_steps0:\n",
    "                    opts = list(filter(None,[s1opt,s2opt,sbopt]))\n",
    "                    for o in opts: o.zero_grad()\n",
    "\n",
    "                    X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    if _s2[-1] != n_tasks*n_persons:\n",
    "                        Z = Y\n",
    "                    else:\n",
    "                        i = defaultdict(lambda c= count(0):next(c))\n",
    "                        I = torch.tensor([[i[g]] for g in G.tolist()]) * n_tasks + torch.arange(n_tasks).unsqueeze(0)\n",
    "                        R = torch.arange(len(Y)).unsqueeze(1)\n",
    "                        Z = torch.full((len(G),len(i)*n_tasks), float('nan'))\n",
    "                        Z[R,I] = Y\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Z,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    for _ in range(self.ws_steps0):\n",
    "                        for _X,_z,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()                        \n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w.squeeze() if 2 in self.weighted else None)\n",
    "                            loss(sb(s2(s1(_X.nan_to_num())))[~_z.isnan()],_z[~_z.isnan()]).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "                if self.ws_steps1:\n",
    "                    opts = list(filter(None,[s3opt] if self.ws_steps0 else [s1opt,s2opt,s3opt]))\n",
    "                    for o in opts: o.zero_grad()\n",
    "\n",
    "                    X, Y, G, W = trn_X,trn_Y,trn_G,make_weights(trn_G)\n",
    "\n",
    "                    torch_dataset = torch.utils.data.TensorDataset(X,Y,W)\n",
    "                    torch_loader  = torch.utils.data.DataLoader(torch_dataset,batch_size=8,drop_last=True,shuffle=True)\n",
    "\n",
    "                    loss = torch.nn.BCEWithLogitsLoss()\n",
    "                    for _ in range(self.ws_steps1):\n",
    "                        for _X,_y,_w in torch_loader:\n",
    "                            for o in opts: o.zero_grad()\n",
    "                            loss = torch.nn.BCEWithLogitsLoss(weight=_w if 3 in self.weighted else None)\n",
    "                            loss(s3(s2(s1(_X.nan_to_num()))),_y).backward()\n",
    "                            for o in opts: o.step()\n",
    "\n",
    "            for mods,_ in mods_opts:\n",
    "                for m in mods: m.eval()\n",
    "\n",
    "            def predict(X):\n",
    "                preds = torch.tensor(0)\n",
    "                for mods,_ in mods_opts:\n",
    "                    [s1,_,s2,_,s3] = mods\n",
    "                    if s3: preds = preds + torch.sigmoid(s3(s2(s1(X.nan_to_num()))))\n",
    "                return preds/len(mods_opts)\n",
    "\n",
    "            def score(X,Y):\n",
    "                out = dict()\n",
    "                with torch.no_grad():\n",
    "                    probs = predict(X)\n",
    "                    preds = (probs>=.5).float()\n",
    "                    for i in range(n_tasks):\n",
    "\n",
    "                        one = len(set(Y[:,i].tolist())) == 1\n",
    "\n",
    "                        tp = ((preds[:,i]==1) & (Y[:,i]==1)).float().mean().item()\n",
    "                        tn = ((preds[:,i]==0) & (Y[:,i]==0)).float().mean().item()\n",
    "                        fp = ((preds[:,i]==1) & (Y[:,i]==0)).float().mean().item()\n",
    "                        fn = ((preds[:,i]==0) & (Y[:,i]==1)).float().mean().item()\n",
    "\n",
    "                        out[f\"auc{i}\"] = float('nan') if one else roc_auc_score(Y[:,i],probs[:,i])\n",
    "                        out[f\"bal{i}\"] = float('nan') if one else balanced_accuracy_score(Y[:,i],preds[:,i])\n",
    "                        out[f\"sen{i}\"] = float('nan') if one else tp/(tp+fn)\n",
    "                        out[f\"spe{i}\"] = float('nan') if one else tn/(tn+fp)\n",
    "\n",
    "                        for j in [0,1]:\n",
    "                            out[f\"f1{j}{i}\" ] = float('nan') if one else f1_score(Y[:,i],preds[:,i],pos_label=j)\n",
    "                            out[f\"pre{j}{i}\"] = float('nan') if one else precision_score(Y[:,i],preds[:,i],pos_label=j,zero_division=0)\n",
    "                            out[f\"rec{j}{i}\"] = float('nan') if one else recall_score(Y[:,i],preds[:,i],pos_label=j)\n",
    "\n",
    "                        out[f\"f1m{i}\"] = float('nan') if one else f1_score(Y[:,i],preds[:,i],average='macro')\n",
    "                        out[f\"f1w{i}\"] = float('nan') if one else f1_score(Y[:,i],preds[:,i],average='weighted')\n",
    "\n",
    "                return out\n",
    "\n",
    "            scores = [ [] for _ in range(N+1) ]\n",
    "            unchanged_mods_opts = mods_opts\n",
    "\n",
    "            X,Y = tst_X, tst_Y\n",
    "\n",
    "            for j in range(15):\n",
    "\n",
    "                mods_opts = []\n",
    "                for mods,opts in unchanged_mods_opts:\n",
    "                    mods = deepcopy(mods)\n",
    "                    opts = deepcopy(opts)\n",
    "                    mods_opts.append([mods,opts])\n",
    "\n",
    "                trn,tst = next(StratifiedShuffleSplit(1,train_size=N,random_state=j).split(X,Y))\n",
    "                trn_X,trn_Y = X[trn],Y[trn]\n",
    "                scr_X,scr_Y = X[tst],Y[tst]\n",
    "\n",
    "                for mods,opts in mods_opts:\n",
    "                    if not self.pers_rank:\n",
    "                        opts[-1] = COCOB(mods[-1].parameters()) if opts[-1] else None\n",
    "\n",
    "                lrnxs = [[] for _ in range(len(mods_opts))]\n",
    "                lrnys = [[] for _ in range(len(mods_opts))]\n",
    "                memss = [[] for _ in range(len(mods_opts))]\n",
    "\n",
    "                scores[0].append(score(scr_X, scr_Y))\n",
    "\n",
    "                rng = cb.CobaRandom(1)\n",
    "\n",
    "                loss = torch.nn.BCEWithLogitsLoss()\n",
    "                for i in range(N):\n",
    "\n",
    "                    for lrnx,lrny,mems,(mods,opts) in zip(lrnxs,lrnys,memss,mods_opts):\n",
    "                        [s1,_,s2,_,s3 ] = mods\n",
    "                        [_,_,_,_,s3opt] = opts\n",
    "\n",
    "                        x,y = trn_X[i,:], trn_Y[i,:]\n",
    "\n",
    "                        if self.pers_lrn_cnt:\n",
    "                            lrnx.append(x)\n",
    "                            lrny.append(y)\n",
    "\n",
    "                            if self.pers_mem_cnt: \n",
    "                                mems.append([x,y,self.pers_mem_rpt])\n",
    "\n",
    "                            if len(mems) > self.pers_mem_cnt:\n",
    "                                rng.shuffle(mems, inplace=True)\n",
    "                                for j in reversed(range(self.pers_mem_rcl)):\n",
    "                                    if j >= len(mems): continue\n",
    "                                    x,y,n = mems[j]\n",
    "                                    lrnx.append(x)\n",
    "                                    lrny.append(y)\n",
    "                                    if n == 1: mems.pop(j)\n",
    "                                    else: mems[j] = [x,y,n-1]\n",
    "\n",
    "                            if len(lrnx) >= self.pers_lrn_cnt:\n",
    "                                x = torch.stack(lrnx[:self.pers_lrn_cnt])\n",
    "                                y = torch.stack(lrny[:self.pers_lrn_cnt])\n",
    "\n",
    "                                if s3opt:\n",
    "                                    if s3opt: s3opt.zero_grad()\n",
    "                                    loss(s3(s2(s1(x.nan_to_num()))),y).backward()\n",
    "                                    if s3opt: s3opt.step()\n",
    "\n",
    "                                del lrnx[:self.pers_lrn_cnt]\n",
    "                                del lrny[:self.pers_lrn_cnt]\n",
    "\n",
    "                    scores[i+1].append(score(scr_X, scr_Y))\n",
    "\n",
    "            for s in scores:\n",
    "                yield {k:nanmean([s_[k] for s_ in s]) for k in s[0].keys()}\n",
    "\n",
    "        yield from get_scores(X,Y[:,env.L],G,env.g,20)\n",
    "\n",
    "envs = []\n",
    "for g,rng in product(testable_G,range(1)):\n",
    "    envs.append(MyEnvironment((True, 300, 0, 0, 0, 300, 0, 0, 0, False, True, 'del'),[0],[g],rng))\n",
    "    envs.append(MyEnvironment((True, 0, 0, 300, 300, 300, 300, 300, 0, False, True, 'del'),[1],[g],rng))\n",
    "\n",
    "lrns = [ None ]\n",
    "\n",
    "lcs = list(range(1,8))\n",
    "mcs = list(range(1,8))\n",
    "mrs = list(range(1,8))\n",
    "vals = []\n",
    "\n",
    "for lc in [1,2,3]:\n",
    "    vals.append(MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,'y'), 0, 0, 4, 1, 0, lc, 0, 0, 0, 0, 1, []))\n",
    "    vals.append(MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,'y'), 0, 0, 4, 1, 4, lc, 0, 0, 0, 0, 1, []))\n",
    "\n",
    "for lc,mc,mr in product(lcs,mcs,mrs):\n",
    "    if mr <= mc:\n",
    "        vals.append(MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,'y'), 0, 0, 4, 1, 0, lc, mc, mr, mr, 0, 1, []))\n",
    "        vals.append(MyEvaluator((), ('x',120,'l','r',90,'l','r',-1), (90,'y'), 0, 0, 4, 1, 4, lc, mc, mr, mr, 0, 1, []))\n",
    "\n",
    "cb.Experiment(envs,lrns,vals).run('../logs/2/onp-replay.log.gz',processes=38,quiet=True) #type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
